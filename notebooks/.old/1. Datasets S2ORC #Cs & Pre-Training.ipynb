{
 "cells": [
  {
   "cell_type": "raw",
   "id": "70438d49",
   "metadata": {},
   "source": [
    "for name in list(vars()):\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a08672a5",
   "metadata": {},
   "source": [
    "%reset_selective?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4bb5c9a3",
   "metadata": {},
   "source": [
    "for name in list(vars()):\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cfd9c7e",
   "metadata": {},
   "source": [
    "Change background color for output as it wasn't distinguishable from the Markdown text."
   ]
  },
  {
   "cell_type": "raw",
   "id": "c0d69e95-2b18-4d81-8cbc-4c627afe9d4b",
   "metadata": {},
   "source": [
    "%%html\n",
    "<style>\n",
    ".output_wrapper .output .output_area .output_subarea {\n",
    "    background: #E0FFFF\n",
    "}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "33a43354",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9166ba56",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[autoreload of utils.s2orc.read_dataset failed: Traceback (most recent call last):\n",
      "  File \"/home/vivoli/miniconda3/envs/arxiv-manipulation/lib/python3.7/site-packages/IPython/extensions/autoreload.py\", line 245, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"/home/vivoli/miniconda3/envs/arxiv-manipulation/lib/python3.7/site-packages/IPython/extensions/autoreload.py\", line 394, in superreload\n",
      "    module = reload(module)\n",
      "  File \"/home/vivoli/miniconda3/envs/arxiv-manipulation/lib/python3.7/imp.py\", line 314, in reload\n",
      "    return importlib.reload(module)\n",
      "  File \"/home/vivoli/miniconda3/envs/arxiv-manipulation/lib/python3.7/importlib/__init__.py\", line 169, in reload\n",
      "    _bootstrap._exec(spec, module)\n",
      "  File \"<frozen importlib._bootstrap>\", line 630, in _exec\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 728, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\n",
      "  File \"/home/vivoli/Thesis/notebooks/utils/s2orc/read_dataset.py\", line 7, in <module>\n",
      "    from ..cache import _caching\n",
      "ImportError: cannot import name '_caching' from 'utils.cache' (/home/vivoli/Thesis/notebooks/utils/cache.py)\n",
      "]\n",
      "[autoreload of utils.s2orc.preprocessing failed: Traceback (most recent call last):\n",
      "  File \"/home/vivoli/miniconda3/envs/arxiv-manipulation/lib/python3.7/site-packages/IPython/extensions/autoreload.py\", line 245, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"/home/vivoli/miniconda3/envs/arxiv-manipulation/lib/python3.7/site-packages/IPython/extensions/autoreload.py\", line 394, in superreload\n",
      "    module = reload(module)\n",
      "  File \"/home/vivoli/miniconda3/envs/arxiv-manipulation/lib/python3.7/imp.py\", line 314, in reload\n",
      "    return importlib.reload(module)\n",
      "  File \"/home/vivoli/miniconda3/envs/arxiv-manipulation/lib/python3.7/importlib/__init__.py\", line 169, in reload\n",
      "    _bootstrap._exec(spec, module)\n",
      "  File \"<frozen importlib._bootstrap>\", line 630, in _exec\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 728, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\n",
      "  File \"/home/vivoli/Thesis/notebooks/utils/s2orc/preprocessing.py\", line 31, in <module>\n",
      "    from ..cache import no_caching, _caching\n",
      "ImportError: cannot import name 'no_caching' from 'utils.cache' (/home/vivoli/Thesis/notebooks/utils/cache.py)\n",
      "]\n",
      "[autoreload of utils.s2orc.loader failed: Traceback (most recent call last):\n",
      "  File \"/home/vivoli/miniconda3/envs/arxiv-manipulation/lib/python3.7/site-packages/IPython/extensions/autoreload.py\", line 245, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"/home/vivoli/miniconda3/envs/arxiv-manipulation/lib/python3.7/site-packages/IPython/extensions/autoreload.py\", line 394, in superreload\n",
      "    module = reload(module)\n",
      "  File \"/home/vivoli/miniconda3/envs/arxiv-manipulation/lib/python3.7/imp.py\", line 314, in reload\n",
      "    return importlib.reload(module)\n",
      "  File \"/home/vivoli/miniconda3/envs/arxiv-manipulation/lib/python3.7/importlib/__init__.py\", line 169, in reload\n",
      "    _bootstrap._exec(spec, module)\n",
      "  File \"<frozen importlib._bootstrap>\", line 630, in _exec\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 728, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\n",
      "  File \"/home/vivoli/Thesis/notebooks/utils/s2orc/loader.py\", line 7, in <module>\n",
      "    from .preprocessing import (\n",
      "ImportError: cannot import name 'get_dataset' from 'utils.s2orc.preprocessing' (/home/vivoli/Thesis/notebooks/utils/s2orc/preprocessing.py)\n",
      "]\n",
      "[autoreload of utils.config.datasets failed: Traceback (most recent call last):\n",
      "  File \"/home/vivoli/miniconda3/envs/arxiv-manipulation/lib/python3.7/site-packages/IPython/extensions/autoreload.py\", line 245, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"/home/vivoli/miniconda3/envs/arxiv-manipulation/lib/python3.7/site-packages/IPython/extensions/autoreload.py\", line 394, in superreload\n",
      "    module = reload(module)\n",
      "  File \"/home/vivoli/miniconda3/envs/arxiv-manipulation/lib/python3.7/imp.py\", line 314, in reload\n",
      "    return importlib.reload(module)\n",
      "  File \"/home/vivoli/miniconda3/envs/arxiv-manipulation/lib/python3.7/importlib/__init__.py\", line 169, in reload\n",
      "    _bootstrap._exec(spec, module)\n",
      "  File \"<frozen importlib._bootstrap>\", line 630, in _exec\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 728, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\n",
      "  File \"/home/vivoli/Thesis/notebooks/utils/config/datasets.py\", line 5, in <module>\n",
      "    from ..cache import _caching\n",
      "ImportError: cannot import name '_caching' from 'utils.cache' (/home/vivoli/Thesis/notebooks/utils/cache.py)\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5ed4f099",
   "metadata": {},
   "source": [
    "!git clone https://github.com/huggingface/transformers"
   ]
  },
  {
   "cell_type": "raw",
   "id": "12386b7d",
   "metadata": {},
   "source": [
    "!cd transformers; pip install ."
   ]
  },
  {
   "cell_type": "raw",
   "id": "5342f94c",
   "metadata": {},
   "source": [
    "!cd transformers; cd examples; cd language-modeling; pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "smooth-studio",
   "metadata": {},
   "source": [
    "# Datasets\n",
    "---\n",
    "\n",
    "In this notebook we'll build/implement the Dataset classes we need to work with all the dataset we have.\n",
    "First we will introduce the datasets, then we will separate those based on the usage we are going to make of them, then we will use/build/implement our classes in order to manage those different datasets and tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rolled-latter",
   "metadata": {},
   "source": [
    "# 0.0 Utils\n",
    "---\n",
    "\n",
    "We will be using the 🤗*Datasets* library, the 🤗 *Tranformers* library, as we need a tokenizer and a vocab and we'll be using (for loggin) Weigths and Biases (`wandb`) so we are going to install it, independently from Hugging face, and use it within it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "outdoor-organizer",
   "metadata": {},
   "source": [
    "Let's define all the `imports` and `hyperparameters` in one place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "viral-flesh",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------- #\n",
    "#           All Imports\n",
    "# ----------------------------------- #\n",
    "import os # generic\n",
    "import time # logging\n",
    "from tqdm.auto import tqdm # custom progress bar\n",
    "\n",
    "import io\n",
    "import json # load/write data\n",
    "import torch \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# 🤗 Datasets\n",
    "from datasets import (\n",
    "    load_dataset, \n",
    "    # concatenate_datasets,\n",
    "    DatasetDict, \n",
    "    Dataset as hfDataset\n",
    ")\n",
    "\n",
    "# 🤗 Tranformers\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModel, \n",
    "    PreTrainedTokenizer, \n",
    "    DataCollatorForLanguageModeling, \n",
    "    Trainer, \n",
    "    BertForMaskedLM,\n",
    "    TrainingArguments\n",
    ")\n",
    "\n",
    "# s2orc read dataset\n",
    "from utils.s2orc.read_dataset import (\n",
    "    # read_meta_json_list_dict,\n",
    "    # read_pdfs_json_list_dict,\n",
    "    # s2orc_chunk_read,\n",
    "    s2orc_multichunk_read\n",
    ")\n",
    "\n",
    "# s2orc load dataset (not preprocessed)\n",
    "from utils.s2orc.loader import (\n",
    "    s2ortc_loader\n",
    ")\n",
    "\n",
    "# Preprocessing\n",
    "from utils.s2orc.preprocessing import (\n",
    "    # fuse_dictionaries,\n",
    "    # getDataset,\n",
    "    # data_target_preprocess,\n",
    "    # mag_preprocess,\n",
    "    preprocessing\n",
    ")\n",
    "\n",
    "# Dataset configuration files\n",
    "from utils.config.datasets import (\n",
    "    S2orcConfig,\n",
    "    KeyPHConfig\n",
    ")\n",
    "\n",
    "from utils.config.execution import (\n",
    "    RunConfig\n",
    ")\n",
    "\n",
    "# Padding\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# data types\n",
    "from torch.utils.data import (\n",
    "    Dataset, \n",
    "    DataLoader\n",
    ")\n",
    "from typing import (\n",
    "    Dict, List, Union\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "average-society",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------- #\n",
    "#           Hyperparameters\n",
    "# ----------------------------------- #\n",
    "\n",
    "# --------- dataset         --------- #\n",
    "dataset_config = {\n",
    "    'dataset_name':'s2orc',           # str\n",
    "    'dataset_config_name': 'full',    # str\n",
    "    'idxs': '0,1,2',                  # list or comma separated list value\n",
    "    'only_extrated': False,           # bool\n",
    "    'keep_extracted': False,          # bool\n",
    "    'mag_field_of_study': ['Computer Science'], # list\n",
    "    'data': ['abstract'],             # list\n",
    "    'target': ['title'],              # list\n",
    "    'classes': ['mag_field_of_study'] # list\n",
    "}\n",
    "\n",
    "# --------- logging         --------- #\n",
    "logging_config = {\n",
    "    'verbose': True,      # bool\n",
    "    'debug': False,       # bool\n",
    "    'callback': 'WandbCallback'   # Callbacks: (supported) 'PrinterCallback/ProgressCallback', 'TensorBoardCallback', 'WandbCallback', 'CometCallback', 'MLflowCallback', 'AzureMLCallback'\n",
    "}\n",
    "\n",
    "# verbose = True\n",
    "# debug = False\n",
    "# wandb_flag = True\n",
    "\n",
    "# Logging on Weigths and Biases\n",
    "if logging_config['callback'] == 'WandbCallback':\n",
    "    import wandb\n",
    "    wandb.login()\n",
    "\n",
    "# --------- preprocessing   --------- #\n",
    "# in **partial_prepare_data**\n",
    "preprocessing_config = {\n",
    "    'keep_none_papers': False, # if False, remove papers with None eather in abstract or title\n",
    "    'keep_unused_columns': False # if False, remove columns not in dictionary\n",
    "}\n",
    "\n",
    "# remove_None_papers = True \n",
    "# remove_Unused_columns = True\n",
    "\n",
    "\n",
    "# --------- paths           --------- #\n",
    "DATA_PATH = '/home/vivoli/Thesis/data'\n",
    "\n",
    "# --------- model/tokenizer --------- #\n",
    "# hugginface model/tokenizer name\n",
    "MODEL_PATH = 'allenai/scibert_scivocab_uncased'\n",
    "\n",
    "# --------- Run config -------------- #\n",
    "run_config = {\n",
    "    # RunArguments\n",
    "    'name': 'scibert-s2orc',\n",
    "    'number': 0,\n",
    "    'iter': 0,\n",
    "    # TrainingArguments\n",
    "    'seed': 1234 # seed for reproducibility of experiments\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8101f453-19fa-45a0-81ed-2ec098fabb91",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_config: S2orcConfig = S2orcConfig(**dataset_config)\n",
    "run_config: RunConfig = RunConfig(**run_config)\n",
    "\n",
    "output_dir=f\"./tmp_trainer/#{run_config.number}_{run_config.iteration}_{run_config.name}_seed{run_config.seed}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "scheduled-color",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------- #\n",
    "#           Logging\n",
    "# ----------------------------------- #\n",
    "LOGS_PATH = 'logs'\n",
    "import logging\n",
    "\n",
    "# Create a custom logger\n",
    "logger = logging.getLogger(\"datasets.explanation\")\n",
    "\n",
    "# Create handlers\n",
    "c_handler = logging.StreamHandler()\n",
    "f_handler = logging.FileHandler(f'{LOGS_PATH}/file.log')\n",
    "d_handler = logging.FileHandler(f'{LOGS_PATH}/debug.log')\n",
    "\n",
    "c_handler.setLevel(logging.DEBUG if verbose else logging.WARNING) # verbose is to log everything\n",
    "f_handler.setLevel(logging.ERROR)\n",
    "d_handler.setLevel(logging.DEBUG)\n",
    "\n",
    "# Create formatters and add it to handlers\n",
    "c_format = logging.Formatter('%(name)s - %(levelname)s - %(message)s')\n",
    "f_format = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "d_format = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "\n",
    "c_handler.setFormatter(c_format)\n",
    "f_handler.setFormatter(f_format)\n",
    "d_handler.setFormatter(d_format)\n",
    "\n",
    "# Add handlers to the logger\n",
    "logger.addHandler(c_handler)\n",
    "logger.addHandler(f_handler)\n",
    "logger.addHandler(d_handler)\n",
    "\n",
    "logger.warning('This is a warning')\n",
    "logger.error('This is an error')\n",
    "logger.info('This is an info')\n",
    "logger.debug('This is a debug')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "968b1fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.init(project=\"huggingface\")\n",
    "# Optional: log both gradients and parameters\n",
    "%env WANDB_WATCH=all"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "timely-attitude",
   "metadata": {},
   "source": [
    "# 0.1 KeyPhrase Dataset\n",
    "---\n",
    "\n",
    "These are testing datasets"
   ]
  },
  {
   "cell_type": "raw",
   "id": "hidden-kazakhstan",
   "metadata": {},
   "source": [
    "dataset_names = ['inspec', 'krapivin', 'nus', 'semeval', 'kp20k', 'duc', 'stackexchange']\n",
    "json_base_dir = DATA_PATH + '/keyphrase/json/'"
   ]
  },
  {
   "cell_type": "raw",
   "id": "returning-spine",
   "metadata": {},
   "source": [
    "def json_keyphrase_read( dataset_name, json_base_dir, file_name=None ):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        dataset_name (string): Directory name with the json file.\n",
    "        json_base_dir (string): Path to the Dataset directory.\n",
    "        file_name (string): (Optional) Json file name.\n",
    "        \n",
    "    Return:\n",
    "        json_list (list of dict): List of dictionaries, each one with the fields \n",
    "            - 'title' (string)\n",
    "            - 'abstract' (string)\n",
    "            - 'fulltext' (string | '')\n",
    "            - 'keywords' (list)\n",
    "    \"\"\"\n",
    "    logger.debug(dataset_name)\n",
    "\n",
    "    input_json_path = os.path.join(json_base_dir, dataset_name, \n",
    "                                   '%s_test.json' % dataset_name if file_name is None else file_name)\n",
    "\n",
    "    json_list_of_dict = []\n",
    "    with open(input_json_path, 'r') as input_json:\n",
    "        for json_line in input_json:\n",
    "            json_dict = json.loads(json_line)\n",
    "\n",
    "            if dataset_name == 'stackexchange':\n",
    "                json_dict['abstract'] = json_dict['question']\n",
    "                json_dict['keywords'] = json_dict['tags']            \n",
    "                del json_dict['question']\n",
    "                del json_dict['tags']\n",
    "\n",
    "            keywords = json_dict['keywords']\n",
    "\n",
    "            if isinstance(keywords, str):\n",
    "                keywords = keywords.split(';')\n",
    "                json_dict['keywords'] = keywords\n",
    "\n",
    "            json_list_of_dict.append(json_dict)\n",
    "                \n",
    "    return json_list_of_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "liberal-atlantic",
   "metadata": {},
   "source": [
    "This keyphrase dataset could be useful for testing some model on keyphrase task or abstract-title summarization/generation/embedding.\n",
    "\n",
    "For now, we can avoid implementing the Dataset's and DataLoader's classes for this objects.\n",
    "\n",
    "Although, the dataset and Dataloader would be simple as follow:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "infinite-capture",
   "metadata": {},
   "source": [
    "def data_keyphrase_process(json_list_of_dict, tokenizer, debug=False):\n",
    "    data = []\n",
    "    for json_dict in json_list_of_dict:\n",
    "        title_tensor_ = torch.tensor(tokenizer.encode(json_dict['title']),\n",
    "                                dtype=torch.long)\n",
    "        if debug: print(title_tensor_)\n",
    "        abstract_tensor_ = torch.tensor(tokenizer.encode(json_dict['abstract']),\n",
    "                                dtype=torch.long)\n",
    "        if debug: print(abstract_tensor_)\n",
    "        fulltext_tensor_ = torch.tensor(tokenizer.encode(json_dict['fulltext']),\n",
    "                                dtype=torch.long)\n",
    "        if debug: print(fulltext_tensor_)\n",
    "        keywords_tensor_ = torch.tensor(tokenizer(json_dict['keywords'], padding=True)['input_ids'], \n",
    "                                dtype=torch.long)\n",
    "        if debug: print(keywords_tensor_)\n",
    "\n",
    "        data.append((title_tensor_, abstract_tensor_, fulltext_tensor_, keywords_tensor_))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "raw",
   "id": "flush-companion",
   "metadata": {},
   "source": [
    "# we need to get `vocab` and the `tokenizer`, all comes with *AutoTokenizer*\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "model = AutoModel.from_pretrained(MODEL_PATH)\n",
    "\n",
    "# now we can use them\n",
    "json_list_of_dict = json_keyphrase_read( dataset_names[0], json_base_dir )\n",
    "data = data_keyphrase_process( json_list_of_dict, tokenizer )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "changing-clinic",
   "metadata": {},
   "source": [
    "The `data` object is composed by `500 tuples`, each one composed by 4 objects:\n",
    "- `title_tensor_` is the title embedding (composed by integers values)\n",
    "- `abstract_tensor_` is the abstract embedding (composed by integers values)\n",
    "- `fulltext_tensor_` is the fulltext embedding (composed by integers values)\n",
    "- `keywords_tensor_` is the keywords embedding (composed by integers values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "initial-innocent",
   "metadata": {},
   "source": [
    "# 0.2 S2ORC Dataset\n",
    "---\n",
    "\n",
    "## 0.2.1 S2ORC ( jsonl | jsonl.gz ) Loader \n",
    "---\n",
    "First of all we need to manage with the data, to unzip or already unzipped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "modular-provincial",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "README.md  s2orc-full-20200705v1  s2orc-sample-20200705v1  sentence-tranformers\n",
      "keyphrase  s2orc-mini\t\t  scibert\t\t   snli_1.0\n"
     ]
    }
   ],
   "source": [
    "DATA_PATH = DATA_PATH\n",
    "!ls $DATA_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "general-webster",
   "metadata": {},
   "source": [
    "## 0.2.2 Creation (s2orc)\n",
    "---\n",
    "\n",
    "Now we have explored the `S2ORC` structure, we are ready to load the data (starting from the `sample` and following on the `full` folder). The first thing to do is create (as we did before) a method for read the json: `json_s2orc_read`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "practical-scanning",
   "metadata": {},
   "source": [
    "Lets's see what's inside the folder (in this case `metadata` but should be the same for `pdf_parses`:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "selective-valuation",
   "metadata": {},
   "source": [
    "#### a. Intersection\n",
    "---\n",
    "As we want to examinate all `meta_s2orc` and `pdfs_s2orc` files, we need to search the intersection between those files namea and `metadata_output` and `pdf_parses_output` lists."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "under-lawrence",
   "metadata": {},
   "source": [
    "So we can describe the function in charge to load the `jsonl` files. The function must have in input the `generic_path` (f\"{DATA_PATH}/{SAMPLE_FOLDER}\") and then searching in `metadata` and `pdf_parses` for the files present in `file_names`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "authorized-treaty",
   "metadata": {},
   "source": [
    "Important objects are:\n",
    "    \n",
    "- `s2orc_path` ('/home/vivoli/Thesis/data/s2orc-full-20200705v1/full')\n",
    "- `meta_s2orc_path` (f'{s2orc_path}/metadata')\n",
    "- `pdfs_s2orc_path` (f'{s2orc_path}/pdf_parses')\n",
    "- `toread_meta_s2orc` ( ['metadata_0.jsonl.gz', 'metadata_1.jsonl.gz'] )\n",
    "- `toread_pdfs_s2orc` ( ['pdf_parses_0.jsonl.gz', 'pdf_parses_1.jsonl.gz'] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f9cee7e1-4fbd-4c3b-8a9b-bda350319e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need to get `vocab` and the `tokenizer`, all comes with *AutoTokenizer*\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "model = AutoModel.from_pretrained(MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5801c04d-23ea-4b86-88bf-7e5d821a268d",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_length = model.config.max_position_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "26f0486b",
   "metadata": {},
   "outputs": [],
   "source": [
    "toread_meta_s2orc, toread_pdfs_s2orc = dataset_config.memory_save_pipelines()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d90d970d-3ff8-418b-9de0-7c4e104f4a7f",
   "metadata": {},
   "source": [
    "# for everychunk we get an element composed by 4 elements:\n",
    "multichunks_lists = s2orc_multichunk_read(dataset_config, toread_meta_s2orc, toread_pdfs_s2orc, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bulgarian-grain",
   "metadata": {},
   "source": [
    "We have used only the `sample.jsonl` or the pair (`metadata_0.jsonl`-`pdf_parses_0.jsonl`) so we just have one element in the `multichunks_lists`. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "subjective-animation",
   "metadata": {},
   "source": [
    "We have parses all the `metadata` and `pdf_parses` elements, so we have now a dictionary that is composed by:\n",
    "```python\n",
    "json_dict_of_list = {\n",
    "    'metadata': [], \n",
    "    'pdf_parses': {}, \n",
    "    'meta_key_idx': {}, \n",
    "    'pdf_key_idx': {}\n",
    "}\n",
    "```\n",
    "In this dictionary we see:\n",
    "* metadata - `List[dict]` of type `metadata`.\n",
    "* pdf_parses - `List[dict]` of type `pdf_parses`.\n",
    "* meta_key_idx - `dict` with keys: `paper_id` and values: `index` in the metadata list.\n",
    "* pdf_key_idx - `dict` with keys: `paper_id` and values: `index` in the pdf_parses list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5b5e2514-64b4-4d99-8d29-f0e05a44b882",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get dictionary input from config\n",
    "dictionary_input = dataset_config.get_dictionary_input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6c21a7c1-3dce-46b9-a574-5f7a81dc8293",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data': ['abstract'], 'target': ['title'], 'classes': ['mag_field_of_study']}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "aab4b037-bafb-46b0-8d3f-40d0daf2a2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary_columns = sum(dictionary_input.values(), [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4d95cbb3-9afe-45bb-be7d-6ded978efcf0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['abstract', 'title', 'mag_field_of_study']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d2f779b3-125a-41c4-bcda-831821e7459b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2dfecd70963d462eb992a65579488d3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2476ee689ad24445bac7064b392a76d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aeb5fb20fb0e40009a996a6c71c669d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "846eee2436d645aebbe4bb949a075c23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "809025547b124355a4a58cf2df948bf3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc984c764ed2476ab95760c138b139a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99ccb6fe392b4452881da58a18a955ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "len meta single_chunk: 121562\n",
      "len pdfs single_chunk: 51058\n",
      "[TIME] load_dataset: 2.0178585052490234\n",
      "[TIME] dataset_train selection: 2.384185791015625e-07\n",
      "[TIME] remove.indexes: 0.34812331199645996\n",
      "[TIME] remove.concat: 9.560585021972656e-05\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8d5b4297f86428895c45641f1a8ba6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=122.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[TIME] remove.filter: 2.034330368041992\n",
      "[TIME] remove None fields: 2.3826234340667725\n",
      "[TIME] remove.column: 5.0067901611328125e-06\n",
      "[TIME] first [train-(test-val)] split: 0.0033721923828125\n",
      "[TIME] second [test-val] split: 0.00179290771484375\n",
      "[TIME] TOTAL: 4.405852794647217\n",
      "len meta single_chunk: 121126\n",
      "len pdfs single_chunk: 50853\n",
      "[TIME] load_dataset: 2.082895040512085\n",
      "[TIME] dataset_train selection: 2.384185791015625e-07\n",
      "[TIME] remove.indexes: 0.3542666435241699\n",
      "[TIME] remove.concat: 9.274482727050781e-05\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e908f136a9ce46b69dd8c4cd05286be8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=122.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[TIME] remove.filter: 2.026700019836426\n",
      "[TIME] remove None fields: 2.3811447620391846\n",
      "[TIME] remove.column: 4.5299530029296875e-06\n",
      "[TIME] first [train-(test-val)] split: 0.003193378448486328\n",
      "[TIME] second [test-val] split: 0.0017948150634765625\n",
      "[TIME] TOTAL: 4.4692065715789795\n",
      "len meta single_chunk: 121487\n",
      "len pdfs single_chunk: 50529\n",
      "[TIME] load_dataset: 2.072370767593384\n",
      "[TIME] dataset_train selection: 0.0\n",
      "[TIME] remove.indexes: 0.35334277153015137\n",
      "[TIME] remove.concat: 9.1552734375e-05\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e1dfbaf676f4bb7999c040cfb9d9ad7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=122.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[TIME] remove.filter: 1.955249309539795\n",
      "[TIME] remove None fields: 2.3087570667266846\n",
      "[TIME] remove.column: 2.8371810913085938e-05\n",
      "[TIME] first [train-(test-val)] split: 0.0032918453216552734\n",
      "[TIME] second [test-val] split: 0.0017571449279785156\n",
      "[TIME] TOTAL: 4.3863441944122314\n"
     ]
    }
   ],
   "source": [
    "all_datasets = s2ortc_loader(dataset_config, run_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a235e40c-7863-4398-bb7e-b5a32a91e6a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['abstract', 'title', 'mag_field_of_study'],\n",
       "        num_rows: 216042\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['abstract', 'title', 'mag_field_of_study'],\n",
       "        num_rows: 27007\n",
       "    })\n",
       "    valid: Dataset({\n",
       "        features: ['abstract', 'title', 'mag_field_of_study'],\n",
       "        num_rows: 27005\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0baa68a7-8b73-4017-a870-15a1272f7518",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'abstract': ['We develop a new algorithm to perform facial reconstruction from a given skull. This technique has forensic application in helping the identification of skeletal remains when other information is unavailable. Unlike most existing strategies that directly reconstruct the face from the skull, we utilize a database of portrait photos to create many face candidates, then perform a superimposition to get a well matched face, and then revise it according to the superimposition. To support this pipeline, we build an effective autoencoder for image-based facial reconstruction, and a generative model for constrained face inpainting. Our experiments have demonstrated that the proposed pipeline is stable and accurate.',\n",
       "  'Thank you for downloading reporting public affairsproblems and solutions. As you may know, people have search hundreds times for their favorite novels like this reporting public affairsproblems and solutions, but end up in infectious downloads. Rather than reading a good book with a cup of coffee in the afternoon, instead they are facing with some malicious bugs inside their computer. reporting public affairsproblems and solutions is available in our book collection an online access to it is set as public so you can download it instantly. Our digital library saves in multiple locations, allowing you to get the most less latency time to download any of our books like this one. Kindly say, the reporting public affairsproblems and solutions is universally compatible with any devices to read.',\n",
       "  \"In this paper, we discuss the existence of solutions for the 2n-order boundary value problem(-1)^nu^(^2^n^)(t)=f(t,u(t),u''(t),...,u^(^2^n^-^2^)(t)),0R is continuous.\"],\n",
       " 'mag_field_of_study': [['Computer Science'],\n",
       "  ['Computer Science'],\n",
       "  ['Computer Science', 'Mathematics']],\n",
       " 'title': ['Superimposition-guided Facial Reconstruction from Skull',\n",
       "  'Reporting Public Affairsproblems And Solutions',\n",
       "  'Existence of solutions for 2n-order boundary value problem']}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_datasets['train'][:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c1de9972-b641-476f-b819-f92fe1fc5108",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Couldn't find file locally at stsb_multi_mt/stsb_multi_mt.py, or remotely at https://raw.githubusercontent.com/huggingface/datasets/1.5.0/datasets/stsb_multi_mt/stsb_multi_mt.py.\n",
      "The file was picked from the master branch on github instead at https://raw.githubusercontent.com/huggingface/datasets/master/datasets/stsb_multi_mt/stsb_multi_mt.py.\n",
      "Reusing dataset stsb_multi_mt (/home/vivoli/.cache/huggingface/datasets/stsb_multi_mt/en/1.0.0/bc6de0eaa8d97c28a4c22a07e851b05879ae62c60b0b69dd6b331339e8020f07)\n"
     ]
    }
   ],
   "source": [
    "raw_datasets = load_dataset(\"stsb_multi_mt\", name=\"en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0ba26afb-a7ce-43fe-8aa4-cc368e3b834f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'similarity_score'],\n",
       "        num_rows: 5749\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'similarity_score'],\n",
       "        num_rows: 1379\n",
       "    })\n",
       "    dev: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'similarity_score'],\n",
       "        num_rows: 1500\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "fae095e6-ab37-42fb-85af-c716a7aefdaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence1': ['A plane is taking off.',\n",
       "  'A man is playing a large flute.',\n",
       "  'A man is spreading shreded cheese on a pizza.'],\n",
       " 'sentence2': ['An air plane is taking off.',\n",
       "  'A man is playing a flute.',\n",
       "  'A man is spreading shredded cheese on an uncooked pizza.'],\n",
       " 'similarity_score': [5.0, 3.799999952316284, 3.799999952316284]}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets['train'][:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "endless-quality",
   "metadata": {},
   "source": [
    "## Multichunks getDataset( (id, multichunk) | (single_chunk) )\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "racial-frontier",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc3b816b",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FIELD =  [\"title\", \"abstract\"]\n",
    "dataset_dict_test = fuse_dictionaries(multichunks_lists[0], data_field=DATA_FIELD)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "smooth-jonathan",
   "metadata": {},
   "source": [
    "## Multichunks getDatasets\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intense-guest",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer from 'allenai/scibert_scivocab_uncased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "model = BertForMaskedLM.from_pretrained(MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "distinct-tenant",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_length = model.config.max_position_embeddings\n",
    "print(max_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "executed-approach",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time\n",
    "\n",
    "dictionary_input = { \"data\": [\"abstract\"], \"target\": [\"title\"], \"classes\": [\"mag_field_of_study\"]}\n",
    "dictionary_columns = sum(dictionary_input.values(), [])\n",
    "\n",
    "# here we use meta_s2orc for speed, \n",
    "dataset = getDataset(multichunks_lists[0], tokenizer, data_field=dictionary_columns, max_seq_length=max_seq_length, seed=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd5b60b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "noted-washington",
   "metadata": {},
   "source": [
    "## S2ORC Preparation\n",
    "---\n",
    "\n",
    "To build a generic loading function we take inspiration from [here](https://discuss.huggingface.co/t/pipeline-with-custom-dataset-tokenizer-when-to-save-load-manually/1084/11)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "bb4d55e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PAD]: 0\n",
      "[UNK]: 101\n",
      "[SEP]: 103\n",
      "[CLS]: 102\n",
      "0: [PAD]\n",
      "1: [unused0]\n",
      "2: [unused1]\n",
      "99: [unused98]\n",
      "100: [unused99]\n",
      "101: [UNK]\n"
     ]
    }
   ],
   "source": [
    "vocab = tokenizer.get_vocab()\n",
    "print(f\"[PAD]: {vocab['[PAD]']}\")\n",
    "print(f\"[UNK]: {vocab['[UNK]']}\")\n",
    "print(f\"[SEP]: {vocab['[SEP]']}\")\n",
    "print(f\"[CLS]: {vocab['[CLS]']}\")\n",
    "print(f\"0: {tokenizer.convert_ids_to_tokens(0)}\")\n",
    "print(f\"1: {tokenizer.convert_ids_to_tokens(1)}\")\n",
    "print(f\"2: {tokenizer.convert_ids_to_tokens(2)}\")\n",
    "print(f\"99: {tokenizer.convert_ids_to_tokens(99)}\")\n",
    "print(f\"100: {tokenizer.convert_ids_to_tokens(100)}\")\n",
    "print(f\"101: {tokenizer.convert_ids_to_tokens(101)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b1ccb743",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PreTrainedTokenizerFast(name_or_path='allenai/scibert_scivocab_uncased', vocab_size=31090, model_max_len=1000000000000000019884624838656, is_fast=True, padding_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'})"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "royal-ranking",
   "metadata": {},
   "source": [
    "Finally, I found [this](https://huggingface.co/docs/datasets/package_reference/main_classes.html?highlight=datasetdict#datasets.DatasetDict.map) documentation for the function `DatasetDict.map` from the `dataset` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "624dc269",
   "metadata": {},
   "outputs": [],
   "source": [
    "debug = False\n",
    "\n",
    "dataset_map = dataset.map(data_target_preprocess, input_columns= dictionary_columns, fn_kwargs= dictionary_input, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "48c1ae96",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataset_map' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-43-3d3e29e0571b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdataset_map\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'dataset_map' is not defined"
     ]
    }
   ],
   "source": [
    "dataset_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e7b9708",
   "metadata": {},
   "outputs": [],
   "source": [
    "mag_field_dict: Dict = {\n",
    "    \"Medicine\":    0,\n",
    "    \"Biology\":     1,\n",
    "    \"Chemistry\":   2,\n",
    "    \"Engineering\": 4,\n",
    "    \"Computer Science\":    5,\n",
    "    \"Physics\":     6,\n",
    "    \"Materials Science\":     7,\n",
    "    \"Mathematics\":        8,\n",
    "    \"Psychology\":  9,\n",
    "    \"Economics\":   10,\n",
    "    \"Political Science\":    11,\n",
    "    \"Business\":    12,\n",
    "    \"Geology\":     13,\n",
    "    \"Sociology\":   14,\n",
    "    \"Geography\":   15,\n",
    "    \"Environmental Science\":     16,\n",
    "    \"Art\":         17,\n",
    "    \"History\":     18,\n",
    "    \"Philosophy\":  19\n",
    "    # \"null\":         3, \n",
    "}\n",
    "\n",
    "# The key null is actually null, not \"null\":str\n",
    "#\n",
    "#      real_mag_field_value = paper_metadata['mag_field_of_study']\n",
    "#\n",
    "# so we could return the id 3 if it not contained as key of dictionary\n",
    "#\n",
    "#      mag_field_dict.get(real_mag_field_value, 3)\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e31f0ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_mag_map = dataset_map.map(mag_preprocessing, input_columns= dictionary_input['classes'], batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f3af54",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_mag_map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "periodic-shanghai",
   "metadata": {},
   "source": [
    "# Rename it as you want\n",
    "---\n",
    "\n",
    "- `dataset_map.rename_column` ,method for renaming\n",
    "- `dataset_map.set_format`, method for define what columns need to be returned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "progressive-entrance",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_mag_map = dataset_mag_map.rename_column(\"data_input_ids\", \"input_ids\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "respiratory-stock",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_mag_map.set_format(\"torch\", columns=[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "elect-antarctica",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset_mag_map['train'][1]['input_ids'].size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "religious-winter",
   "metadata": {},
   "source": [
    "Then, if you want to store it, it will be stored in the conda environment you are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "everyday-duration",
   "metadata": {},
   "outputs": [],
   "source": [
    "%store dataset_mag_map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "educated-format",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "## ❌ FAKE PIPELINE for train BERT-based NETS\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "specified-chester",
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r dataset_mag_map"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7ff95f00",
   "metadata": {},
   "source": [
    "# tokenizer from 'allenai/scibert_scivocab_uncased'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "model = BertForMaskedLM.from_pretrained(MODEL_PATH)\n",
    "\n",
    "max_seq_length = model.config.max_position_embeddings\n",
    "print(max_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fuzzy-youth",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you print some element from `dataset_map['train'][element_index]['input_ids']` you'll see that lots of element\n",
    "vect = [ele[ele.nonzero()].size(0) for ele in dataset_mag_map['train'][:]['input_ids']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unlike-return",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_vect = max(vect)\n",
    "min_vect = min(vect)\n",
    "sum_vect = sum(vect)\n",
    "len_vect = len(vect)\n",
    "\n",
    "print(f\" max: {max_vect} \\n min: {min_vect} \\n avg: {sum_vect/len_vect}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9adb966",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset_mag_map['train'][:10])"
   ]
  },
  {
   "cell_type": "raw",
   "id": "bcba8721",
   "metadata": {},
   "source": [
    "dataset_mag_map.set_format(\"torch\", columns=[\"input_ids\", \"target_input_ids\", \"mag_index\"])"
   ]
  },
  {
   "cell_type": "raw",
   "id": "39b7a3be",
   "metadata": {},
   "source": [
    "def pad_collate(batch):\n",
    "    batch = np.asarray(batch)\n",
    "    \n",
    "    xx, yy, zz = [], [], []\n",
    "    for elem in batch:\n",
    "        values = list(elem.values())\n",
    "        \n",
    "        x, y, z = values\n",
    "        xx.append(x)\n",
    "        yy.append(y)\n",
    "        zz.append(z)\n",
    "\n",
    "    # print(xx,yy,zz)\n",
    "    \n",
    "    # xx, yy = np.transpose([list(elem.values()) for elem in batch])\n",
    "    if verbose: print(f\"xx: \\n {xx} \\n\\nyy: \\n {yy} \\n\\nzz: \\n {zz}\")\n",
    "    \n",
    "    if debug: print(xx)\n",
    "    x_lens = [x.size() for x in xx]\n",
    "    \n",
    "    if debug: print(yy)\n",
    "    y_lens = [y.size() for y in yy]\n",
    "    \n",
    "    if debug: print(zz)\n",
    "    z_lens = [z.size() for z in zz]\n",
    "    \n",
    "    if verbose: print(f\"x_lens: \\n {x_lens} \\n\\ny_lens: \\n {y_lens} \\n\\nz_lens: \\n {z_lens}\")\n",
    "    \n",
    "    # xx_pad = pad_sequence(xx, batch_first=True, padding_value=0)\n",
    "    # yy_pad = pad_sequence(yy, batch_first=True, padding_value=0)\n",
    "    # zz_pad = pad_sequence(zz, batch_first=True, padding_value=0)\n",
    "\n",
    "    # return xx_pad, yy_pad, x_lens, y_lens\n",
    "    return xx, yy, zz, x_lens, y_lens, z_lens"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5a65ecef",
   "metadata": {},
   "source": [
    "print(debug)\n",
    "print(verbose)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f88c041e",
   "metadata": {},
   "source": [
    "dataset_mag_map.items()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1b2d8938",
   "metadata": {},
   "source": [
    "dataloader_train = DataLoader(dataset_mag_map['train'],\n",
    "                            batch_size=64,\n",
    "                            shuffle=True,\n",
    "                            num_workers=4,\n",
    "                            #collate_fn=pad_collate,\n",
    "                            pin_memory=True)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "aa4177f9",
   "metadata": {},
   "source": [
    "for data in dataloader_train:\n",
    "    print(data)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9c4af5f6",
   "metadata": {},
   "source": [
    "dataset_result = { partition: DataLoader(ds,\n",
    "                                        batch_size=64,\n",
    "                                        shuffle=True,\n",
    "                                        num_workers=4,\n",
    "                                        collate_fn=pad_collate,\n",
    "                                        pin_memory=True) for partition, ds in dataset_mag_map.items()}"
   ]
  },
  {
   "cell_type": "raw",
   "id": "25bb3859",
   "metadata": {},
   "source": [
    "for ele in dataset_result['train']:  \n",
    "    # print(ele.size())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b3812560",
   "metadata": {},
   "source": [
    "for data_, target_, mag_, data_len, target_len, mag_len in dataset_result['train']:  \n",
    "    print(f\"data_len      : {data_len}\\n\")\n",
    "    print(f\"data_         : {data_}\\n\")\n",
    "    print(f\"data_.shape   : {data_.shape}\\n\\n\")\n",
    "    \n",
    "    print(f\"target_len    : {target_len}\\n\")\n",
    "    print(f\"target_       : {target_}\\n\")\n",
    "    print(f\"target_.shape : {target_.shape}\\n\\n\")\n",
    "    \n",
    "    print(f\"mag_len      : {mag_len}\\n\")\n",
    "    print(f\"mag_         : {mag_}\\n\")\n",
    "    print(f\"mag_.shape   : {mag_.shape}\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d9abadc",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "## ❌ FAKE PIPELINE for train BERT-based NETS\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "raw",
   "id": "scheduled-arkansas",
   "metadata": {},
   "source": [
    "for data_ in dataset_map['train']:  \n",
    "    print(f\"data_        : {data_['input_ids'].size()}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "continued-structure",
   "metadata": {},
   "source": [
    "From [here](https://github.com/huggingface/transformers/blob/master/examples/language-modeling/run_mlm.py) you can get an idea from were the code has been borrowed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489144e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data collator\n",
    "# This one will take care of randomly masking the tokens.\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=True, mlm_probability=0.15)\n",
    "\n",
    "train_dataset = dataset_mag_map['train']\n",
    "eval_dataset = dataset_mag_map['valid']\n",
    "\n",
    "# Inizialize TrainerArguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,           # [def.`tmp_trainer`] output directory\n",
    "    num_train_epochs=3,              # [def.   3 ] total # of training epochs\n",
    "    per_device_train_batch_size=16,   # [def.   8 ] batch size per device during training\n",
    "    per_device_eval_batch_size=16,    # [def.   8 ] batch size for evaluation\n",
    "    evaluation_strategy=\"no\",        # [def. 'no'] evaluation is done (and logged) every eval_steps\n",
    "    warmup_steps=0,                  # [def.   0 ] number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0,                  # [def.   0 ] strength of weight decay \n",
    "    learning_rate=5e-4,              # [def. 5e-5] \n",
    "    logging_dir='./logs',            # [def. runs/__id__] directory for storing logs. TensorBoard log directory.\n",
    ")\n",
    "\n",
    "# Initialize our Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c1e4d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "\n",
    "checkpoint = None\n",
    "train_result = trainer.train(resume_from_checkpoint=checkpoint)\n",
    "trainer.save_model()  # Saves the tokenizer too for easy upload\n",
    "metrics = train_result.metrics\n",
    "\n",
    "# max_train_samples = data_args.max_train_samples if data_args.max_train_samples is not None else len(train_dataset)\n",
    "max_train_samples = len(train_dataset)\n",
    "metrics[\"train_samples\"] = min(max_train_samples, len(train_dataset))\n",
    "\n",
    "trainer.log_metrics(\"train\", metrics)\n",
    "trainer.save_metrics(\"train\", metrics)\n",
    "trainer.save_state()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef32f8b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "\n",
    "logger.info(\"*** Evaluate ***\")\n",
    "\n",
    "metrics = trainer.evaluate()\n",
    "\n",
    "# max_val_samples = data_args.max_val_samples if data_args.max_val_samples is not None else len(eval_dataset)\n",
    "max_val_samples = len(eval_dataset)\n",
    "metrics[\"eval_samples\"] = min(max_val_samples, len(eval_dataset))\n",
    "\n",
    "import math\n",
    "perplexity = math.exp(metrics[\"eval_loss\"])\n",
    "metrics[\"perplexity\"] = perplexity\n",
    "\n",
    "trainer.log_metrics(\"eval\", metrics)\n",
    "trainer.save_metrics(\"eval\", metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f6c6f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "opened-sudan",
   "metadata": {},
   "source": [
    "---\n",
    "# 1. Introduction\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "supported-senior",
   "metadata": {},
   "source": [
    "The following datasets were downloaded from the internet (we try to provide links to those we have the right to do so). We divide the dataset based on the task they are mostly used for.\n",
    "\n",
    "## 1.1 Keyphrase task\n",
    "---\n",
    "\n",
    "SOTA: [keyphrase generation](https://arxiv.org/pdf/1704.06879.pdf).\n",
    "\n",
    "The Keyphrase datasets (***duc***, ***Inspect***, ***Krapivin***, ***NUS***, ***SemEval-2010***, ***KP20k dataset***, ***MagKP-CS***) are structured as follow:\n",
    "\n",
    "- title\n",
    "- abstract\n",
    "- fulltext\n",
    "- keywords\n",
    "\n",
    "The only dataset that variates is ***STACKEX*** that instead of having *abstract* and *keywords* has:\n",
    "\n",
    "- question (abstract)\n",
    "- tags (keywords)\n",
    "\n",
    "Here there is a list of the datasets previously cited, with some information:\n",
    "\n",
    "- **duc**, we haven't had much information on this dataset untill now.\n",
    "\n",
    "- **Inspec** [(Hulth, 2003)](https://www.aclweb.org/anthology/W03-1028.pdf), This dataset provides *2,000 paper abstracts*. We adopt the *500 testing* papers and their corresponding uncontrolled keyphrases for evaluation, and the remaining *1,500 papers* are used for *training* the supervised baseline models.\n",
    "\n",
    "- **Krapivin** [(Krapivin et al., 2008)](http://eprints.biblio.unitn.it/1671/1/disi09055-krapivin-autayeu-marchese.pdf): This dataset provides *2,304 papers with full-text* and *author-assigned keyphrases*. However, the author did not mention how to split testing data, so we selected the first *400 papers in alphabetical order as the testing data*, and the *remaining* papers are used to *train* the supervised baselines.\n",
    "\n",
    "- **NUS** [(Nguyen and Kan, 2007)](https://www.comp.nus.edu.sg/~kanmy/papers/icadl2007.pdf): We use both author-assigned and reader-assigned keyphrases and treat *all 211 papers as the testing data*. Since the NUS dataset did not specifically mention the ways of splitting training and testing data, the results of the supervised baseline models are obtained through a *five-fold cross-validation*.\n",
    "\n",
    "- **SemEval-2010** [(Kim et al., 2010)](https://www.aclweb.org/anthology/S10-1004.pdf): 288 articles were collected from the ACM Digital Library. 100 articles were used for testing and the rest were used for training supervised baselines.\n",
    "\n",
    "- **KP20k dataset** [(Meng et al., 2018)](https://arxiv.org/abs/1704.06879): They built a new testing dataset that contains the *titles, abstracts, and keyphrases* of *20,000 scientific articles* in computer science. They were *randomly selected from their obtained 567,830 articles*. Thus they took the 20,000 articles in the validation set to train the supervised baselines.\n",
    "\n",
    "- **MagKP-CS** (from OpenNMT-py and [OpenNMT-kpg-release](https://github.com/memray/OpenNMT-kpg-release)) that is available for download. \n",
    "\n",
    "- **STACKEX** (from [StackExchange](https://archive.org/details/stackexchange)) has been constructed from the computer science forums (CS/AI) at StackExchange using “title” + “body” as source text and “tags” as the target keyphrases. After removing questions without valid tags, they collected 330,965 questions. They randomly selected *16,000 for validation*, and another *16,000 as test set*. Note some questions in StackExchange forums contain large blocks of code, resulting in long texts (sometimes more than 10,000 tokens after tokenization), this is difficult for most neural models to handle. Consequently, the texts have been truncated to 300 tokens and 1,000 tokens for training and evaluation splits respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rural-enough",
   "metadata": {},
   "source": [
    "###### ⚠️ATTENTION\n",
    "> As we aren't going to use the Keyphrase dataset for now, we don't need any custom classes for managing this dataset. We will implement this functions and classes as we go, if there will be the needs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "warming-gates",
   "metadata": {},
   "source": [
    "## 1.2 Sentence embedding task\n",
    "---\n",
    "\n",
    "SOTA: [sBERT](https://arxiv.org/abs/1908.10084)\n",
    "\n",
    "- **SNLI** [(Bowman et al., 2015)](https://arxiv.org/abs/1508.05326) is a collection of *570,000 sentence pairs* annotated with the *labels contradiction, eintailment, and neutral*.\n",
    "\n",
    "- **MultiNLI** [(Williams et al., 2018)](https://arxiv.org/abs/1704.05426) contains *430,000 sentence pairs* and covers a *range of genres of spoken and written text*.\n",
    "\n",
    "- **SciTail** [(allenai)](http://ai2-website.s3.amazonaws.com/publications/scitail-aaai-2018_cameraready.pdf), the entailment dataset consists of 27k. In contrast to the SNLI and MultiNLI, it was not crowd-sourced but created from sentences that already exist “in the wild”. *Hypotheses* were created from *science questions* and the corresponding *answer candidates*, while relevant web sentences from a large corpus were used as premises. Models are evaluated based on accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incorrect-seeking",
   "metadata": {},
   "source": [
    "###### ❌ATTENTION\n",
    "> As we aren't going to use the NLI tasks dataset (for now), we don't need any custom classes for managing this dataset. We will implement this functions and classes as we go, if there will be the needs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "protected-constitutional",
   "metadata": {},
   "source": [
    "## 1.3 Generic NLP tasks\n",
    "---\n",
    "\n",
    "- **S2ORC** [(Lo et al., 2020)](https://github.com/allenai/s2orc) is a large corpus of *81.1M English-language academic papers* spanning many academic disciplines. The corpus consists of *rich metadata, paper abstracts, resolved bibliographic references*, as well as *structured full text for 8.1M open access papers*. Full text is annotated with automatically-detected inline mentions of citations, figures, and tables, each linked to their corresponding paper objects. In S2ORC, they aggregate papers from hundreds of academic publishers and digital archives into a unified source, and create the largest publicly-available collection of machine-readable academic text to date. Built for text mining over academic text.\n",
    "\n",
    "- **OAG** [(Tang et al., 2008)](http://keg.cs.tsinghua.edu.cn/jietang/publications/KDD08-Tang-et-al-ArnetMiner.pdf)  is a large knowledge graph unifying *two billion-scale academic graphs*: Microsoft Academic Graph (**MAG**) and **AMiner**. In mid 2017, they published OAG v1, which contains *166,192,182 papers from MAG and 154,771,162 papers from AMiner* and generated *64,639,608 linking (matching) relations between the two graphs*. This time, in OAG v2, author, venue and newer publication data and the corresponding matchings are available.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "impressive-scanner",
   "metadata": {},
   "source": [
    "###### ✅ATTENTION\n",
    "> We are going to use the S2ORC dataset as it contains full_text data as well as citation/reference informations. It contains also authorship - title - tables data that we will describe below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "activated-amendment",
   "metadata": {},
   "source": [
    "---\n",
    "# 2. S2ORC\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "color-indonesian",
   "metadata": {},
   "source": [
    "## 2.1 Description (s2orc)\n",
    "---\n",
    "The `S2ORC` dataset is in the `data` path under the folder `s2orc-full-20200705v1` (where `s2orc` is the name of the dataset, `full` is the type, as there is also a sample fingerprint; and `20200705v1` is the version). \n",
    "We can reach the data folder exiting by the project and entering in the data folder:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "chinese-simon",
   "metadata": {},
   "source": [
    "DATA_PATH = '/home/vivoli/Thesis/data' \n",
    "!ls $DATA_PATH"
   ]
  },
  {
   "cell_type": "raw",
   "id": "described-homework",
   "metadata": {},
   "source": [
    "s2orc_path = f\"{DATA_PATH}/s2orc-{s2orc_type}-20200705v1/{s2orc_type}\"\n",
    "!ls $s2orc_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "activated-chemistry",
   "metadata": {},
   "source": [
    "As you can see (going into `s2orc-full-20200705v1/full/`) there are the `metadata` folder and the `pdf_parses` folder. The main difference (as we can already get it from the names) is that in the `metadata` you only have some information about the dataset (retrieved from the published metadata), while in the `pdf_parses` you get all the extensive data conteined in the paper (if the paper was present, was correctly parsed and no restriction in the paper data were applied due to limited licence permition). For some reason, the `title` of the paper is contained only in the `metadata` file, but it can get from the `paper_id` field of the paper itself.\n",
    "\n",
    "More information about the `S2ORC` dataset can be read in the [README.md](https://github.com/allenai/s2orc/blob/master/README.md) of the project and in the [project repository](https://github.com/allenai/s2orc/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dangerous-horror",
   "metadata": {},
   "source": [
    "### mag field\n",
    "- MAG fields of study:\n",
    "\n",
    "| Field of study | All papers | Full text |\n",
    "|----------------|------------|-----------|\n",
    "| Medicine       | 12.8M      | 1.8M      |\n",
    "| Biology        | 9.6M       | 1.6M      |\n",
    "| Chemistry      | 8.7M       | 484k      |\n",
    "| n/a            | 7.7M       | 583k      |\n",
    "| Engineering    | 6.3M       | 228k      |\n",
    "| Comp Sci       | 6.0M       | 580k      |\n",
    "| Physics        | 4.9M       | 838k      |\n",
    "| Mat Sci        | 4.6M       | 213k      |\n",
    "| Math           | 3.9M       | 669k      |\n",
    "| Psychology     | 3.4M       | 316k      |\n",
    "| Economics      | 2.3M       | 198k      |\n",
    "| Poli Sci       | 1.8M       | 69k       |\n",
    "| Business       | 1.8M       | 94k       |\n",
    "| Geology        | 1.8M       | 115k      |\n",
    "| Sociology      | 1.6M       | 93k       |\n",
    "| Geography      | 1.4M       | 58k       |\n",
    "| Env Sci        | 766k       | 52k       |\n",
    "| Art            | 700k       | 16k       |\n",
    "| History        | 690k       | 22k       |\n",
    "| Philosophy     | 384k       | 15k       |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hungarian-recommendation",
   "metadata": {},
   "source": [
    "We need now a function that reads all the lines of the `jsonl` files inside both `metadata` and `pdf_parses` folders. Then we'll "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "commercial-continent",
   "metadata": {},
   "source": [
    "## `metadata` schema\n",
    "\n",
    "We recommend everyone work with `metadata/` as the starting point.  This is a JSONlines file (one line per paper) with the following keys:\n",
    "\n",
    "#### Identifier fields\n",
    "\n",
    "* `paper_id`: a `str`-valued field that is a unique identifier for each S2ORC paper.\n",
    "\n",
    "* `arxiv_id`: a `str`-valued field for papers on [arXiv.org](https://arxiv.org).\n",
    "\n",
    "* `acl_id`: a `str`-valued field for papers on [the ACL Anthology](https://www.aclweb.org/anthology/).\n",
    "\n",
    "* `pmc_id`: a `str`-valued field for papers on [PubMed Central](https://www.ncbi.nlm.nih.gov/pmc/articles).\n",
    "\n",
    "* `pubmed_id`: a `str`-valued field for papers on [PubMed](https://pubmed.ncbi.nlm.nih.gov/), which includes MEDLINE.  Also known as `pmid` on PubMed.\n",
    "\n",
    "* `mag_id`: a `str`-valued field for papers on [Microsoft Academic](https://academic.microsoft.com).\n",
    "\n",
    "* `doi`: a `str`-valued field for the [DOI](http://doi.org/).  \n",
    "\n",
    "Notably:\n",
    "\n",
    "* Resolved citation links are represented by the cited paper's `paper_id`.\n",
    "\n",
    "* The `paper_id` resolves to a Semantic Scholar paper page, which can be verified using the `s2_url` field.\n",
    "\n",
    "* We don't always have a value for every identifier field.  When missing, they take `null` value.\n",
    "\n",
    "\n",
    "#### Metadata fields\n",
    "\n",
    "* `title`: a `str`-valued field for the paper title.  Every S2ORC paper *must* have one, though the source can be from publishers or parsed from PDFs.  We prioritize publisher-provided values over parsed values.\n",
    "\n",
    "* `authors`: a `List[Dict]`-valued field for the paper authors.  Authors are listed in order.  Each dictionary has the keys `first`, `middle`, `last`, and `suffix` for the author name, which are all `str`-valued with exception of `middle`, which is a `List[str]`-valued field.  Every S2ORC paper *must* have at least one author.\n",
    "\n",
    "* `venue` and `journal`: `str`-valued fields for the published venue/journal.  *Please note that there is not often agreement as to what constitutes a \"venue\" versus a \"journal\". Consolidating these fields is being considered for future releases.*   \n",
    "\n",
    "* `year`: an `int`-valued field for the published year.  If a paper is preprinted in 2019 but published in 2020, we try to ensure the `venue/journal` and `year` fields agree & prefer non-preprint published info. *We know this decision prohibits certain types of analysis like comparing preprint & published versions of a paper.  We're looking into it for future releases.*  \n",
    "\n",
    "* `abstract`: a `str`-valued field for the abstract.  These are provided directly from gold sources (not parsed from PDFs).  We preserve newline breaks in structured abstracts, which are common in medical papers, by denoting breaks with `':::'`.     \n",
    "\n",
    "* `inbound_citations`: a `List[str]`-valued field containing `paper_id` of other S2ORC papers that cite the current paper.  *Currently derived from PDF-parsed bibliographies, but may have gold sources in the future.*\n",
    "\n",
    "* `outbound_citations`: a `List[str]`-valued field containing `paper_id` of other S2ORC papers that the current paper cites.  Same note as above.   \n",
    "\n",
    "* `has_inbound_citations`: a `bool`-valued field that is `true` if `inbound_citations` has at least one entry, and `false` otherwise.\n",
    "\n",
    "* `has_outbound_citations` a `bool`-valued field that is `true` if `outbound_citations` has at least one entry, and `false` otherwise.\n",
    "\n",
    "We don't always have a value for every metadata field.  When missing, `str` fields take `null` value, while `List` fields are empty lists.\n",
    "\n",
    "#### PDF parse-related metadata fields\n",
    "\n",
    "* `has_pdf_parse`:  a `bool`-valued field that is `true` if this paper has a corresponding entry in `pdf_parses/`, which means we had processed that paper's PDF(s) at some point.  The field is `false` otherwise.\n",
    "\n",
    "* `has_pdf_parsed_abstract`: a `bool`-valued field that is `true` if the paper's PDF parse contains a parsed abstract, and `false` otherwise.   \n",
    "\n",
    "* `has_pdf_parsed_body_text`: a `bool`-valued field that is `true` if the paper's PDF parse contains parsed body text, and `false` otherwise.\n",
    "\n",
    "* `has_pdf_parsed_bib_entries`: a `bool`-valued field that is `true` if the paper's PDF parse contains parsed bibliography entries, and `false` otherwise.\n",
    "\n",
    "* `has_pdf_parsed_ref_entries`: a `bool`-valued field that is `true` if the paper's PDF parse contains parsed reference entries (e.g. tables, figures), and `false` otherwise.\n",
    "\n",
    "Please note:\n",
    "\n",
    "* If `has_pdf_parse = false`, the other four fields will not be present in the JSON (trivially `false`).\n",
    "\n",
    "* If `has_pdf_parse = true` but `has_pdf_parsed_abstract`, `has_pdf_parsed_body_text`, or `has_pdf_parsed_ref_entries` are `false`, this can be because:\n",
    "\n",
    "    * Our PDF parser failed to extract that element\n",
    "    * Our PDF parser succeeded but that paper simply did not have that element (e.g. papers without abstracts)\n",
    "    * Our PDF parser succeeded but that element was removed because the paper is not identified as open-access.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "diverse-humidity",
   "metadata": {},
   "source": [
    "##### metadata_CLASS\n",
    "```python\n",
    "{\n",
    " \"paper_id\": (string), \n",
    " \"title\": (string), \n",
    " \"authors\": [\n",
    "     {\n",
    "         \"first\": (string), \n",
    "         \"middle\": [], \n",
    "         \"last\": (string), \n",
    "         \"suffix\": (string)\n",
    "     },\n",
    "     ...\n",
    "   ]: **Author_Class**, \n",
    " \"abstract\": (string), \n",
    " \"year\": (int), \n",
    " \"arxiv_id\": null, \n",
    " \"acl_id\": null, \n",
    " \"pmc_id\": null, \n",
    " \"pubmed_id\": null, \n",
    " \"doi\": null, \n",
    " \"venue\": null, \n",
    " \"journal\": (string), \n",
    " \"mag_id\": (string-number), \n",
    " \"mag_field_of_study\": [\n",
    "     \"Medicine\",\n",
    "     \"Computer Science\"\n",
    "   ]: **FieldOfStudy_Enum**, \n",
    " \"outbound_citations\": [], \n",
    " \"inbound_citations\": [], \n",
    " \"has_outbound_citations\": false, \n",
    " \"has_inbound_citations\": false, \n",
    " \"has_pdf_parse\": false, \n",
    " \"s2_url\": (string)\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "neural-centre",
   "metadata": {},
   "source": [
    "Here I represent Author_Class as an object of \n",
    "```python\n",
    "{\n",
    "    \"first\": (string), \n",
    "    \"middle\": [], \n",
    "    \"last\": (string), \n",
    "    \"suffix\": (string)\n",
    "}\n",
    "```\n",
    "and `FieldOfStudy_Enum` as an Enum of string such as `[ \"Medicine\", \"Computer Science\", \"Physics\", \"Mathematics\", ... ]`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aggregate-tsunami",
   "metadata": {},
   "source": [
    "\n",
    "## `pdf_parses` schema\n",
    "\n",
    "We view `pdf_parses/` as supplementary to the `metadata/` entries.  PDF parses are also represented as JSONlines file (one line per paper) with the following keys:\n",
    "\n",
    "* `paper_id`: a `str`-valued field which is the same S2ORC paper ID in `metadata/`\n",
    "\n",
    "* `_pdf_hash`: a `str`-valued field.  Internal usage only.  We use this for debugging.\n",
    "\n",
    "* `abstract` and `body_text` are `List[Dict]`-valued fields representing parsed text from the PDF.  Each `Dict` corresponds to a paragraph.  `List` preserves their original ordering.\n",
    "\n",
    "* `bib_entries` and `ref_entries` are `Dict`-valued fields representing extracted entities that can be referenced (inline) within the text.\n",
    "\n",
    "#### example 1\n",
    "\n",
    "One example paragraph in `abstract` or `body_text` might look like:\n",
    "\n",
    "```python\n",
    "{\n",
    "    \"section\": \"Introduction\",\n",
    "    \"text\": \"Dogs are happier cats [13, 15]. See Figure 3 for a diagram.\",\n",
    "    \"cite_spans\": [\n",
    "        {\"start\": 22, \"end\": 25, \"text\": \"[13\", \"ref_id\": \"BIBREF11\"},\n",
    "        {\"start\": 27, \"end\": 30, \"text\": \"15]\", \"ref_id\": \"BIBREF30\"},\n",
    "        ...\n",
    "    ],\n",
    "    \"ref_spans\": [\n",
    "        {\"start\": 36, \"end\": 44, \"text\": \"Figure 3\", \"ref_id\": \"FIGREF2\"},\n",
    "    ]\n",
    "}\n",
    "```\n",
    "\n",
    "and example `bib_entries` and `ref_entries` might look like:\n",
    "\n",
    "```python\n",
    "{\n",
    "    ...,\n",
    "    \"BIBREF11\": {\n",
    "        \"title\": \"Do dogs dream of electric humans?\",\n",
    "        \"authors\": [\n",
    "            {\"first\": \"Lucy\", \"middle\": [\"Lu\"], \"last\": \"Wang\", \"suffix\": \"\"}, \n",
    "            {\"first\": \"Mark\", \"middle\": [], \"last\": \"Neumann\", \"suffix\": \"V\"}\n",
    "        ],\n",
    "        \"year\": \"\", \n",
    "        \"venue\": \"barXiv\",\n",
    "        \"link\": null\n",
    "    },\n",
    "    ...\n",
    "}\n",
    "```\n",
    "\n",
    "```python\n",
    "{\n",
    "    \"TABREF4\": {\n",
    "        \"text\": \"Table 5. Clearly, we achieve SOTA here or something.\",\n",
    "        \"type\": \"table\"\n",
    "    }\n",
    "    ...,\n",
    "    \"FIGREF2\": {\n",
    "        \"text\": \"Figure 3. This is the caption of a pretty figure.\",\n",
    "        \"type\": \"figure\"\n",
    "    },\n",
    "    ...\n",
    "}\n",
    "```\n",
    "\n",
    "Notice: \n",
    "\n",
    "* Inline `spans` are represented by character start and end indices into the paragraph `text`\n",
    "* `spans` resolve to `BIBREF`, `TABREF` or `FIGREF` entries.\n",
    "* `BIBREF` are IDs of bibliographic elements of `bib_entries`.  Bib entries may be missing fields (e.g. `year`).  They can be linked to S2ORC papers, as specified by `link`, but we also preserve any unlinked entries by setting `link` to `null`.\n",
    "* `FIGREF` and `TABREF` are IDs of figure and table elements of `ref_entries`.  Ref entries contain the caption text of the corresponding object, and also indicate the type of object.\n",
    "\n",
    "\n",
    "#### example 2\n",
    "\n",
    "You may see empty `pdf_parses/` JSONs that look like: \n",
    "\n",
    "```python\n",
    "{\n",
    "    \"paper_id\": \"...\", \n",
    "    \"_pdf_hash\": \"...\", \n",
    "    \"abstract\": [], \n",
    "    \"body_text\": [], \n",
    "    \"bib_entries\": {}, \n",
    "    \"ref_entries\": {}\n",
    "}\n",
    "```\n",
    "\n",
    "We keep these around for our internal usage, but the way to interpret these is that there is no usable PDF parse here, despite the corresponding `metadata/` entry still displaying `has_pdf_parse = true`.\n",
    "\n",
    "These exist when (i) `bib_entries` does not successfully parse *and* (ii) the paper is not open-access, so we had to remove `abstract`, `body_text`, and `ref_entries`.   \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "golden-packet",
   "metadata": {},
   "source": [
    "##### pdf_parses_CLASS\n",
    "```python\n",
    "{\n",
    " \"paper_id\": (string), \n",
    " \"_pdf_hash\": (string-number), \n",
    " \"abstract\": [\n",
    "     {\n",
    "         \"section\": (string) \"Abstract\", \n",
    "         \"text\": (string), \n",
    "         \"cite_spans\": [\n",
    "             {\n",
    "                 \"start\": (int), \n",
    "                 \"end\": (int), \n",
    "                 \"text\": (string-number) \"[4, \n",
    "                 \"ref_id\": (string)\n",
    "             }\n",
    "           ]: **CiteSpan_Class**, \n",
    "         \"ref_spans\": []\n",
    "     },\n",
    "     ...\n",
    " ]: **TextSection_Class**, \n",
    " \"body_text\": [], \n",
    " \"bib_entries\": \n",
    "     {\n",
    "         \"BIBREF0\": \n",
    "             {\n",
    "              \"title\": (string), \n",
    "              \"authors\": [\n",
    "                  {\n",
    "                      \"first\": (string), \n",
    "                      \"middle\": [], \n",
    "                      \"last\": (string), \n",
    "                      \"suffix\": (string)\n",
    "                   }\n",
    "                 ], \n",
    "               \"year\": (int), \n",
    "               \"venue\": (string), \n",
    "               \"link\": (string-number)\n",
    "              }, \n",
    "          \"BIBREF1\": \n",
    "              {\n",
    "                  ...\n",
    "              }\n",
    "       }: **BIBREF_Class**, \n",
    " \"ref_entries\": {}\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "broke-complement",
   "metadata": {},
   "source": [
    "Here I represent `TextSection_Class` as an object of \n",
    "```python\n",
    "{\n",
    " \"section\": (string), \n",
    " \"text\": (string), \n",
    " \"cite_spans\": [\n",
    "     {\n",
    "         \"start\": (int), \n",
    "         \"end\": (int), \n",
    "         \"text\": (string-number) \"[4, \n",
    "         \"ref_id\": (string)\n",
    "     }\n",
    "   ], \n",
    " \"ref_spans\": []\n",
    "}\n",
    "```\n",
    "where `CiteSpan_Class` itself is another structured object:\n",
    "```python\n",
    "{\n",
    " \"start\": (int), \n",
    " \"end\": (int), \n",
    " \"text\": (string-number), \n",
    " \"ref_id\": (string)\n",
    "}\n",
    "```\n",
    "and `BIBREF_Class` as dictionary field with `BIBREF_#` as key and related to it an object as follow:\n",
    "```python\n",
    "\"BIBREF_#\": \n",
    " {\n",
    "  \"title\": (string), \n",
    "  \"authors\": [\n",
    "      {\n",
    "          \"first\": (string), \n",
    "          \"middle\": [] (list of string),\n",
    "          \"last\": (string), \n",
    "          \"suffix\": (string)\n",
    "       }\n",
    "     ], \n",
    "   \"year\": (int), \n",
    "   \"venue\": (string), \n",
    "   \"link\": null\n",
    "  }\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "polar-glass",
   "metadata": {},
   "source": [
    "## 2.3 Title Abstract - Full text  (s2orc)\n",
    "---\n",
    "We have loaded the `S2ORC` dataset, created our (one chunk) dataset parses and we want now starting creating our dataset objects (Classes and Loaders).\n",
    "\n",
    "Let's start with the datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hollywood-pitch",
   "metadata": {},
   "source": [
    "### Dataset creation\n",
    "We want to create the datasets for papers' title-abstract and fulltext-(title-abstract) generation. \n",
    "> we'd like also to create a KeyPhrase dataset, we are actualling waiting for the response from the `S2ORC` authors to understand where can we possibly obtain the keyphrases/keywords.\n",
    "\n",
    "In order to do this, we want to create the two datasets (saving them as `jsonl` files).\n",
    "We can organize the data folder as :\n",
    "```bash\n",
    "- data/\n",
    "    # keyphrase dataset \n",
    "    - keyphrase/\n",
    "        # (title - abstract - fulltext - keyphrase)\n",
    "        - s2orc/\n",
    "            - README.md\n",
    "            - chuncks_dataset_idx.json\n",
    "            - train/\n",
    "                - train_0.jsonl\n",
    "                - train_1.jsonl\n",
    "                - ...\n",
    "            - test/\n",
    "                - test_0.jsonl\n",
    "                - test_1.jsonl\n",
    "                - ...\n",
    "            - val/\n",
    "                - val_0.jsonl\n",
    "                - val_1.jsonl\n",
    "                - ...\n",
    "    \n",
    "    # sts datasets\n",
    "    - sts/ \n",
    "        # (title - abstract - cosine_similarity)\n",
    "        - s2orc_partial/\n",
    "            - README.md\n",
    "            - chuncks_dataset_idx.json\n",
    "            - train/\n",
    "                - train_0.jsonl\n",
    "                - train_1.jsonl\n",
    "                - ...\n",
    "            - test/\n",
    "                - test_0.jsonl\n",
    "                - test_1.jsonl\n",
    "                - ...\n",
    "            - val/\n",
    "                - val_0.jsonl\n",
    "                - val_1.jsonl\n",
    "                - ...\n",
    "                \n",
    "        # (title - abstract - fulltext - cosine_similarity)\n",
    "        - s2orc_full/\n",
    "            - README.md\n",
    "            - chuncks_dataset_idx.json\n",
    "            - train/\n",
    "                - train_0.jsonl\n",
    "                - train_1.jsonl\n",
    "                - ...\n",
    "            - test/\n",
    "                - test_0.jsonl\n",
    "                - test_1.jsonl\n",
    "                - ...\n",
    "            - val/\n",
    "                - val_0.jsonl\n",
    "                - val_1.jsonl\n",
    "                - ...\n",
    "```\n",
    "and in the `chuncks_dataset_idx.json` there is the dictionary that maps the `chuncks` (`metadata_{id}.jsonl, pdf_parses_{id}.jsonl for id in range(99)`) into the {train|test|validation}_{id}.\n",
    "\n",
    "A first step to not-using chuncks (neither metadata nor fulltext) anymore is to summarize the data we want into a new python structure (dict) as follow, and save them \n",
    "\n",
    "```python\n",
    "{\n",
    "    \"paper_id\": (string-int), \n",
    "    \"title\":  (string),\n",
    "    \"abstract\": (string), \n",
    "    \"fulltext\": (string), \n",
    "    \"keywords\": List[string],\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collaborative-conviction",
   "metadata": {},
   "source": [
    "1. get the training/validation dataset by extracting Title-Abstract from the `S2ORC` dataset, and getting the testing data from the `KeyPhrase` (*'inspec', 'krapivin', 'nus', 'semeval', 'kp20k', 'duc', 'stackexchange'*) datasets. We should have a pair of sentence (indicativelly a *title* and an *abstract*), possibly a *fulltext* and a *keywords* fields those can be\n",
    "\n",
    "    - completelly related (abstract and its corresponding title)\n",
    "    - someway related (abstract and a field-keyphrase related title {cs+(deep learning; metric learning; nlp; sts;)}\n",
    "    - unrelated but not far away (abstract and a field-**not**keyphrase related title {cs+(nlp; transformer;)-vs-(cv; attention)}\n",
    "    - completelly unrelated (abstract and title are field-keyphrase unrelated {cs+a -vs- phy+z})\n",
    "\n",
    "\n",
    "\n",
    "2. **🤗transformers**, we can see [here](https://huggingface.co/docs/datasets/loading_datasets.html#json-files) the dataset loader (from `jsonl` files) can be used to load train/validation datasets. As we have alrerady load the dataset as dictionary (it is called `multichunks_lists` now, depending on how many chuncks we need to load in one shot) we could also be using the example [here](https://huggingface.co/docs/datasets/loading_datasets.html#from-a-python-dictionary) in order to load the dataset from an existing dictionary. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "clear-values",
   "metadata": {},
   "source": [
    "1. **sentence-transformer**, [sBERT example for train](https://www.sbert.net/docs/training/overview.html#loss-functions) \n",
    "\n",
    "2. **🤗transformers**, we can see [here](https://huggingface.co/docs/datasets/loading_datasets.html#json-files) the dataset loader (from `jsonl` files) can be used to load train/validation datasets. As we have alrerady load the dataset as dictionary (it is called `multichunks_lists` now, depending on how many chuncks we need to load in one shot) we could also be using the example [here](https://huggingface.co/docs/datasets/loading_datasets.html#from-a-python-dictionary) in order to load the dataset from an existing dictionary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cubic-columbia",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# TADataset states for TitleAbstractDataset\n",
    "class TADataset(torch.utils.data.Dataset):\n",
    "    \n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "noted-nomination",
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_id = metadata_dict['paper_id']\n",
    "print(f\"Currently viewing S2ORC paper: {paper_id}\")\n",
    "\n",
    "# suppose we only care about ACL anthology papers\n",
    "if not metadata_dict['acl_id']:\n",
    "    continue\n",
    "\n",
    "# and we want only papers with resolved outbound citations\n",
    "if not metadata_dict['has_outbound_citations']:\n",
    "    continue\n",
    "\n",
    "# get citation context (paragraphs)!\n",
    "if paper_id in paper_id_to_pdf_parse:\n",
    "    # (1) get the full pdf parse from the previously computed lookup dict\n",
    "    pdf_parse = paper_id_to_pdf_parse[paper_id]\n",
    "\n",
    "    # (2) pull out fields we need from the pdf parse, including bibliography & text\n",
    "    bib_entries = pdf_parse['bib_entries']\n",
    "    paragraphs = pdf_parse['abstract'] + pdf_parse['body_text']\n",
    "\n",
    "    # (3) loop over paragraphs, grabbing citation contexts\n",
    "    for paragraph in paragraphs:\n",
    "\n",
    "        # (4) loop over each inline citation in this paragraph\n",
    "        for cite_span in paragraph['cite_spans']:\n",
    "\n",
    "            # (5) each inline citation can be resolved to a bib entry\n",
    "            cited_bib_entry = bib_entries[cite_span['ref_id']]\n",
    "\n",
    "            # (6) that bib entry *may* be linked to a S2ORC paper.  if so, grab paragraph\n",
    "            linked_paper_id = cited_bib_entry['link']\n",
    "            if linked_paper_id:\n",
    "                citation_contexts.append({\n",
    "                    'citing_paper_id': paper_id,\n",
    "                    'cited_paper_id': linked_paper_id,\n",
    "                    'context': paragraph['text'],\n",
    "                    'citation_mention_start': cite_span['start'],\n",
    "                    'citation_mention_end': cite_span['end'],\n",
    "                })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "express-tract",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "accessory-invalid",
   "metadata": {},
   "source": [
    "# 3. Computing Word Embeddings: `Continuous Bag-of-Words`\n",
    "\n",
    "The Continuous Bag-of-Words model (CBOW) is frequently used in NLP deep learning. It is a model that tries to predict words given the context of a few words before and a few words after the target word. This is distinct from language modeling, since CBOW is not sequential and does not have to be probabilistic. Typcially, CBOW is used to quickly train word embeddings, and these embeddings are used to initialize the embeddings of some more complicated model. Usually, this is referred to as pretraining embeddings. It almost always helps performance a couple of percent.\n",
    "\n",
    "The CBOW model is as follows. Given a target word $w_i$ and an $N$ context window on each side, $w_{i−1}, … , w_{i−N}$ and $w_{i+1},…,w_{i+N}$, referring to all context words collectively as $C$, CBOW tries to minimize:\n",
    "\n",
    "\n",
    "$$ −log p(w_i|C) = − log Softmax( A( \\sum_{w∈C}{}{q_w})+b) $$\n",
    "\n",
    "where $q_w$ is the embedding for word $w$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "closing-milan",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTEXT_SIZE = 2  # 2 words to the left, 2 to the right\n",
    "raw_text = \"\"\"We are about to study the idea of a computational process.\n",
    "Computational processes are abstract beings that inhabit computers.\n",
    "As they evolve, processes manipulate other abstract things called data.\n",
    "The evolution of a process is directed by a pattern of rules\n",
    "called a program. People create programs to direct processes. In effect,\n",
    "we conjure the spirits of the computer with our spells.\"\"\".split()\n",
    "\n",
    "# By deriving a set from `raw_text`, we deduplicate the array\n",
    "vocab = set(raw_text)\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "word_to_ix = {word: i for i, word in enumerate(vocab)}\n",
    "data = []\n",
    "for i in range(2, len(raw_text) - 2):\n",
    "    context = [raw_text[i - 2], raw_text[i - 1],\n",
    "               raw_text[i + 1], raw_text[i + 2]]\n",
    "    target = raw_text[i]\n",
    "    data.append((context, target))\n",
    "print(data[:5])\n",
    "\n",
    "\n",
    "class CBOW(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        pass\n",
    "\n",
    "# create your model and train.  here are some functions to help you make\n",
    "# the data ready for use by your module\n",
    "\n",
    "\n",
    "def make_context_vector(context, word_to_ix):\n",
    "    idxs = [word_to_ix[w] for w in context]\n",
    "    return torch.tensor(idxs, dtype=torch.long)\n",
    "\n",
    "\n",
    "make_context_vector(data[0][0], word_to_ix)  # example"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
