{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5179e8ec-dc37-4cc6-a879-ee5ef27af32c",
   "metadata": {},
   "source": [
    "0. [ ] Utils\n",
    "    2. [ ] Dataset config\n",
    "        1. [ ] dataset Name (\"s2orc\", \"keyphrase\", and the others)\n",
    "        2. [ ] create generic DatasetConfig\n",
    "        3. [ ] create s2orc DatasetConfig\n",
    "        4. [ ] create keyphrase DatasetConfig\n",
    "1. [ ] <span style=\"color:red\">S2ORC dataset (train/test)</span>\n",
    "    1. [ ] full/sample\n",
    "    2. [ ] mag_field specification (e.g. \\[\"Computer Science\", \"Phisics\"\\] )\n",
    "    3. [ ] (only for full) chunk ids (e.g. \\[0, 1, 2\\] over the 99 we have downloaded)\n",
    "    4. [ ] data, target, classes (e.g. dictionary_input = { \"data\": \\[\"abstract\"\\], \"target\": \\[\"title\"\\], \"classes\": \\[\"mag_field_of_study\"\\]})\n",
    "2. [ ] <span style=\"color:blue\">KeyPhrase dataset (train)</span>\n",
    "    1. [ ] Find it !\n",
    "3. [ ] <span style=\"color:green\">KeyPhrase dataset (test)</span>\n",
    "    1. [ ] title/abstract/keyphrase\n",
    "    2. [ ] field specification (e.g. \\[\"Computer Science\", \"Phisics\"\\] )\n",
    "4. [ ] <span style=\"color:orange\">Fusion s2orc + keyphrase</span>\n",
    "    1. [ ] having fulltest + keyphrases (same papers title | paper_id | arxive_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d772399-19b7-4f5b-96a2-cd975edb2a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2ee8483-fb89-4ac5-aac8-d3a19a174d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b05a4cf-dc56-4d40-a0fb-c652cfe17cf0",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'utils'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-298e3a171a7e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mload_metric\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeneral\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_dataset_wrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m# parsing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'utils'"
     ]
    }
   ],
   "source": [
    "# All Imports\n",
    "\n",
    "import argparse\n",
    "import logging\n",
    "import os\n",
    "import io\n",
    "import sys\n",
    "import math\n",
    "import random\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# ðŸ¤— Datasets \n",
    "import datasets\n",
    "from datasets import load_dataset, load_metric\n",
    "from utils.general import load_dataset_wrapper\n",
    "\n",
    "# parsing \n",
    "from utils.parsers.args_parser import parse_args\n",
    "\n",
    "# dataset managements\n",
    "# from torch.utils.data.dataloader import DataLoader\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# data managements\n",
    "import json # load/write data\n",
    "import torch \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# dataclasses and types\n",
    "from dataclasses import field, dataclass\n",
    "from typing import Dict, List, Union, Optional\n",
    "\n",
    "# accelerator for speed up experiments/runs\n",
    "from accelerate import Accelerator\n",
    "\n",
    "# ðŸ¤— Tranformers\n",
    "import transformers\n",
    "from transformers import (\n",
    "    AdamW,\n",
    "    AutoTokenizer, \n",
    "    AutoConfig,\n",
    "    HfArgumentParser,\n",
    "    TrainingArguments as HfTrainingArguments,\n",
    "    SchedulerType,\n",
    "    get_scheduler,\n",
    "    set_seed,\n",
    ")\n",
    "    \n",
    "logger = logging.getLogger(__name__)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ee17da89-a08f-4c74-9295-2a843e47cb67",
   "metadata": {},
   "source": [
    "# Imports for mlm\n",
    "\n",
    "\"\"\"\n",
    "Fine-tuning the library models for masked language modeling (BERT, ALBERT, RoBERTa...)\n",
    "on a text file or a dataset without using HuggingFace Trainer.\n",
    "Here is the full list of checkpoints on the hub that can be fine-tuned by this script:\n",
    "https://huggingface.co/models?filter=masked-lm\n",
    "\"\"\"\n",
    "\n",
    "# ðŸ¤— Tranformers\n",
    "from transformers import (\n",
    "    CONFIG_MAPPING,\n",
    "    MODEL_MAPPING,\n",
    "    AutoModel, \n",
    "    AutoModelForMaskedLM,\n",
    "    PreTrainedTokenizer, \n",
    "    DataCollatorForLanguageModeling, \n",
    "    BertForMaskedLM\n",
    ")\n",
    "\n",
    "\n",
    "MODEL_CONFIG_CLASSES = list(MODEL_MAPPING.keys())\n",
    "MODEL_TYPES = tuple(conf.model_type for conf in MODEL_CONFIG_CLASSES)\n",
    "\n",
    "DICTIONARY_FIELD_NAMES = dict(\n",
    "    train         = ['train'],\n",
    "    test          = ['test', 'debug', 'dev'],\n",
    "    validation    = ['validation', 'valid']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "128393ba-39f7-4344-aebb-edc063bc148e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports for glue\n",
    "\n",
    "\"\"\" Finetuning a ðŸ¤— Transformers model for sequence classification on GLUE.\"\"\"\n",
    "\n",
    "# ðŸ¤— Tranformers\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    DataCollatorWithPadding,\n",
    "    PretrainedConfig,\n",
    "    default_data_collator,\n",
    ")\n",
    "\n",
    "task_to_keys = {\n",
    "    \"cola\": (\"sentence\", None),\n",
    "    \"mnli\": (\"premise\", \"hypothesis\"),\n",
    "    \"mrpc\": (\"sentence1\", \"sentence2\"),\n",
    "    \"qnli\": (\"question\", \"sentence\"),\n",
    "    \"qqp\": (\"question1\", \"question2\"),\n",
    "    \"rte\": (\"sentence1\", \"sentence2\"),\n",
    "    \"sst2\": (\"sentence\", None),\n",
    "    \"stsb\": (\"sentence1\", \"sentence2\"),\n",
    "    \"wnli\": (\"sentence1\", \"sentence2\"),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d0a9fee-f357-45fb-a91b-5b8338d8984a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9916725-3c2a-4008-aed1-fe76f09ab9fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a0f32c-d2e5-4a49-9012-f0a443200a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(args_list):\n",
    "    # Pass the args_list to parse_args. We will use it for \n",
    "    # args = parse_args(args_list)\n",
    "    \n",
    "    dataset_args, training_args, s2orc_args, keyph_args, run_args, log_args, embedding_args = parse_args(args_list)\n",
    "    \n",
    "\n",
    "    # ------------------\n",
    "    # Checkpoints\n",
    "    # ------------------\n",
    "    \n",
    "    # Detecting last checkpoint.\n",
    "    last_checkpoint = None\n",
    "    if os.path.isdir(training_args.output_dir) and training_args.do_train and not training_args.overwrite_output_dir:\n",
    "        last_checkpoint = get_last_checkpoint(training_args.output_dir)\n",
    "        if last_checkpoint is None and len(os.listdir(training_args.output_dir)) > 0:\n",
    "            raise ValueError(\n",
    "                f\"Output directory ({training_args.output_dir}) already exists and is not empty. \"\n",
    "                \"Use --overwrite_output_dir to overcome.\"\n",
    "            )\n",
    "        elif last_checkpoint is not None:\n",
    "            logger.info(\n",
    "                f\"Checkpoint detected, resuming training at {last_checkpoint}. To avoid this behavior, change \"\n",
    "                \"the `--output_dir` or add `--overwrite_output_dir` to train from scratch.\"\n",
    "            )  \n",
    "            \n",
    "    # Initialize the accelerator. We will let the accelerator handle device placement for us in this example.\n",
    "    accelerator = Accelerator()\n",
    "    # Make one log on every process with the configuration for debugging.\n",
    "    logging.basicConfig(\n",
    "        format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n",
    "        datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "        level=logging.INFO,\n",
    "    )\n",
    "    logger.info(accelerator.state)\n",
    "\n",
    "    # Setup logging, we only want one process per machine to log things on the screen.\n",
    "    # accelerator.is_local_main_process is only True for one process per machine.\n",
    "    logger.info(f\"Accelerator local main process: {accelerator.is_local_main_process}\")\n",
    "    logger.setLevel(logging.INFO if accelerator.is_local_main_process else logging.ERROR)\n",
    "    if accelerator.is_local_main_process:\n",
    "        datasets.utils.logging.set_verbosity_warning()\n",
    "        transformers.utils.logging.set_verbosity_info()\n",
    "    else:\n",
    "        datasets.utils.logging.set_verbosity_error()\n",
    "        transformers.utils.logging.set_verbosity_error()\n",
    "\n",
    "    # If passed along, set the training seed now.\n",
    "    if training_args.seed is not None:\n",
    "        set_seed(training_args.seed)\n",
    "\n",
    "    # Get the datasets: you can either provide your own CSV/JSON/TXT training and evaluation files (see below)\n",
    "    # or just provide the name of one of the public datasets available on the hub at https://huggingface.co/datasets/\n",
    "    # (the dataset will be downloaded automatically from the datasets Hub).\n",
    "    #\n",
    "    # For CSV/JSON files, this script will use the column called 'text' or the first column if no column called\n",
    "    # 'text' is found. You can easily tweak this behavior (see below).\n",
    "    #\n",
    "    # In distributed training, the load_dataset function guarantee that only one local process can concurrently\n",
    "    # download the dataset.\n",
    "    if dataset_args.dataset_name is not None:\n",
    "        # --------------------- #\n",
    "        #     added  here       #\n",
    "        # --------------------- #\n",
    "        custom_load_dataset = load_dataset_wrapper()\n",
    "        raw_datasets = custom_load_dataset(dataset_args, training_args, s2orc_args, keyph_args, run_args, log_args, embedding_args)\n",
    "    else:\n",
    "        data_files = {}\n",
    "        if dataset_args.train_file is not None:\n",
    "            data_files[\"train\"] = dataset_args.train_file\n",
    "        if dataset_args.validation_file is not None:\n",
    "            data_files[\"validation\"] = dataset_args.validation_file\n",
    "        extension = dataset_args.train_file.split(\".\")[-1]\n",
    "        if extension == \"txt\":\n",
    "            extension = \"text\"\n",
    "        elif extension == \"jsonl\": # jsonl files are file with json element per row\n",
    "            extension = \"json\"\n",
    "        raw_datasets = load_dataset(extension, data_files=data_files)\n",
    "    # See more about loading any type of standard or custom dataset (from files, python dict, pandas DataFrame, etc) at\n",
    "    # https://huggingface.co/docs/datasets/loading_datasets.html.\n",
    "\n",
    "    # Load pretrained model and tokenizer\n",
    "    #\n",
    "    # In distributed training, the .from_pretrained methods guarantee that only one local process can concurrently\n",
    "    # download model & vocab.\n",
    "    if dataset_args.config_name:\n",
    "        config = AutoConfig.from_pretrained(dataset_args.config_name)\n",
    "    elif dataset_args.model_name_or_path:\n",
    "        config = AutoConfig.from_pretrained(dataset_args.model_name_or_path)\n",
    "    else:\n",
    "        config = CONFIG_MAPPING[training_args.model_type]()\n",
    "        logger.warning(\"You are instantiating a new config instance from scratch.\")\n",
    "\n",
    "    if dataset_args.tokenizer_name:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(dataset_args.tokenizer_name, use_fast=not dataset_args.use_slow_tokenizer)\n",
    "    elif dataset_args.model_name_or_path:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(dataset_args.model_name_or_path, use_fast=not dataset_args.use_slow_tokenizer)\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            \"You are instantiating a new tokenizer from scratch. This is not supported by this script.\"\n",
    "            \"You can do it from another script, save it, and load it from here, using --tokenizer_name.\"\n",
    "        )\n",
    "\n",
    "    if dataset_args.model_name_or_path:\n",
    "        model = AutoModelForMaskedLM.from_pretrained(\n",
    "            dataset_args.model_name_or_path,\n",
    "            from_tf=bool(\".ckpt\" in dataset_args.model_name_or_path),\n",
    "            config=config,\n",
    "        )\n",
    "    else:\n",
    "        logger.info(\"Training new model from scratch\")\n",
    "        model = AutoModelForMaskedLM.from_config(config)\n",
    "\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "    # Preprocessing the datasets.\n",
    "    # First we tokenize all the texts.\n",
    "    column_names = raw_datasets[\"train\"].column_names\n",
    "    text_column_name = \"text\" if \"text\" in column_names else column_names[0]\n",
    "\n",
    "    if dataset_args.max_seq_length is None:\n",
    "        max_seq_length = tokenizer.model_max_length\n",
    "        if max_seq_length > 1024:\n",
    "            logger.warning(\n",
    "                f\"The tokenizer picked seems to have a very large `model_max_length` ({tokenizer.model_max_length}). \"\n",
    "                \"Picking 1024 instead. You can change that default value by passing --max_seq_length xxx.\"\n",
    "            )\n",
    "            max_seq_length = 1024\n",
    "    else:\n",
    "        if dataset_args.max_seq_length > tokenizer.model_max_length:\n",
    "            logger.warning(\n",
    "                f\"The max_seq_length passed ({dataset_args.max_seq_length}) is larger than the maximum length for the\"\n",
    "                f\"model ({tokenizer.model_max_length}). Using max_seq_length={tokenizer.model_max_length}.\"\n",
    "            )\n",
    "        max_seq_length = min(dataset_args.max_seq_length, tokenizer.model_max_length)\n",
    "\n",
    "    if dataset_args.line_by_line:\n",
    "        # When using line_by_line, we just tokenize each nonempty line.\n",
    "        padding = \"max_length\" if dataset_args.pad_to_max_length else False\n",
    "\n",
    "        def tokenize_function(examples):\n",
    "            # Remove empty lines\n",
    "            examples[\"text\"] = [line for line in examples[\"text\"] if len(line) > 0 and not line.isspace()]\n",
    "            return tokenizer(\n",
    "                examples[\"text\"],\n",
    "                padding=padding,\n",
    "                truncation=True,\n",
    "                max_length=max_seq_length,\n",
    "                # We use this option because DataCollatorForLanguageModeling (see below) is more efficient when it\n",
    "                # receives the `special_tokens_mask`.\n",
    "                return_special_tokens_mask=True,\n",
    "            )\n",
    "\n",
    "        tokenized_datasets = raw_datasets.map(\n",
    "            tokenize_function,\n",
    "            batched=True,\n",
    "            num_proc=dataset_args.preprocessing_num_workers,\n",
    "            remove_columns=[text_column_name],\n",
    "            load_from_cache_file=not dataset_args.overwrite_cache,\n",
    "        )\n",
    "    else:\n",
    "        # Otherwise, we tokenize every text, then concatenate them together before splitting them in smaller parts.\n",
    "        # We use `return_special_tokens_mask=True` because DataCollatorForLanguageModeling (see below) is more\n",
    "        # efficient when it receives the `special_tokens_mask`.\n",
    "        def tokenize_function(examples):\n",
    "            return tokenizer(examples[text_column_name], return_special_tokens_mask=True)\n",
    "\n",
    "        tokenized_datasets = raw_datasets.map(\n",
    "            tokenize_function,\n",
    "            batched=True,\n",
    "            num_proc=dataset_args.preprocessing_num_workers,\n",
    "            remove_columns=column_names,\n",
    "            load_from_cache_file=not dataset_args.overwrite_cache,\n",
    "        )\n",
    "\n",
    "        # Main data processing function that will concatenate all texts from our dataset and generate chunks of\n",
    "        # max_seq_length.\n",
    "        def group_texts(examples):\n",
    "            # Concatenate all texts.\n",
    "            concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "            total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "            # We drop the small remainder, we could add padding if the model supported it instead of this drop, you can\n",
    "            # customize this part to your needs.\n",
    "            total_length = (total_length // max_seq_length) * max_seq_length\n",
    "            # Split by chunks of max_len.\n",
    "            result = {\n",
    "                k: [t[i : i + max_seq_length] for i in range(0, total_length, max_seq_length)]\n",
    "                for k, t in concatenated_examples.items()\n",
    "            }\n",
    "            return result\n",
    "\n",
    "        # Note that with `batched=True`, this map processes 1,000 texts together, so group_texts throws away a\n",
    "        # remainder for each of those groups of 1,000 texts. You can adjust that batch_size here but a higher value\n",
    "        # might be slower to preprocess.\n",
    "        #\n",
    "        # To speed up this part, we use multiprocessing. See the documentation of the map method for more information:\n",
    "        # https://huggingface.co/docs/datasets/package_reference/main_classes.html#datasets.Dataset.map\n",
    "\n",
    "        tokenized_datasets = tokenized_datasets.map(\n",
    "            group_texts,\n",
    "            batched=True,\n",
    "            num_proc=dataset_args.preprocessing_num_workers,\n",
    "            load_from_cache_file=not dataset_args.overwrite_cache,\n",
    "        )\n",
    "\n",
    "    train_dataset = tokenized_datasets[\"train\"]\n",
    "    eval_dataset = tokenized_datasets[\"validation\"]\n",
    "\n",
    "    # Log a few random samples from the training set:\n",
    "    for index in random.sample(range(len(train_dataset)), 3):\n",
    "        logger.info(f\"Sample {index} of the training set: {train_dataset[index]}.\")\n",
    "\n",
    "    # Data collator\n",
    "    # This one will take care of randomly masking the tokens.\n",
    "    data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False, mlm_probability=dataset_args.mlm_probability)\n",
    "\n",
    "    # DataLoaders creation:\n",
    "    train_dataloader = DataLoader(\n",
    "        train_dataset, shuffle=True, collate_fn=data_collator, batch_size=training_args.per_device_train_batch_size\n",
    "    )\n",
    "    eval_dataloader = DataLoader(eval_dataset, collate_fn=data_collator, batch_size=training_args.per_device_eval_batch_size)\n",
    "\n",
    "    # Optimizer\n",
    "    # Split weights in two groups, one with weight decay and the other not.\n",
    "    no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "    optimizer_grouped_parameters = [\n",
    "        {\n",
    "            \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "            \"weight_decay\": args.weight_decay,\n",
    "        },\n",
    "        {\n",
    "            \"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "            \"weight_decay\": 0.0,\n",
    "        },\n",
    "    ]\n",
    "    optimizer = AdamW(optimizer_grouped_parameters, lr=training_args.learning_rate)\n",
    "\n",
    "    # Prepare everything with our `accelerator`.\n",
    "    model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(\n",
    "        model, optimizer, train_dataloader, eval_dataloader\n",
    "    )\n",
    "\n",
    "    # Note -> the training dataloader needs to be prepared before we grab his length below (cause its length will be\n",
    "    # shorter in multiprocess)\n",
    "\n",
    "    # Scheduler and math around the number of training steps.\n",
    "    num_update_steps_per_epoch = math.ceil(len(train_dataloader) / args.gradient_accumulation_steps)\n",
    "    if training_args.max_train_steps is None:\n",
    "        training_args.max_train_steps = training_args.num_train_epochs * num_update_steps_per_epoch\n",
    "    else:\n",
    "        training_args.num_train_epochs = math.ceil(training_args.max_train_steps / num_update_steps_per_epoch)\n",
    "\n",
    "    lr_scheduler = get_scheduler(\n",
    "        name=training_args.lr_scheduler_type,\n",
    "        optimizer=optimizer,\n",
    "        num_warmup_steps=training_args.num_warmup_steps,\n",
    "        num_training_steps=training_args.max_train_steps,\n",
    "    )\n",
    "\n",
    "    # Train!\n",
    "    total_batch_size = training_args.per_device_train_batch_size * accelerator.num_processes * training_args.gradient_accumulation_steps\n",
    "\n",
    "    logger.info(\"***** Running training *****\")\n",
    "    logger.info(f\"  Num examples = {len(train_dataset)}\")\n",
    "    logger.info(f\"  Num Epochs = {training_args.num_train_epochs}\")\n",
    "    logger.info(f\"  Instantaneous batch size per device = {training_args.per_device_train_batch_size}\")\n",
    "    logger.info(f\"  Total train batch size (w. parallel, distributed & accumulation) = {total_batch_size}\")\n",
    "    logger.info(f\"  Gradient Accumulation steps = {training_args.gradient_accumulation_steps}\")\n",
    "    logger.info(f\"  Total optimization steps = {training_args.max_train_steps}\")\n",
    "    # Only show the progress bar once on each machine.\n",
    "    progress_bar = tqdm(range(training_args.max_train_steps), disable=not accelerator.is_local_main_process)\n",
    "    completed_steps = 0\n",
    "\n",
    "    for epoch in range(training_args.num_train_epochs):\n",
    "        model.train()\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss\n",
    "            loss = loss / training_args.gradient_accumulation_steps\n",
    "            accelerator.backward(loss)\n",
    "            if step % training_args.gradient_accumulation_steps == 0 or step == len(train_dataloader) - 1:\n",
    "                optimizer.step()\n",
    "                lr_scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "                progress_bar.update(1)\n",
    "                completed_steps += 1\n",
    "\n",
    "            if completed_steps >= training_args.max_train_steps:\n",
    "                break\n",
    "\n",
    "        model.eval()\n",
    "        losses = []\n",
    "        for step, batch in enumerate(eval_dataloader):\n",
    "            with torch.no_grad():\n",
    "                outputs = model(**batch)\n",
    "\n",
    "            loss = outputs.loss\n",
    "            losses.append(accelerator.gather(loss.repeat(training_args.per_device_eval_batch_size)))\n",
    "\n",
    "        losses = torch.cat(losses)\n",
    "        losses = losses[: len(eval_dataset)]\n",
    "        perplexity = math.exp(torch.mean(losses))\n",
    "\n",
    "        logger.info(f\"epoch {epoch}: perplexity: {perplexity}\")\n",
    "\n",
    "    if args.output_dir is not None:\n",
    "        accelerator.wait_for_everyone()\n",
    "        unwrapped_model = accelerator.unwrap_model(model)\n",
    "        unwrapped_model.save_pretrained(training_args.output_dir, save_function=accelerator.save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c83995-a704-4f2b-8af8-7202757910cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    args_list = [\n",
    "\n",
    "        # DatasetArguments\n",
    "        \"--model_name_or_path\"  , \"allenai/scibert_scivocab_uncased\",\n",
    "        \"--dataset_name\"        , \"s2orc\",\n",
    "        \"--dataset_config_name\" , \"full\",\n",
    "        \n",
    "        # \"--dataset_name\"        , \"keyphrase\",\n",
    "        # \"--dataset_config_name\" , \"inspect\",\n",
    "        \n",
    "        \n",
    "        # TrainingArguments        \n",
    "        \"--seed\"                , '1234', # seed for reproducibility of experiments\n",
    "        \"--output_dir\"          , \"output\",\n",
    "        \"--run_name\"            , \"scibert-s2orc\",\n",
    "        \"--debug\"               , 'False',\n",
    "        \n",
    "        # S2orcArguments & KeyPhArguments\n",
    "        \"--dataset_path\"        , \"/home/vivoli/Thesis/data\",\n",
    "        \n",
    "        # S2orcArguments\n",
    "        \"--idxs\"                , '0',\n",
    "        \"--only_extrated\"       , 'False',\n",
    "        \"--keep_extracted\"      , 'False',\n",
    "        \"--mag_field_of_study\"  , \"Computer Science\",    # list\n",
    "        \"--data\"                , \"abstract\",            # list\n",
    "        \"--target\"              , \"title\",               # list\n",
    "        \"--classes\"             , \"mag_field_of_study\",  # list\n",
    "        \"--keep_none_papers\"    , 'False',\n",
    "        \"--keep_unused_columns\" , 'False',\n",
    "        \n",
    "        # RunArguments\n",
    "        # \"--run_name\"            , \"scibert-s2orc\",\n",
    "        \"--run_number\"          , '0',\n",
    "        \"--run_iteration\"       , '0',\n",
    "        \n",
    "        # LoggingArguments\n",
    "        \"--verbose\"             , 'True',\n",
    "        # \"--debug\"               , False,\n",
    "        \"--time\"                , 'False',\n",
    "        \"--callback\"            , \"WandbCallback\",\n",
    "        \n",
    "        # EmbeddingArguments\n",
    "        \"--max_seq_length\"      , '512',\n",
    "        # \"--pooling\"             , 'none',\n",
    "        # \"--batch_size\"          , '32'\n",
    "    ]\n",
    "    \n",
    "    main(args_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f8811f9-6b8d-4199-baef-319eac7193e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a40971-654a-43ae-a609-7ad13c4c4482",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b857c6-5e03-4d24-8df5-1984573e4c1c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
