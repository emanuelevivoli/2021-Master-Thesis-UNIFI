{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0046c39f",
   "metadata": {},
   "source": [
    "Change background color for output as it wasn't distinguishable from the Markdown text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a9fdfc56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".output_wrapper .output .output_area .output_subarea {\n",
       "    background: #E0FFFF\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<style>\n",
    ".output_wrapper .output .output_area .output_subarea {\n",
    "    background: #E0FFFF\n",
    "}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "smooth-studio",
   "metadata": {},
   "source": [
    "# Datasets\n",
    "---\n",
    "\n",
    "In this notebook we'll build/implement the Dataset classes we need to work with all the dataset we have.\n",
    "First we will introduce the datasets, then we will separate those based on the usage we are going to make of them, then we will use/build/implement our classes in order to manage those different datasets and tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rolled-latter",
   "metadata": {},
   "source": [
    "# 0.0 Utils\n",
    "---\n",
    "\n",
    "We will be using the 🤗*Datasets* library, the 🤗 *Tranformers* library, as we need a tokenizer and a vocab and we'll be using (for loggin) Weigths and Biases (`wandb`) so we are going to install it, independently from Hugging face, and use it within it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "outdoor-organizer",
   "metadata": {},
   "source": [
    "Let's define all the `imports` and `hyperparameters` in one place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "viral-flesh",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------- #\n",
    "#           Imports\n",
    "# ----------------------------------- #\n",
    "import os # generic\n",
    "import time # logging\n",
    "from tqdm.auto import tqdm # custom progress bar\n",
    "import enlighten # another custom progress bar\n",
    "import io\n",
    "import json # load/write data\n",
    "import torch \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# 🤗 Datasets\n",
    "from datasets import (\n",
    "    load_dataset, \n",
    "    DatasetDict, \n",
    "    Dataset as hfDataset\n",
    ")\n",
    "\n",
    "# 🤗 Tranformers\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModel, \n",
    "    PreTrainedTokenizer, \n",
    "    DataCollatorForLanguageModeling, \n",
    "    Trainer, \n",
    "    BertForMaskedLM,\n",
    "    TrainingArguments\n",
    ")\n",
    "\n",
    "# Padding\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# data types\n",
    "from torch.utils.data import (\n",
    "    Dataset, \n",
    "    DataLoader\n",
    ")\n",
    "from typing import (\n",
    "    Dict, List, Union\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "average-society",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------- #\n",
    "#           Hyperparameters\n",
    "# ----------------------------------- #\n",
    "\n",
    "# --------- dataset         --------- #\n",
    "\n",
    "\n",
    "\n",
    "# --------- logging         --------- #\n",
    "# generic\n",
    "verbose = True\n",
    "debug = False\n",
    "wandb_flag = False\n",
    "\n",
    "# Logging on Weigths and Biases\n",
    "if wandb_flag:\n",
    "    import wandb\n",
    "    # wandb\n",
    "    wandb.login()\n",
    "    wandb.init(project=\"datasets.explanation\")\n",
    "    # Optional: log both gradients and parameters\n",
    "    %env WANDB_WATCH=all\n",
    "\n",
    "# --------- preprocessing   --------- #\n",
    "# in **partial_prepare_data**\n",
    "remove_None_papers = True # if True, remove papers with None eather in abstract or title\n",
    "remove_Unused_columns = True\n",
    "# in **preprocess**\n",
    "clean_None_data = False # if True, changes all the None (abstract of title) to ''\n",
    "remove_None_data = False # if True (and clean_None_data set False), remove all the None abstract/title and the correspond title/abstract\n",
    "\n",
    "# --------- paths           --------- #\n",
    "# data folder path\n",
    "data_base_dir = '/home/vivoli/Thesis/data'\n",
    "s2orc_type = 'full'\n",
    "N = None\n",
    "\n",
    "# --------- model/tokenizer --------- #\n",
    "# hugginface model/tokenizer name\n",
    "MODEL_PATH = 'allenai/scibert_scivocab_uncased'\n",
    "RUN_NAME   = 'scibert-s2orc'\n",
    "RUN_NUMBER = 2\n",
    "RUN_ITER   = 1\n",
    "\n",
    "output_dir=f'./tmp_trainer/#{RUN_NUMBER}_{RUN_ITER}_{RUN_NAME}'\n",
    "# seed for reproducibility of experiments\n",
    "SEED = 1234"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "scheduled-color",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "datasets.explanation - WARNING - This is a warning\n",
      "datasets.explanation - ERROR - This is an error\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------- #\n",
    "#           Logging\n",
    "# ----------------------------------- #\n",
    "LOGS_PATH = 'logs'\n",
    "import logging\n",
    "\n",
    "# Create a custom logger\n",
    "logger = logging.getLogger(\"datasets.explanation\")\n",
    "\n",
    "# Create handlers\n",
    "c_handler = logging.StreamHandler()\n",
    "f_handler = logging.FileHandler(f'{LOGS_PATH}/file.log')\n",
    "d_handler = logging.FileHandler(f'{LOGS_PATH}/debug.log')\n",
    "\n",
    "c_handler.setLevel(logging.DEBUG if verbose else logging.WARNING) # verbose is to log everything\n",
    "f_handler.setLevel(logging.ERROR)\n",
    "d_handler.setLevel(logging.DEBUG)\n",
    "\n",
    "# Create formatters and add it to handlers\n",
    "c_format = logging.Formatter('%(name)s - %(levelname)s - %(message)s')\n",
    "f_format = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "d_format = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "\n",
    "c_handler.setFormatter(c_format)\n",
    "f_handler.setFormatter(f_format)\n",
    "d_handler.setFormatter(d_format)\n",
    "\n",
    "# Add handlers to the logger\n",
    "logger.addHandler(c_handler)\n",
    "logger.addHandler(f_handler)\n",
    "logger.addHandler(d_handler)\n",
    "\n",
    "logger.warning('This is a warning')\n",
    "logger.error('This is an error')\n",
    "logger.info('This is an info')\n",
    "logger.debug('This is a debug')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "timely-attitude",
   "metadata": {},
   "source": [
    "# 0.1 KeyPhrase Dataset\n",
    "---\n",
    "\n",
    "These are testing datasets"
   ]
  },
  {
   "cell_type": "raw",
   "id": "hidden-kazakhstan",
   "metadata": {},
   "source": [
    "dataset_names = ['inspec', 'krapivin', 'nus', 'semeval', 'kp20k', 'duc', 'stackexchange']\n",
    "json_base_dir = data_base_dir + '/keyphrase/json/'"
   ]
  },
  {
   "cell_type": "raw",
   "id": "returning-spine",
   "metadata": {},
   "source": [
    "def json_keyphrase_read( dataset_name, json_base_dir, file_name=None ):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        dataset_name (string): Directory name with the json file.\n",
    "        json_base_dir (string): Path to the Dataset directory.\n",
    "        file_name (string): (Optional) Json file name.\n",
    "        \n",
    "    Return:\n",
    "        json_list (list of dict): List of dictionaries, each one with the fields \n",
    "            - 'title' (string)\n",
    "            - 'abstract' (string)\n",
    "            - 'fulltext' (string | '')\n",
    "            - 'keywords' (list)\n",
    "    \"\"\"\n",
    "    logger.debug(dataset_name)\n",
    "\n",
    "    input_json_path = os.path.join(json_base_dir, dataset_name, \n",
    "                                   '%s_test.json' % dataset_name if file_name is None else file_name)\n",
    "\n",
    "    json_list_of_dict = []\n",
    "    with open(input_json_path, 'r') as input_json:\n",
    "        for json_line in input_json:\n",
    "            json_dict = json.loads(json_line)\n",
    "\n",
    "            if dataset_name == 'stackexchange':\n",
    "                json_dict['abstract'] = json_dict['question']\n",
    "                json_dict['keywords'] = json_dict['tags']            \n",
    "                del json_dict['question']\n",
    "                del json_dict['tags']\n",
    "\n",
    "            keywords = json_dict['keywords']\n",
    "\n",
    "            if isinstance(keywords, str):\n",
    "                keywords = keywords.split(';')\n",
    "                json_dict['keywords'] = keywords\n",
    "\n",
    "            json_list_of_dict.append(json_dict)\n",
    "                \n",
    "    return json_list_of_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "liberal-atlantic",
   "metadata": {},
   "source": [
    "This keyphrase dataset could be useful for testing some model on keyphrase task or abstract-title summarization/generation/embedding.\n",
    "\n",
    "For now, we can avoid implementing the Dataset's and DataLoader's classes for this objects.\n",
    "\n",
    "Although, the dataset and Dataloader would be simple as follow:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "infinite-capture",
   "metadata": {},
   "source": [
    "def data_keyphrase_process(json_list_of_dict, tokenizer, debug=False):\n",
    "    data = []\n",
    "    for json_dict in json_list_of_dict:\n",
    "        title_tensor_ = torch.tensor(tokenizer.encode(json_dict['title']),\n",
    "                                dtype=torch.long)\n",
    "        if debug: print(title_tensor_)\n",
    "        abstract_tensor_ = torch.tensor(tokenizer.encode(json_dict['abstract']),\n",
    "                                dtype=torch.long)\n",
    "        if debug: print(abstract_tensor_)\n",
    "        fulltext_tensor_ = torch.tensor(tokenizer.encode(json_dict['fulltext']),\n",
    "                                dtype=torch.long)\n",
    "        if debug: print(fulltext_tensor_)\n",
    "        keywords_tensor_ = torch.tensor(tokenizer(json_dict['keywords'], padding=True)['input_ids'], \n",
    "                                dtype=torch.long)\n",
    "        if debug: print(keywords_tensor_)\n",
    "\n",
    "        data.append((title_tensor_, abstract_tensor_, fulltext_tensor_, keywords_tensor_))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "raw",
   "id": "flush-companion",
   "metadata": {},
   "source": [
    "# we need to get `vocab` and the `tokenizer`, all comes with *AutoTokenizer*\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "model = AutoModel.from_pretrained(MODEL_PATH)\n",
    "\n",
    "# now we can use them\n",
    "json_list_of_dict = json_keyphrase_read( dataset_names[0], json_base_dir )\n",
    "data = data_keyphrase_process( json_list_of_dict, tokenizer )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "changing-clinic",
   "metadata": {},
   "source": [
    "The `data` object is composed by `500 tuples`, each one composed by 4 objects:\n",
    "- `title_tensor_` is the title embedding (composed by integers values)\n",
    "- `abstract_tensor_` is the abstract embedding (composed by integers values)\n",
    "- `fulltext_tensor_` is the fulltext embedding (composed by integers values)\n",
    "- `keywords_tensor_` is the keywords embedding (composed by integers values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "initial-innocent",
   "metadata": {},
   "source": [
    "# 0.2 S2ORC Dataset\n",
    "---\n",
    "\n",
    "## 0.2.1 S2ORC ( jsonl | jsonl.gz ) Loader \n",
    "---\n",
    "First of all we need to manage with the data, to unzip or already unzipped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "modular-provincial",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "DATA_PATH = '/home/vivoli/Thesis/data' \n",
    "!ls $DATA_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "revised-translation",
   "metadata": {},
   "outputs": [],
   "source": [
    "s2orc_path = f\"{DATA_PATH}/s2orc-{s2orc_type}-20200705v1/{s2orc_type}\"\n",
    "!ls $s2orc_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quality-valuable",
   "metadata": {},
   "outputs": [],
   "source": [
    "args:Dict = dict({\n",
    "    'range': {\n",
    "        'N': 3,\n",
    "        'to': True, # N is int, this must be true\n",
    "        'into': False, # N is tuple, or N is two element list but 'into' must be True\n",
    "    },\n",
    "\n",
    "    'only_extrated': False,\n",
    "    'keep_extracted': False,\n",
    "    \n",
    "    'mag_field_of_study': ['Computer Science'] # options are empty List() or List(string)\n",
    "\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "relevant-knitting",
   "metadata": {},
   "outputs": [],
   "source": [
    "if s2orc_type == 'sample':\n",
    "    metadata_filenames = [ f\"sample\" ]\n",
    "    pdf_parses_filenames = [ f\"sample\"]\n",
    "\n",
    "    if N is not None:\n",
    "        logging.warning(f\"You set 'sample' but you also set `N` for full bucket range. \\n The N selection will be discarded as only `sample` element will be used.\")\n",
    "        N = args['range']['N'] = 0\n",
    "        list_range = [N]\n",
    "    \n",
    "elif s2orc_type == 'full':\n",
    "    N = args['range']['N']\n",
    "\n",
    "    if N is None:\n",
    "        logging.warning(f\"You set 'full' but no bucket index was specified. \\n We'll use the index 0, so the first bucket will be used.\")\n",
    "        N = args['range']['N'] = 0\n",
    "        list_range = [N]\n",
    "        \n",
    "    elif type(N) is list:\n",
    "        if args['range']['into']:\n",
    "            list_range = range(N[0], N[1]) \n",
    "            logging.warning(f\"The range is intended as [{N[0]}, {N[1]}] (start {N[0]}, end {N[1]})\")\n",
    "        else:\n",
    "            list_range = N\n",
    "            logging.warning(f\"The element list is intended as: {N}\")\n",
    "        \n",
    "    elif type(N) is int:\n",
    "        if args['range']['to']:\n",
    "            list_range = range(0,N)\n",
    "            logging.warning(f\"The range is intended as [ 0, {N}] (start 0, end {N})\")\n",
    "        else:\n",
    "            list_range = [N]\n",
    "            logging.warning(f\"The element list is intended as: [{N}]\")\n",
    "            \n",
    "    metadata_filenames = [ f\"metadata_{n}\" for n in list_range ]\n",
    "    pdf_parses_filenames = [ f\"pdf_parses_{n}\" for n in list_range ]\n",
    "\n",
    "else:\n",
    "    raise NameError(f\"You must select an existed S2ORC dataset \\n \\\n",
    "                You selected {s2orc_type}, but options are ['sample' or 'full']\")\n",
    "\n",
    "extention = 'jsonl' if args['only_extrated'] else 'jsonl.gz'\n",
    "\n",
    "# unpossible option\n",
    "if extention is None: \n",
    "    message = f\"Extention cannot be None, options: \\n - *jsonl* if only_extrated True \\n - *jsonl.gz* if only_extrated False\"\n",
    "    logging.error(message)\n",
    "    raise RuntimeError(message)\n",
    "\n",
    "# we could also leave {s2orc_path} for later, if path's lenght is big\n",
    "meta_s2orc_path = f'{s2orc_path}/metadata'\n",
    "pdfs_s2orc_path = f'{s2orc_path}/pdf_parses'\n",
    "meta_s2orc = [f'{metadata_filename}.{extention}' for metadata_filename in metadata_filenames]\n",
    "pdfs_s2orc = [f'{pdf_parses_filename}.{extention}' for pdf_parses_filename in pdf_parses_filenames]\n",
    "\n",
    "logging.info(f\"meta_s2orc len: {len(meta_s2orc)}\")\n",
    "logging.info(f\"pdfs_s2orc len: {len(pdfs_s2orc)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "general-webster",
   "metadata": {},
   "source": [
    "## 0.2.2 Creation (s2orc)\n",
    "---\n",
    "\n",
    "Now we have explored the `S2ORC` structure, we are ready to load the data (starting from the `sample` and following on the `full` folder). The first thing to do is create (as we did before) a method for read the json: `json_s2orc_read`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "running-albany",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(f'{s2orc_type}_meta:\\n {meta_s2orc} \\n')\n",
    "print(f'{s2orc_type}_pdfs:\\n {pdfs_s2orc} \\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "express-fountain",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'meta_s2orc_path: {meta_s2orc_path}')\n",
    "print(f'pdfs_s2orc_path: {pdfs_s2orc_path}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "practical-scanning",
   "metadata": {},
   "source": [
    "Lets's see what's inside the folder (in this case `metadata` but should be the same for `pdf_parses`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brave-latter",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "grep_extention = f\".*\\.{extention}$\"\n",
    "\n",
    "metadata_output = !ls $meta_s2orc_path | grep $grep_extention\n",
    "print('metadata (len):', len(metadata_output))\n",
    "print('metadata (first 10):', metadata_output[:10])\n",
    "\n",
    "pdf_parses_output = !ls $pdfs_s2orc_path | grep $grep_extention\n",
    "print('pdf_parses (len):', len(pdf_parses_output))\n",
    "print('pdf_parses (first 10):', pdf_parses_output[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "selective-valuation",
   "metadata": {},
   "source": [
    "#### a. Intersection\n",
    "---\n",
    "As we want to examinate all `meta_s2orc` and `pdfs_s2orc` files, we need to search the intersection between those files namea and `metadata_output` and `pdf_parses_output` lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unsigned-pencil",
   "metadata": {},
   "outputs": [],
   "source": [
    "toread_meta_s2orc = sorted(list(set(meta_s2orc) & set(metadata_output)))\n",
    "toread_pdfs_s2orc = sorted(list(set(pdfs_s2orc) & set(pdf_parses_output)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "apparent-dollar",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'toread meta len: {len(toread_meta_s2orc)} \\n {toread_meta_s2orc} \\n')\n",
    "print(f'toread pdfs len: {len(toread_pdfs_s2orc)} \\n {toread_pdfs_s2orc} \\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "under-lawrence",
   "metadata": {},
   "source": [
    "So we can describe the function in charge to load the `jsonl` files. The function must have in input the `generic_path` (f\"{DATA_PATH}/{SAMPLE_FOLDER}\") and then searching in `metadata` and `pdf_parses` for the files present in `file_names`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accepting-niagara",
   "metadata": {},
   "source": [
    "(Unused)\n",
    "```python\n",
    "def read_json_list(jsonl_path):\n",
    "    json_list_of_dict = []\n",
    "    with open(jsonl_path, 'r') as input_json:\n",
    "        for json_line in input_json:\n",
    "            json_dict = json.loads(json_line)\n",
    "            json_list_of_dict.append(json_dict)\n",
    "    return json_list_of_dict\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "serious-release",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_meta_json_list_dict(file_path, args):\n",
    "\n",
    "    if verbose: print(f'file_path: {file_path}')\n",
    "    # list of dictionaries, one for each row in pdf_parses\n",
    "    json_list_of_dict = []\n",
    "    # create a list index\n",
    "    json_list_of_dict_idx = 0\n",
    "    # dictionary of indexes, to obtain the object from list, starting from the `paper_id`\n",
    "    json_dict_of_index = {}\n",
    "\n",
    "    delete_it = False\n",
    "    \n",
    "    if args['only_extrated']:\n",
    "        # just open as usual\n",
    "        input_json = open(file_path, 'r')\n",
    "        \n",
    "        if verbose: print('You choose to only use unzipped files')\n",
    "\n",
    "    else:\n",
    "        # import for unzip\n",
    "        import gzip\n",
    "        # open by firstly unzipping it\n",
    "        gz = gzip.open(file_path, 'rb')\n",
    "        input_json = io.BufferedReader(gz)\n",
    "        \n",
    "        if args['keep_extracted']:\n",
    "            if verbose: print('You choose to unzip files and keep the unzipped file (can full the memory)')\n",
    "        else:\n",
    "            delete_it = True\n",
    "            if verbose: print('You choose to unzip files and delete the unzipped file (memory-driven decision)')\n",
    "    \n",
    "    # check if [\"mag_field_of_study\"] is in args, and is valid\n",
    "    mag_field_filter = False\n",
    "    mag_field_all = False\n",
    "    mag_field_dict = {}\n",
    "    if type(args[\"mag_field_of_study\"]) is list:\n",
    "        mag_field_filter = True\n",
    "        if not args[\"mag_field_of_study\"]:\n",
    "            print(\"List is empty\")\n",
    "            mag_field_all = True\n",
    "        else:\n",
    "            for field in args[\"mag_field_of_study\"]:\n",
    "                mag_field_dict[field] = True\n",
    "    \n",
    "    \n",
    "    # tocks_1 = manager.counter(total=1e6, desc='Tocks_1', unit='tocks')\n",
    "    with input_json:\n",
    "        for index, json_line in tqdm(enumerate(input_json)):\n",
    "            json_dict = json.loads(json_line)\n",
    "        \n",
    "            # print(index, json_dict[\"mag_field_of_study\"])\n",
    "            mag_field_filter_pass = sum([ True if json_field in mag_field_dict else False for json_field in json_dict[\"mag_field_of_study\"]]) >= 1 if json_dict[\"mag_field_of_study\"] is not None else False\n",
    "            \n",
    "            # dataset = dataset.filter((lambda x, ids: none_papers_indexes.get(ids, True)), with_indices=True)\n",
    "            if not mag_field_filter or mag_field_all or mag_field_filter_pass:\n",
    "                # append the dictionary to the dictionaries' list\n",
    "                json_list_of_dict.append(json_dict)\n",
    "                # insert (paper_id, index) pair as (key, value) to the dictionary\n",
    "                json_dict_of_index[json_dict['paper_id']] = json_list_of_dict_idx\n",
    "                # increment the list index\n",
    "                json_list_of_dict_idx += 1\n",
    "            \n",
    "            # tocks_1.update()\n",
    "\n",
    "    # tocks_1.fill()\n",
    "    \n",
    "    if delete_it:\n",
    "        if verbose: print(f'[INFO-START] Delete file operation: deleting {file_path}')\n",
    "        # os.remove(file_path)\n",
    "        if verbose: print(f'[INFO] Delete file operation skipped for {file_path}')\n",
    "        if verbose: print(f'[INFO-END  ] Delete file operation: {file_path} deleted correclty')\n",
    "            \n",
    "    return json_list_of_dict, json_dict_of_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "recorded-assumption",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_pdfs_json_list_dict(file_path, json_dict_of_index_meta, args):\n",
    "\n",
    "    if verbose: print(f'file_path: {file_path}')\n",
    "    # list of dictionaries, one for each row in pdf_parses\n",
    "    json_list_of_dict = []\n",
    "    # create a list index\n",
    "    json_list_of_dict_idx = 0\n",
    "    # dictionary of indexes, to obtain the object from list, starting from the `paper_id`\n",
    "    json_dict_of_index = {}\n",
    "\n",
    "    delete_it = False\n",
    "    \n",
    "    if args['only_extrated']:\n",
    "        # just open as usual\n",
    "        input_json = open(file_path, 'r')\n",
    "        \n",
    "        if verbose: print('You choose to only use unzipped files')\n",
    "\n",
    "    else:\n",
    "        # import for unzip\n",
    "        import gzip\n",
    "        # open by firstly unzipping it\n",
    "        gz = gzip.open(file_path, 'rb')\n",
    "        input_json = io.BufferedReader(gz)\n",
    "        \n",
    "        if args['keep_extracted']:\n",
    "            if verbose: print('You choose to unzip files and keep the unzipped file (can full the memory)')\n",
    "        else:\n",
    "            delete_it = True\n",
    "            if verbose: print('You choose to unzip files and delete the unzipped file (memory-driven decision)')\n",
    "    \n",
    "    # tocks_2 = manager.counter(total=1.5e6, desc='Tocks_2', unit='tocks')\n",
    "    with input_json:\n",
    "        for index, json_line in tqdm(enumerate(input_json)):\n",
    "            json_dict = json.loads(json_line)\n",
    "                        \n",
    "            # if the metadata has been selected\n",
    "            if json_dict['paper_id'] in json_dict_of_index_meta:\n",
    "                # append the dictionary to the dictionaries' list\n",
    "                json_list_of_dict.append(json_dict)\n",
    "                # insert (paper_id, index) pair as (key, value) to the dictionary\n",
    "                json_dict_of_index[json_dict['paper_id']] = json_list_of_dict_idx\n",
    "                # increment the list index\n",
    "                json_list_of_dict_idx += 1\n",
    "            \n",
    "            # tocks_2.update()\n",
    "    \n",
    "    # tocks_2.fill()\n",
    "    \n",
    "    if delete_it:\n",
    "        if verbose: print(f'[INFO-START] Delete file operation: deleting {file_path}')\n",
    "        # os.remove(file_path)\n",
    "        if verbose: print(f'[INFO] Delete file operation skipped for {file_path}')\n",
    "        if verbose: print(f'[INFO-END  ] Delete file operation: {file_path} deleted correclty')\n",
    "            \n",
    "    return json_list_of_dict, json_dict_of_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "korean-breath",
   "metadata": {},
   "outputs": [],
   "source": [
    "def s2orc_chunk_read( s2orc_path, meta_s2orc_single_file, pdfs_s2orc_single_file, extention, args ):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        - s2orc_path (string): \n",
    "            Path to the Dataset directory (es. '{data}/s2orc-{sample|full}-20200705v1/{sample|full}').\n",
    "            \n",
    "        - meta_s2orc_single_file (string): \n",
    "            Filenames with extentions (es. 'sample_0.jsonl') present in `{dataset_path}/metadata`.\n",
    "\n",
    "        - pdfs_s2orc_single_file (string): \n",
    "            Filenames with extentions (es. 'sample_0.jsonl') present in `{dataset_path}/pdf_parses`.\n",
    "            \n",
    "        - extention (string | None):\n",
    "            String element either `jsonl` (only decompressed files) or `jsonl.gz` (to decompress files).\n",
    "            \n",
    "        - args (dict):\n",
    "            Dictionary containing some config params.\n",
    "            \n",
    "    Return:\n",
    "        - json_dict (list of dict): \n",
    "            Dictionary such as: { 'metadata': [...], 'pdf_parses': [...] } with objects of type \n",
    "            metadata_CLASS and pdf_parses_CLASS respectively.\n",
    "    \"\"\"\n",
    "    if verbose: print(\"[INFO-START] Metadata Chunk read  : \", meta_s2orc_single_file)\n",
    "    if verbose: print(\"[INFO-START] Pdf parses Chunk read: \", pdfs_s2orc_single_file)\n",
    "\n",
    "    json_dict_of_list = {'metadata': [], 'pdf_parses': [], 'meta_key_idx': {}, 'pdf_key_idx': {}}\n",
    "    \n",
    "    # tocks_0 = manager.counter(total=2, desc='Tocks_0', unit='tocks')    \n",
    "    \n",
    "    if verbose: print(\"[INFO] Metadata read: \", meta_s2orc_single_file)    \n",
    "    path_metadata = os.path.join(s2orc_path, 'metadata', meta_s2orc_single_file)\n",
    "    if verbose: print(f\"{path_metadata}\")\n",
    "    \n",
    "    json_list_metadata, json_dict_of_index_meta = read_meta_json_list_dict(path_metadata, args)\n",
    "    \n",
    "    json_dict_of_list['metadata'] = json_list_metadata\n",
    "    json_dict_of_list['meta_key_idx'] = json_dict_of_index_meta\n",
    "    \n",
    "    # tocks_0.update()\n",
    "    \n",
    "    if verbose: print(\"[INFO] Pdf_Parses read: \", pdfs_s2orc_single_file)\n",
    "    path_pdf_parses = os.path.join(s2orc_path, 'pdf_parses', pdfs_s2orc_single_file)\n",
    "    if verbose: print(f\"{path_pdf_parses}\")\n",
    "    \n",
    "    json_list_pdf_parses, json_dict_of_index_pdf = read_pdfs_json_list_dict(path_pdf_parses, json_dict_of_index_meta, args)\n",
    "    \n",
    "    json_dict_of_list['pdf_parses'] = json_list_pdf_parses\n",
    "    json_dict_of_list['pdf_key_idx'] = json_dict_of_index_pdf\n",
    "    \n",
    "    if verbose: print(\"[INFO-END  ] Chunk read: \", meta_s2orc_single_file, pdfs_s2orc_single_file)                 \n",
    "    \n",
    "    # tocks_0.update()\n",
    "    \n",
    "    return json_dict_of_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nutritional-associate",
   "metadata": {},
   "outputs": [],
   "source": [
    "def s2orc_multichunk_read(s2orc_path, toread_meta_s2orc, toread_pdfs_s2orc, extention, args):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        - s2orc_path (string): \n",
    "            Path to the Dataset directory (es. '{data}/s2orc-{sample|full}-20200705v1/{sample|full}').\n",
    "            \n",
    "        - toread_meta_s2orc (list of string): \n",
    "            List of filenames with extentions (es. ['sample_0.jsonl', 'sample_1.jsonl'])\n",
    "            present in `{dataset_path}/metadata`.\n",
    "\n",
    "        - toread_meta_s2orc (list of string): \n",
    "            List of filenames with extentions (es. ['sample_0.jsonl', 'sample_1.jsonl'])\n",
    "            present in `{dataset_path}/pdf_parses`.\n",
    "            \n",
    "        - extention (string | None):\n",
    "            String element either `jsonl` (only decompressed files) or `jsonl.gz` (to decompress files).\n",
    "            \n",
    "        - args (dict):\n",
    "            Dictionary containing some config params.\n",
    "            \n",
    "    \"\"\"\n",
    "    if verbose: print(\"[INFO-START] Multichunk read\")\n",
    "    if verbose: print(f\"[INFO] Metadata reading  : {toread_meta_s2orc}\")\n",
    "    if verbose: print(f\"[INFO] Pdf Parses reading: {toread_pdfs_s2orc}\")    \n",
    "    \n",
    "    assert len(toread_meta_s2orc) == len(toread_pdfs_s2orc), \"Files list (metadata and pdfs) must be the same length!\"\n",
    "    assert extention is not None, \"Extention must be set!\"\n",
    "    \n",
    "    if verbose: print(f\" \\n \\\n",
    "[INFO] Data read selection : \\n \\\n",
    "    [{'x' if args['only_extrated'] else ' '}] Extracted \\n \\\n",
    "    [{'x' if args['keep_extracted'] else ' '}] To Extract and Keep \\n \\\n",
    "    [{'x' if not args['keep_extracted'] else ' '}] To Extract and Delete \\n \\\n",
    "Only files already extracted will be analyzed.\")\n",
    "\n",
    "    multichunks_lists = []\n",
    "\n",
    "    # ticks = manager.counter(total=min(len(toread_meta_s2orc), len(toread_pdfs_s2orc)), desc='Ticks', unit='ticks')\n",
    "    \n",
    "    for meta_s2orc_single_file , pdfs_s2orc_single_file in tqdm(zip(toread_meta_s2orc, toread_pdfs_s2orc)):\n",
    "\n",
    "        chunk_list = s2orc_chunk_read( s2orc_path, meta_s2orc_single_file, pdfs_s2orc_single_file, extention, args )\n",
    "    \n",
    "        multichunks_lists.append(chunk_list)\n",
    "        \n",
    "        # ticks.update()\n",
    "    \n",
    "    return multichunks_lists"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "authorized-treaty",
   "metadata": {},
   "source": [
    "Important objects are:\n",
    "    \n",
    "- `s2orc_path` ('/home/vivoli/Thesis/data/s2orc-full-20200705v1/full')\n",
    "- `meta_s2orc_path` (f'{s2orc_path}/metadata')\n",
    "- `pdfs_s2orc_path` (f'{s2orc_path}/pdf_parses')\n",
    "- `toread_meta_s2orc` ( ['metadata_0.jsonl.gz', 'metadata_1.jsonl.gz'] )\n",
    "- `toread_pdfs_s2orc` ( ['pdf_parses_0.jsonl.gz', 'pdf_parses_1.jsonl.gz'] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "concerned-limitation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# manager = enlighten.get_manager()\n",
    "multichunks_lists = s2orc_multichunk_read(s2orc_path, toread_meta_s2orc, toread_pdfs_s2orc, extention, args)\n",
    "# manager.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bulgarian-grain",
   "metadata": {},
   "source": [
    "We have used only the `sample.jsonl` or the pair (`metadata_0.jsonl`-`pdf_parses_0.jsonl`) so we just have one element in the `multichunks_lists`. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "subjective-animation",
   "metadata": {},
   "source": [
    "We have parses all the `metadata` and `pdf_parses` elements, so we have now a dictionary that is composed by:\n",
    "```python\n",
    "json_dict_of_list = {\n",
    "    'metadata': [], \n",
    "    'pdf_parses': {}, \n",
    "    'meta_key_idx': {}, \n",
    "    'pdf_key_idx': {}\n",
    "}\n",
    "```\n",
    "In this dictionary we see:\n",
    "* metadata - `List[dict]` of type `metadata`.\n",
    "* pdf_parses - `List[dict]` of type `pdf_parses`.\n",
    "* meta_key_idx - `dict` with keys: `paper_id` and values: `index` in the metadata list.\n",
    "* pdf_key_idx - `dict` with keys: `paper_id` and values: `index` in the pdf_parses list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "close-wagon",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = multichunks_lists[0]['meta_key_idx']['18980380']\n",
    "multichunks_lists[0]['metadata'][index]['paper_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "actual-dressing",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = multichunks_lists[0]['pdf_key_idx']['18980380']\n",
    "multichunks_lists[0]['pdf_parses'][index]['paper_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aeb4337",
   "metadata": {},
   "outputs": [],
   "source": [
    "multichunks_lists[0]['metadata'][0]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1b10e32c",
   "metadata": {},
   "source": [
    "multichunks_lists[0]['pdf_parses'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "endless-quality",
   "metadata": {},
   "source": [
    "## Multichunks getDataset( (id, multichunk) | (single_chunk) )\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "racial-frontier",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fuse_dictionaries(single_chunk: list, \n",
    "                      data_field: List[str] =  [\"title\", \"abstract\"]) -> Dataset:\n",
    "    \n",
    "    # definition of **single_chunk**\n",
    "    # {'metadata': [], 'pdf_parses': [], 'meta_key_idx': {}, 'pdf_key_idx': {}}\n",
    "\n",
    "    print(f\"len meta single_chunk: {len(single_chunk['metadata'])}\")\n",
    "    print(f\"len pdfs single_chunk: {len(single_chunk['pdf_parses'])}\")\n",
    "    \n",
    "    verbose = False\n",
    "    print_all_debug = False\n",
    "    paper_list = []\n",
    "    for key in single_chunk['meta_key_idx']:\n",
    "        \n",
    "        if verbose: print(f\"[INFO] Analyse metadata dictionary for paper {key}\")\n",
    "        # get metadata dictionary for paper with paper_id: key\n",
    "        meta_index = single_chunk['meta_key_idx'].get(key, None)\n",
    "        if verbose: print(f\"meta_index: {meta_index}\")\n",
    "        metadata = single_chunk['metadata'][meta_index] if meta_index is not None else dict()\n",
    "        # print(metadata)\n",
    "        \n",
    "        if verbose: print(f\"[INFO] Analyse pdf_parses dictionary for paper {key}\")\n",
    "        # get pdf_parses dictionary for paper with paper_id: key\n",
    "        pdf_index = single_chunk['pdf_key_idx'].get(key, None)\n",
    "        if verbose: print(f\"pdf_index: {pdf_index}\")\n",
    "        pdf_parses = single_chunk['pdf_parses'][pdf_index] if pdf_index is not None else dict()\n",
    "        # print(pdf_parses)\n",
    "\n",
    "        def not_None(element):\n",
    "            \"\"\"\n",
    "                Here we see if the element is None, '' or [] \n",
    "                considering it to be Falsy type, in python.\n",
    "            \"\"\"\n",
    "            if element == None:\n",
    "                return False\n",
    "            elif type(element)==str and element is '':\n",
    "                return False\n",
    "            elif type(element)==list and element is []:\n",
    "                return False\n",
    "            return True\n",
    "        \n",
    "        def fuse_field(meta_field, pdf_field):\n",
    "            \"\"\"\n",
    "                With inspiration from https://docs.python.org/3/library/stdtypes.html#truth-value-testing\n",
    "                both '' and `None` seems to be Falsy type, in python.\n",
    "            \"\"\"\n",
    "            \n",
    "            class s2orcBaseElement():\n",
    "                \"\"\"\n",
    "                    'section': str,\n",
    "                    'text': str,\n",
    "                    'cite_spans': list,\n",
    "                    'ref_spans': list                \n",
    "                \"\"\"\n",
    "                def __init__(self, dictionary):\n",
    "                    self.section:str = dictionary['section']\n",
    "                    self.text:str = dictionary['text']\n",
    "                    self.cite_spans:list = dictionary['cite_spans']\n",
    "                    self.ref_spans:list = dictionary['ref_spans']\n",
    "                    \n",
    "                def get_text(self):\n",
    "                    return self.text\n",
    "            \n",
    "            if type(pdf_field)==list:\n",
    "                pdf_field = ' '.join([s2orcBaseElement(elem).get_text() for elem in pdf_field])\n",
    "                # print(pdf_field)\n",
    "        \n",
    "            return meta_field if not_None(meta_field) else pdf_field\n",
    "        \n",
    "        if verbose: print(f\"[INFO] Start fusion for paper {key}\")\n",
    "        paper = dict()\n",
    "        for field in data_field:\n",
    "            if print_all_debug: print(f\"[INFO] Fusing field {field} for meta ({metadata.get(field, None)}) and pdf_parses ({pdf_parses.get(field, None)})\")\n",
    "            paper[field] = fuse_field(metadata.get(field, None), pdf_parses.get(field, None))\n",
    "        \n",
    "        paper_list.append(paper)\n",
    "        \n",
    "        if print_all_debug: print(f\"[INFO] Deleting meta and pdf for paper {key}\")\n",
    "        # if meta_index is not None: del single_chunk['metadata'][meta_index]\n",
    "        # if pdf_index is not None: del single_chunk['pdf_parses'][pdf_index]\n",
    "    \n",
    "    # Dataset.from_pandas(my_dict) could be a good try if we only convert our paper_list to Pandas Dataframes\n",
    "    paper_df = pd.DataFrame(paper_list)\n",
    "    \n",
    "    # print(paper_df)\n",
    "    # print(paper_df['title'][121560], type(paper_df['title'][121560]))\n",
    "    # print(paper_df['abstract'][121560], type(paper_df['abstract'][121560]))\n",
    "    \n",
    "    return hfDataset.from_pandas(paper_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2590f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FIELD =  [\"title\", \"abstract\"]\n",
    "dataset_dict_test = fuse_dictionaries(multichunks_lists[0], data_field=DATA_FIELD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "seasonal-plaintiff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDataset( single_chunk: list,\n",
    "                tokenizer: PreTrainedTokenizer,\n",
    "                max_seq_length: int = None,\n",
    "                batch_size: int = 64,\n",
    "                num_workers: int = 4,\n",
    "                seed: int = SEED,\n",
    "                data_field: List[str] =  [\"title\", \"abstract\"]) -> Dict[str, DataLoader]:\n",
    "    \"\"\"Given an input file, prepare the train, test, validation dataloaders.\n",
    "    :param dataset_f: input file (format: .txt; line by line)\n",
    "    :param tokenizer: pretrained tokenizer that will prepare the data, i.e. convert tokens into IDs\n",
    "    :param max_seq_length: maximal sequence length. Longer sequences will be truncated\n",
    "    :param batch_size: batch size for the dataloaders\n",
    "    :param num_workers: number of CPU workers to use during dataloading. On Windows this must be zero\n",
    "    :return: a dictionary containing train, test, validation dataloaders\n",
    "    \"\"\"\n",
    "    print_all_debug = False\n",
    "    time_debug = True\n",
    "    print_some_debug = False\n",
    "\n",
    "    ## ------------------ ##\n",
    "    ## -- LOAD DATASET -- ##\n",
    "    ## ------------------ ##\n",
    "    if time_debug: start = time.time()\n",
    "    if time_debug: start_load = time.time()\n",
    "        \n",
    "    ## execution\n",
    "    max_seq_length = tokenizer.model_max_length if not max_seq_length else max_seq_length\n",
    "    if print_some_debug: print(max_seq_length)\n",
    "    dataset_dict = fuse_dictionaries(single_chunk, data_field)\n",
    "    \n",
    "    # print(dataset_dict)\n",
    "    \n",
    "    if print_some_debug: print(dataset_dict)\n",
    "\n",
    "    if time_debug: end_load = time.time()\n",
    "    if time_debug: print(f\"[TIME] load_dataset: {end_load - start_load}\")\n",
    "    \n",
    "    ## ------------------ ##\n",
    "    ## ---- MANAGING ---- ##\n",
    "    ## ------------------ ##\n",
    "    if time_debug: start_selection = time.time()\n",
    "    \n",
    "    ## execution\n",
    "    dataset = dataset_dict #['train']\n",
    "    \n",
    "    if time_debug: end_selection = time.time()\n",
    "    if time_debug: print(f\"[TIME] dataset_train selection: {end_selection - start_selection}\")\n",
    "    if print_all_debug: print(dataset)\n",
    "   \n",
    "    ## ------------------ ##\n",
    "    ## --- REMOVE none -- ##\n",
    "    ## ------------------ ##\n",
    "    if time_debug: start_removing = time.time()\n",
    "    # clean input removing papers with **None** as abstract/title\n",
    "    if remove_None_papers:\n",
    "\n",
    "        ## --------------------- ##\n",
    "        ## --- REMOVE.indexes -- ##\n",
    "        ## --------------------- ##\n",
    "        if time_debug: start_removing_indexes = time.time()\n",
    "        if print_all_debug: print(data_field)\n",
    "        \n",
    "        ## execution\n",
    "        none_papers_indexes = {}\n",
    "        for field in data_field:\n",
    "            none_indexes = [ idx_s for idx_s, s in enumerate(dataset[f\"{field}\"]) if s is None]\n",
    "            none_papers_indexes = {**none_papers_indexes, **dict.fromkeys(none_indexes , False)}\n",
    "\n",
    "        if time_debug: end_removing_indexes = time.time()\n",
    "        if time_debug: print(f\"[TIME] remove.indexes: {end_removing_indexes - start_removing_indexes}\")\n",
    "        if print_all_debug: print(none_papers_indexes)\n",
    "        \n",
    "        ## --------------------- ##\n",
    "        ## --- REMOVE.concat --- ##\n",
    "        ## --------------------- ##\n",
    "        if time_debug: start_removing_concat = time.time()\n",
    "        \n",
    "        ## execution\n",
    "        to_remove_indexes = list(none_papers_indexes.keys())\n",
    "\n",
    "        if time_debug: end_removing_concat = time.time()\n",
    "        if time_debug: print(f\"[TIME] remove.concat: {end_removing_concat - start_removing_concat}\")\n",
    "        if print_all_debug: print(to_remove_indexes)\n",
    "        if print_all_debug: print([ dataset[\"abstract\"][i] for i in to_remove_indexes])\n",
    "\n",
    "        ## --------------------- ##\n",
    "        ## --- REMOVE.filter --- ##\n",
    "        ## --------------------- ##\n",
    "        if time_debug: start_removing_filter = time.time()\n",
    "        \n",
    "        ## execution\n",
    "        dataset = dataset.filter((lambda x, ids: none_papers_indexes.get(ids, True)), with_indices=True)\n",
    "        \n",
    "        if time_debug: end_removing_filter = time.time()\n",
    "        if time_debug: print(f\"[TIME] remove.filter: {end_removing_filter - start_removing_filter}\")\n",
    "        if print_all_debug: print(dataset)\n",
    "\n",
    "        \n",
    "    if time_debug: end_removing = time.time()\n",
    "    if time_debug: print(f\"[TIME] remove None fields: {end_removing - start_removing}\")\n",
    "\n",
    "    ## --------------------- ##\n",
    "    ## --- REMOVE.column --- ##\n",
    "    ## --------------------- ##\n",
    "    if time_debug: start_remove_unused_columns = time.time()\n",
    "    if remove_Unused_columns:\n",
    "        \n",
    "        for column in dataset.column_names:\n",
    "            if column not in data_field:\n",
    "                if debug: print(f\"{column}\")\n",
    "                dataset.remove_columns_(column)\n",
    "\n",
    "    if time_debug: end_remove_unused_columns = time.time()\n",
    "    if time_debug: print(f\"[TIME] remove.column: {end_remove_unused_columns - start_remove_unused_columns}\")\n",
    "        \n",
    "    ## ------------------ ##\n",
    "    ## --- SPLIT 1.    -- ##\n",
    "    ## ------------------ ##\n",
    "    if time_debug: start_first_split = time.time()\n",
    "    \n",
    "    # 80% (train), 20% (test + validation)\n",
    "    ## execution\n",
    "    train_testvalid = dataset.train_test_split(test_size=0.2, seed=SEED)\n",
    "    \n",
    "    if time_debug: end_first_split = time.time()\n",
    "    if time_debug: print(f\"[TIME] first [train-(test-val)] split: {end_first_split - start_first_split}\")\n",
    "\n",
    "    ## ------------------ ##\n",
    "    ## --- SPLIT 2.    -- ##\n",
    "    ## ------------------ ##\n",
    "    if time_debug: start_second_split = time.time()\n",
    "    \n",
    "    # 10% of total (test), 10% of total (validation)\n",
    "    ## execution\n",
    "    test_valid = train_testvalid[\"test\"].train_test_split(test_size=0.5, seed=SEED)\n",
    "\n",
    "    if time_debug: end_second_split = time.time()\n",
    "    if time_debug: print(f\"[TIME] second [test-val] split: {end_second_split - start_second_split}\")\n",
    "\n",
    "    ## execution\n",
    "    dataset = DatasetDict({\"train\": train_testvalid[\"train\"],\n",
    "                          \"test\": test_valid[\"test\"],\n",
    "                          \"valid\": test_valid[\"train\"]})\n",
    "    if time_debug: end = time.time()\n",
    "    if time_debug: print(f\"[TIME] TOTAL: {end - start}\") \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "smooth-jonathan",
   "metadata": {},
   "source": [
    "## Multichunks getDatasets\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "intense-guest",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at allenai/scibert_scivocab_uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# tokenizer from 'allenai/scibert_scivocab_uncased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "model = BertForMaskedLM.from_pretrained(MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "distinct-tenant",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512\n"
     ]
    }
   ],
   "source": [
    "max_seq_length = model.config.max_position_embeddings\n",
    "print(max_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "executed-approach",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time\n",
    "\n",
    "dictionary_input = { \"data\": [\"abstract\"], \"target\": [\"title\"], \"classes\": [\"mag_field_of_study\"]}\n",
    "dictionary_columns = sum(dictionary_input.values(), [])\n",
    "\n",
    "# here we use meta_s2orc for speed, \n",
    "dataset = getDataset(multichunks_lists[0], tokenizer, data_field=dictionary_columns, max_seq_length=max_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a2d5bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "noted-washington",
   "metadata": {},
   "source": [
    "## S2ORC Preparation\n",
    "---\n",
    "\n",
    "To build a generic loading function we take inspiration from [here](https://discuss.huggingface.co/t/pipeline-with-custom-dataset-tokenizer-when-to-save-load-manually/1084/11)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "progressive-capture",
   "metadata": {},
   "source": [
    "(unused)\n",
    "```python\n",
    "class S2orcDataField(Enum):\n",
    "    TITLE: List[str] = [\"title\"]\n",
    "    ABSTRACT: List[str] = [\"abstract\"]\n",
    "    PAPER_ID: List[str] = [\"paper_id\"]\n",
    "    YEAH: List[str] = [\"year\"]\n",
    "    MAG_FIELD_OF_STUDY: List[str] = [\"mag_field_of_study\"]\n",
    "    S2_URL: List[str] = [\"s2_url\"]\n",
    "    TITLE_ABSTRACT: List[str] = [\"title\", \"abstract\"]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "composite-extreme",
   "metadata": {},
   "source": [
    "(unused original function)\n",
    "```python\n",
    "def prepare_data(dataset_f: str,\n",
    "                tokenizer: PreTrainedTokenizer,\n",
    "                max_seq_length: int = None,\n",
    "                batch_size: int = 64,\n",
    "                num_workers: int = 0,\n",
    "                seed: int = SEED,\n",
    "                data_field: List[str] =  [\"title\", \"abstract\"]) -> Dict[str, DataLoader]:\n",
    "    \"\"\"Given an input file, prepare the train, test, validation dataloaders.\n",
    "    :param dataset_f: input file (format: .txt; line by line)\n",
    "    :param tokenizer: pretrained tokenizer that will prepare the data, i.e. convert tokens into IDs\n",
    "    :param max_seq_length: maximal sequence length. Longer sequences will be truncated\n",
    "    :param batch_size: batch size for the dataloaders\n",
    "    :param num_workers: number of CPU workers to use during dataloading. On Windows this must be zero\n",
    "    :return: a dictionary containing train, test, validation dataloaders\n",
    "    \"\"\"\n",
    "    max_seq_length = tokenizer.model_max_length if not max_seq_length else max_seq_length\n",
    "\n",
    "    def preprocess(sentences: List[str]): #-> Dict[str, Union[list, Tensor]]:\n",
    "        \"\"\"Preprocess the raw input sentences from the text file.\n",
    "        :param sentences: a list of sentences (strings)\n",
    "        :return: a dictionary of \"input_ids\"\n",
    "        \"\"\"\n",
    "        tokens = [s.strip().split() for s in sentences]\n",
    "        tokens = [t[:max_seq_length - 1] + [tokenizer.eos_token] for t in tokens]\n",
    "\n",
    "        # The sequences are not padded here. we leave that to the dataloader in a collate_fn\n",
    "        # ----------------------------------------------- #\n",
    "        # -------- TODO include the `collate_fn` -------- #\n",
    "        # ----------------------------------------------- #\n",
    "        # That means: a bit slower processing, but a smaller saved dataset size\n",
    "        encoded_d = tokenizer(tokens,\n",
    "                             add_special_tokens=False,\n",
    "                             is_pretokenized=True,\n",
    "                             return_token_type_ids=False,\n",
    "                             return_attention_mask=False)\n",
    "\n",
    "        return {\"input_ids\": encoded_d[\"input_ids\"]}\n",
    "\n",
    "    dataset_dict = load_dataset(\"json\", data_files=dataset_f)\n",
    "    # dataset = Dataset.from_dict({\"text\": Path(dataset_f).read_text(encoding=\"utf-8\").splitlines()})\n",
    "    dataset = dataset_dict['train']\n",
    "    # 90% (train), 20% (test + validation)\n",
    "    train_testvalid = dataset.train_test_split(test_size=0.2, seed=SEED)\n",
    "    # 10% of total (test), 10% of total (validation)\n",
    "    test_valid = train_testvalid[\"test\"].train_test_split(test_size=0.5, seed=SEED)\n",
    "\n",
    "    dataset = DatasetDict({\"train\": train_testvalid[\"train\"],\n",
    "                          \"test\": test_valid[\"test\"],\n",
    "                          \"valid\": test_valid[\"train\"]})\n",
    "    print(dataset)\n",
    "    \"\"\"\n",
    "    choose one of the dataset columns:\n",
    "    - IMPORTANT fields: \n",
    "        'title', 'authors', 'abstract', \n",
    "    - LESS important fields: \n",
    "        'paper_id', 'year', 'arxiv_id', 'acl_id', 'pmc_id', 'pubmed_id', 'doi', \n",
    "        'venue', 'journal', 'mag_id', 'mag_field_of_study', \n",
    "        'outbound_citations', 'inbound_citations', 'has_outbound_citations', 'has_inbound_citations', \n",
    "        'has_pdf_body_text', 'has_pdf_parse', 'has_pdf_parsed_abstract', 'has_pdf_parsed_body_text', 'has_pdf_parsed_bib_entries', 'has_pdf_parsed_ref_entries', \n",
    "        's2_url'\n",
    "    \"\"\"\n",
    "    dataset = dataset.map(preprocess, input_columns=data_field, batched=True)\n",
    "    dataset.set_format(\"torch\", columns=[\"input_ids\"])\n",
    "\n",
    "    return {partition: DataLoader(ds,\n",
    "                                 batch_size=batch_size,\n",
    "                                 shuffle=True,\n",
    "                                 num_workers=num_workers,\n",
    "                                 pin_memory=True) for partition, ds in dataset.items()}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pending-companion",
   "metadata": {},
   "source": [
    "(unused function)\n",
    "```python\n",
    "# tokenizer from 'allenai/scibert_scivocab_uncased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "\n",
    "# tokenizer.add_special_tokens({\"eos_token\": \"[EOS]\"})\n",
    "DATA_FIELD =  [\"abstract\"]\n",
    "\n",
    "prepare_data(meta_s2orc, tokenizer, DATA_FIELD)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collectible-throw",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "---\n",
    "## ❌ PARTIAL PREPARE\n",
    "---"
   ]
  },
  {
   "cell_type": "raw",
   "id": "cooked-dairy",
   "metadata": {},
   "source": [
    "def partial_prepare_data(dataset_f: str,\n",
    "                tokenizer: PreTrainedTokenizer,\n",
    "                max_seq_length: int = None,\n",
    "                batch_size: int = 64,\n",
    "                num_workers: int = 4,\n",
    "                seed: int = SEED,\n",
    "                data_field: List[str] =  [\"title\", \"abstract\"]) -> Dict[str, DataLoader]:\n",
    "    \"\"\"Given an input file, prepare the train, test, validation dataloaders.\n",
    "    :param dataset_f: input file (format: .txt; line by line)\n",
    "    :param tokenizer: pretrained tokenizer that will prepare the data, i.e. convert tokens into IDs\n",
    "    :param max_seq_length: maximal sequence length. Longer sequences will be truncated\n",
    "    :param batch_size: batch size for the dataloaders\n",
    "    :param num_workers: number of CPU workers to use during dataloading. On Windows this must be zero\n",
    "    :return: a dictionary containing train, test, validation dataloaders\n",
    "    \"\"\"\n",
    "    print_all_debug = False\n",
    "    time_debug = True\n",
    "    print_some_debug = True\n",
    "\n",
    "    ## ------------------ ##\n",
    "    ## -- LOAD DATASET -- ##\n",
    "    ## ------------------ ##\n",
    "    if time_debug: start = time.time()\n",
    "    if time_debug: start_load = time.time()\n",
    "        \n",
    "    ## execution\n",
    "    max_seq_length = tokenizer.model_max_length if not max_seq_length else max_seq_length\n",
    "    if print_some_debug: print(max_seq_length)\n",
    "    dataset_dict = load_dataset(\"json\", data_files=dataset_f)\n",
    "\n",
    "    if time_debug: end_load = time.time()\n",
    "    if time_debug: print(f\"[TIME] load_dataset: {end_load - start_load}\")\n",
    "    \n",
    "    ## ------------------ ##\n",
    "    ## ---- MANAGING ---- ##\n",
    "    ## ------------------ ##\n",
    "    if time_debug: start_selection = time.time()\n",
    "    \n",
    "    ## execution\n",
    "    dataset = dataset_dict['train']\n",
    "    \n",
    "    if time_debug: end_selection = time.time()\n",
    "    if time_debug: print(f\"[TIME] dataset_train selection: {end_selection - start_selection}\")\n",
    "    if print_all_debug: print(dataset)\n",
    "   \n",
    "    ## ------------------ ##\n",
    "    ## --- REMOVE none -- ##\n",
    "    ## ------------------ ##\n",
    "    if time_debug: start_removing = time.time()\n",
    "    # clean input removing papers with **None** as abstract/title\n",
    "    if remove_None_papers:\n",
    "\n",
    "        ## --------------------- ##\n",
    "        ## --- REMOVE.indexes -- ##\n",
    "        ## --------------------- ##\n",
    "        if time_debug: start_removing_indexes = time.time()\n",
    "        if print_all_debug: print(data_field)\n",
    "        \n",
    "        ## execution\n",
    "        none_papers_indexes = {}\n",
    "        for field in data_field:\n",
    "            none_indexes = [ idx_s for idx_s, s in enumerate(dataset[f\"{field}\"]) if s is None]\n",
    "            none_papers_indexes = {**none_papers_indexes, **dict.fromkeys(none_indexes , False)}\n",
    "\n",
    "        if time_debug: end_removing_indexes = time.time()\n",
    "        if time_debug: print(f\"[TIME] remove.indexes: {end_removing_indexes - start_removing_indexes}\")\n",
    "        if print_all_debug: print(none_papers_indexes)\n",
    "        \n",
    "        ## --------------------- ##\n",
    "        ## --- REMOVE.concat --- ##\n",
    "        ## --------------------- ##\n",
    "        if time_debug: start_removing_concat = time.time()\n",
    "        \n",
    "        ## execution\n",
    "        to_remove_indexes = list(none_papers_indexes.keys())\n",
    "\n",
    "        if time_debug: end_removing_concat = time.time()\n",
    "        if time_debug: print(f\"[TIME] remove.concat: {end_removing_concat - start_removing_concat}\")\n",
    "        if print_all_debug: print(to_remove_indexes)\n",
    "        if print_all_debug: print([ dataset[\"abstract\"][i] for i in to_remove_indexes])\n",
    "\n",
    "        ## --------------------- ##\n",
    "        ## --- REMOVE.filter --- ##\n",
    "        ## --------------------- ##\n",
    "        if time_debug: start_removing_filter = time.time()\n",
    "        \n",
    "        ## execution\n",
    "        dataset = dataset.filter((lambda x, ids: none_papers_indexes.get(ids, True)), with_indices=True)\n",
    "        \n",
    "        if time_debug: end_removing_filter = time.time()\n",
    "        if time_debug: print(f\"[TIME] remove.filter: {end_removing_filter - start_removing_filter}\")\n",
    "        if print_all_debug: print(dataset)\n",
    "\n",
    "        \n",
    "    if time_debug: end_removing = time.time()\n",
    "    if time_debug: print(f\"[TIME] remove None fields: {end_removing - start_removing}\")\n",
    "\n",
    "    ## --------------------- ##\n",
    "    ## --- REMOVE.column --- ##\n",
    "    ## --------------------- ##\n",
    "    if time_debug: start_remove_unused_columns = time.time()\n",
    "    if remove_Unused_columns:\n",
    "        \n",
    "        for column in dataset.column_names:\n",
    "            if column not in data_field:\n",
    "                if debug: print(f\"{column}\")\n",
    "                dataset.remove_columns_(column)\n",
    "\n",
    "    if time_debug: end_remove_unused_columns = time.time()\n",
    "    if time_debug: print(f\"[TIME] remove.column: {end_remove_unused_columns - start_remove_unused_columns}\")\n",
    "        \n",
    "    ## ------------------ ##\n",
    "    ## --- SPLIT 1.    -- ##\n",
    "    ## ------------------ ##\n",
    "    if time_debug: start_first_split = time.time()\n",
    "    \n",
    "    # 80% (train), 20% (test + validation)\n",
    "    ## execution\n",
    "    train_testvalid = dataset.train_test_split(test_size=0.2, seed=SEED)\n",
    "    \n",
    "    if time_debug: end_first_split = time.time()\n",
    "    if time_debug: print(f\"[TIME] first [train-(test-val)] split: {end_first_split - start_first_split}\")\n",
    "\n",
    "    ## ------------------ ##\n",
    "    ## --- SPLIT 2.    -- ##\n",
    "    ## ------------------ ##\n",
    "    if time_debug: start_second_split = time.time()\n",
    "    \n",
    "    # 10% of total (test), 10% of total (validation)\n",
    "    ## execution\n",
    "    test_valid = train_testvalid[\"test\"].train_test_split(test_size=0.5, seed=SEED)\n",
    "\n",
    "    if time_debug: end_second_split = time.time()\n",
    "    if time_debug: print(f\"[TIME] second [test-val] split: {end_second_split - start_second_split}\")\n",
    "\n",
    "    ## execution\n",
    "    dataset = DatasetDict({\"train\": train_testvalid[\"train\"],\n",
    "                          \"test\": test_valid[\"test\"],\n",
    "                          \"valid\": test_valid[\"train\"]})\n",
    "    if time_debug: end = time.time()\n",
    "    if time_debug: print(f\"[TIME] TOTAL: {end - start}\") \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "raw",
   "id": "operating-twenty",
   "metadata": {},
   "source": [
    "# tokenizer from 'allenai/scibert_scivocab_uncased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "model = BertForMaskedLM.from_pretrained(MODEL_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "technological-response",
   "metadata": {},
   "source": [
    "Max sequence length from tokenizer, model and input might be differents:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "automated-manual",
   "metadata": {},
   "source": [
    "# max_seq_length = model.config.max_position_embeddings\n",
    "print(max_seq_length)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "motivated-horizontal",
   "metadata": {},
   "source": [
    "%time\n",
    "\n",
    "# tokenizer.add_special_tokens({\"eos_token\": \"[EOS]\"})\n",
    "DATA_FIELD =  [\"title\", \"abstract\"]\n",
    "\n",
    "# here we use meta_s2orc for speed, \n",
    "dataset = partial_prepare_data(meta_s2orc, tokenizer, data_field=DATA_FIELD, max_seq_length=max_seq_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded22613",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c43e22f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74529b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(*sentences_by_column, data, target, classes): #-> Dict[str, Union[list, Tensor]]:\n",
    "    \"\"\"Preprocess the raw input sentences from the text file.\n",
    "    :param sentences: a list of sentences (strings)\n",
    "    :return: a dictionary of \"input_ids\"\n",
    "    \"\"\"\n",
    "    print_all_debug = False\n",
    "    time_debug = False\n",
    "    print_some_debug = False\n",
    "\n",
    "    if debug: print(f\"[INFO-START] Preprocess on data: {data}, target: {target}\") \n",
    "    \n",
    "    assert data == ['abstract'], \"data should be ['abstract']\"\n",
    "    if debug: print(data)\n",
    "    assert target == ['title'], \"target should be ['title']\"\n",
    "    if debug: print(target)\n",
    "        \n",
    "    data_columns_len = len(data)\n",
    "    target_columns_len = len(target)\n",
    "    columns_len = data_columns_len + target_columns_len\n",
    "    \n",
    "    assert data_columns_len == 1, \"data length should be 1\"\n",
    "    if debug: print(data_columns_len)\n",
    "    assert target_columns_len == 1, \"target length should be 1\"\n",
    "    if debug: print(target_columns_len)\n",
    "        \n",
    "    sentences_by_column = np.asarray(sentences_by_column)\n",
    "    input_columns_len = len(sentences_by_column)\n",
    "    \n",
    "    if debug: print(f'all sentences (len {input_columns_len}): {sentences_by_column}')\n",
    "    \n",
    "    if target_columns_len == 0:\n",
    "        raise NameError(\"No target variable selected, \\\n",
    "                    are you sure you don't want any target?\")\n",
    "        \n",
    "    data_sentences = sentences_by_column[0]\n",
    "    target_sentences = sentences_by_column[1] # if columns_len == input_columns_len else sentences_by_column[data_columns_len:-1]\n",
    "    \n",
    "    if debug: print(data_sentences)\n",
    "    if debug: print(target_sentences)\n",
    "\n",
    "    \"\"\"\n",
    "    # clean input removing **None**, converting them to **''**\n",
    "    if clean_None_data:\n",
    "        data_sentences = np.asarray([ s if s is not None else '' for s in data_sentences])\n",
    "        target_sentences = np.asarray([ s if s is not None else '' for s in target_sentences])\n",
    "\n",
    "    # clean input removing papers with **None** as abstract/title\n",
    "    elif remove_None_data:\n",
    "        none_data_indexes = np.asarray([ idx_s for idx_s, s in enumerate(data_sentences) if s is None])\n",
    "        none_target_indexes = np.asarray([ idx_s for idx_s, s in enumerate(target_sentences) if s is None])\n",
    "\n",
    "        if debug: print(none_data_indexes)\n",
    "        if debug: print(none_target_indexes)\n",
    "\n",
    "        to_removed_indexes = np.unique(none_data_indexes, none_target_indexes)\n",
    "\n",
    "        if debug: print(to_removed_indexes)\n",
    "\n",
    "        data_sentences = np.delete(data_sentences, to_removed_indexes)\n",
    "        target_sentences = np.delete(target_sentences, to_removed_indexes)\n",
    "    \n",
    "    if debug: print(data_sentences)\n",
    "    if debug: print(target_sentences)\n",
    "    \"\"\"\n",
    "    \n",
    "    # sentences = [s for s in sentences if s is not None]\n",
    "    # tokens = [s.strip().split() for s in sentences]\n",
    "    # tokens = [t[:max_seq_length - 1] + [tokenizer.eos_token] for t in tokens]\n",
    "\n",
    "    # The sequences are not padded here. we leave that to the dataloader in a collate_fn\n",
    "    # ----------------------------------------------- #\n",
    "    # -------- TODO include the `collate_fn` -------- #\n",
    "    # ----------------------------------------------- #\n",
    "    # That means: a bit slower processing, but a smaller saved dataset size\n",
    "    if print_some_debug: print(max_seq_length)\n",
    "        \n",
    "    data_encoded_d = tokenizer(\n",
    "                        text=data_sentences.tolist(),\n",
    "                        # add_special_tokens=False,\n",
    "                        # is_pretokenized=True,\n",
    "                        padding=True, truncation=True, max_length=max_seq_length,\n",
    "                        return_token_type_ids=False,\n",
    "                        return_attention_mask=False,\n",
    "                        # We use this option because DataCollatorForLanguageModeling (see below) is more efficient when it\n",
    "                        # receives the `special_tokens_mask`.\n",
    "                        return_special_tokens_mask=True,\n",
    "                        return_tensors='np'\n",
    "    )\n",
    "    \n",
    "    target_encoded_d = tokenizer(\n",
    "                        text=target_sentences.tolist(),\n",
    "                        # add_special_tokens=False,\n",
    "                        # is_pretokenized=True,\n",
    "                        padding=True, truncation=True, max_length=max_seq_length,\n",
    "                        return_token_type_ids=False,\n",
    "                        return_attention_mask=False,\n",
    "                        # We use this option because DataCollatorForLanguageModeling (see below) is more efficient when it\n",
    "                        # receives the `special_tokens_mask`.\n",
    "                        return_special_tokens_mask=True,\n",
    "                        return_tensors='np'\n",
    "    )\n",
    "\n",
    "                            \n",
    "\n",
    "    if debug: print(data_encoded_d[\"input_ids\"].shape)\n",
    "    if debug: print(target_encoded_d[\"input_ids\"].shape)\n",
    "    # return encoded_d\n",
    "    \n",
    "    return {\"data_input_ids\": data_encoded_d[\"input_ids\"], \"target_input_ids\": target_encoded_d[\"input_ids\"]}\n",
    "    # return {\"input_ids\": sum(encoded_d['input_ids'], [])} "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lucky-mumbai",
   "metadata": {},
   "source": [
    "print an example\n",
    "```python \n",
    "print(dataset['train'][:10]['title'], dataset['train'][:10]['abstract'])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b583ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = tokenizer.get_vocab()\n",
    "print(f\"[PAD]: {vocab['[PAD]']}\")\n",
    "print(f\"[UNK]: {vocab['[UNK]']}\")\n",
    "print(f\"[SEP]: {vocab['[SEP]']}\")\n",
    "print(f\"[CLS]: {vocab['[CLS]']}\")\n",
    "print(f\"0: {tokenizer.convert_ids_to_tokens(0)}\")\n",
    "print(f\"1: {tokenizer.convert_ids_to_tokens(1)}\")\n",
    "print(f\"2: {tokenizer.convert_ids_to_tokens(2)}\")\n",
    "print(f\"99: {tokenizer.convert_ids_to_tokens(99)}\")\n",
    "print(f\"100: {tokenizer.convert_ids_to_tokens(100)}\")\n",
    "print(f\"101: {tokenizer.convert_ids_to_tokens(101)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aaa7c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "royal-ranking",
   "metadata": {},
   "source": [
    "Finally, I found [this](https://huggingface.co/docs/datasets/package_reference/main_classes.html?highlight=datasetdict#datasets.DatasetDict.map) documentation for the function `DatasetDict.map` from the `dataset` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99afae66",
   "metadata": {},
   "outputs": [],
   "source": [
    "debug = False\n",
    "\n",
    "dataset_map = dataset.map(preprocess, input_columns= dictionary_columns, fn_kwargs= dictionary_input, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47757de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d69d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "mag_field_dict: Dict = {\n",
    "    \"Medicine\":    0,\n",
    "    \"Biology\":     1,\n",
    "    \"Chemistry\":   2,\n",
    "    \"Engineering\": 4,\n",
    "    \"Computer Science\":    5,\n",
    "    \"Physics\":     6,\n",
    "    \"Materials Science\":     7,\n",
    "    \"Mathematics\":        8,\n",
    "    \"Psychology\":  9,\n",
    "    \"Economics\":   10,\n",
    "    \"Political Science\":    11,\n",
    "    \"Business\":    12,\n",
    "    \"Geology\":     13,\n",
    "    \"Sociology\":   14,\n",
    "    \"Geography\":   15,\n",
    "    \"Environmental Science\":     16,\n",
    "    \"Art\":         17,\n",
    "    \"History\":     18,\n",
    "    \"Philosophy\":  19\n",
    "    # \"null\":         3, \n",
    "}\n",
    "\n",
    "# The key null is actually null, not \"null\":str\n",
    "#\n",
    "#      real_mag_field_value = paper_metadata['mag_field_of_study']\n",
    "#\n",
    "# so we could return the id 3 if it not contained as key of dictionary\n",
    "#\n",
    "#      mag_field_dict.get(real_mag_field_value, 3)\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac7a1906",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mag_preprocessing(*mags):\n",
    "    \"\"\"Preprocess the raw input sentences from the text file.\n",
    "    :param sentences: a list of sentences (strings)\n",
    "    :return: a dictionary of \"input_ids\"\n",
    "    \"\"\"\n",
    "    debug = False\n",
    "    \n",
    "    if debug: print(f\"[INFO-START] Mag Preprocess\") \n",
    "        \n",
    "    mag_field = np.array(mags)\n",
    "    input_columns_len = mag_field.shape\n",
    "    if debug: print(f'pre flatten (len {input_columns_len}): {mag_field}')\n",
    "    if debug: print(f'pre types: {[type(ele) for ele in mag_field]}')\n",
    "    if debug: print(f'pre types: {type(mag_field)}')\n",
    "    \n",
    "    mag_field = mag_field.flatten()\n",
    "    input_columns_len = mag_field.shape\n",
    "    if debug: print(f'after flatten (len {input_columns_len}): {mag_field}')\n",
    "    if debug: print(f'after types: {[type(ele) for ele in mag_field]}')\n",
    "    if debug: print(f'after types: {type(mag_field)}')\n",
    "        \n",
    "    mag_field = np.array([ele if type(ele) == str else list(ele)[0] for ele in mag_field])\n",
    "        \n",
    "    if input_columns_len == 0:\n",
    "        raise NameError(\"No mag variable selected, \\\n",
    "                    are you sure you don't want any target?\")\n",
    "    \n",
    "    if debug: print(mag_field)\n",
    "    if debug: print(mag_field_dict)\n",
    "    if debug: print([mag_field_dict.get(real_mag_field_value, 3) for real_mag_field_value in mag_field])\n",
    "        \n",
    "    mag_index = np.asarray([mag_field_dict.get(real_mag_field_value, 3) for real_mag_field_value in mag_field])\n",
    "    \n",
    "    if debug: print(mag_index)\n",
    "    \n",
    "    return {\"mag_index\": mag_index}\n",
    "    # return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acdde217",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_mag_map = dataset_map.map(mag_preprocessing, input_columns= dictionary_input['classes'], batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fedda8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_mag_map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "periodic-shanghai",
   "metadata": {},
   "source": [
    "# Rename it as you want\n",
    "---\n",
    "\n",
    "- `dataset_map.rename_column` ,method for renaming\n",
    "- `dataset_map.set_format`, method for define what columns need to be returned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "progressive-entrance",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_mag_map = dataset_mag_map.rename_column(\"data_input_ids\", \"input_ids\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "respiratory-stock",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_mag_map.set_format(\"torch\", columns=[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "elect-antarctica",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset_mag_map['train'][1]['input_ids'].size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "religious-winter",
   "metadata": {},
   "source": [
    "Then, if you want to store it, it will be stored in the conda environment you are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "everyday-duration",
   "metadata": {},
   "outputs": [],
   "source": [
    "%store dataset_mag_map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "educated-format",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "## ❌ FAKE PIPELINE for train BERT-based NETS\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "specified-chester",
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r dataset_mag_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "comic-understanding",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['abstract', 'input_ids', 'mag_field_of_study', 'mag_index', 'target_input_ids', 'title'],\n",
       "        num_rows: 71952\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['abstract', 'input_ids', 'mag_field_of_study', 'mag_index', 'target_input_ids', 'title'],\n",
       "        num_rows: 8994\n",
       "    })\n",
       "    valid: Dataset({\n",
       "        features: ['abstract', 'input_ids', 'mag_field_of_study', 'mag_index', 'target_input_ids', 'title'],\n",
       "        num_rows: 8994\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_mag_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fuzzy-youth",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vivoli/miniconda3/envs/arxiv-manipulation/lib/python3.7/site-packages/ipykernel_launcher.py:5: UserWarning: This overload of nonzero is deprecated:\n",
      "\tnonzero()\n",
      "Consider using one of the following signatures instead:\n",
      "\tnonzero(*, bool as_tuple) (Triggered internally at  /opt/conda/conda-bld/pytorch_1607370156314/work/torch/csrc/utils/python_arg_parser.cpp:882.)\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "# tokenizer: we already have it\n",
    "# model: we already have it\n",
    "\n",
    "# If you print some element from `dataset_map['train'][element_index]['input_ids']` you'll see that lots of element\n",
    "vect = [ele[ele.nonzero()].size(0) for ele in dataset_mag_map['train'][:]['input_ids']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "unlike-return",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " max: 512 \n",
      " min: 2 \n",
      " avg: 162.88874527462752\n"
     ]
    }
   ],
   "source": [
    "max_vect = max(vect)\n",
    "min_vect = min(vect)\n",
    "sum_vect = sum(vect)\n",
    "len_vect = len(vect)\n",
    "\n",
    "print(f\" max: {max_vect} \\n min: {min_vect} \\n avg: {sum_vect/len_vect}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9ffa5a30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  102, 11261,   669,  ...,     0,     0,     0],\n",
      "        [  102,  6773,   165,  ...,     0,     0,     0],\n",
      "        [  102,   833,   111,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [  102,  5005,  4994,  ...,     0,     0,     0],\n",
      "        [  102,   121,   238,  ...,     0,     0,     0],\n",
      "        [  102, 15794,   190,  ...,     0,     0,     0]])}\n"
     ]
    }
   ],
   "source": [
    "print(dataset_mag_map['train'][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dc34f02f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_mag_map.set_format(\"torch\", columns=[\"input_ids\", \"target_input_ids\", \"mag_index\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b0619435",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  102, 11261,   669,  ...,     0,     0,     0],\n",
      "        [  102,  6773,   165,  ...,     0,     0,     0]]), 'mag_index': tensor([8, 5]), 'target_input_ids': tensor([[  102,   130,  4119, 19638,   579,   791,  4604,   727,   467,  3427,\n",
      "          2713,   131, 16982,   103,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0],\n",
      "        [  102,  2602,  2516,   190,  2652,   645, 28701,  2554,   137,   633,\n",
      "          1836,   147,  6773,   103,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0]])}\n"
     ]
    }
   ],
   "source": [
    "print(dataset_mag_map['train'][:2])"
   ]
  },
  {
   "cell_type": "raw",
   "id": "171f4a98",
   "metadata": {},
   "source": [
    "def pad_collate(batch):\n",
    "    batch = np.asarray(batch)\n",
    "    \n",
    "    xx, yy, zz = [], [], []\n",
    "    for elem in batch:\n",
    "        values = list(elem.values())\n",
    "        \n",
    "        x, y, z = values\n",
    "        xx.append(x)\n",
    "        yy.append(y)\n",
    "        zz.append(z)\n",
    "\n",
    "    # print(xx,yy,zz)\n",
    "    \n",
    "    # xx, yy = np.transpose([list(elem.values()) for elem in batch])\n",
    "    if verbose: print(f\"xx: \\n {xx} \\n\\nyy: \\n {yy} \\n\\nzz: \\n {zz}\")\n",
    "    \n",
    "    if debug: print(xx)\n",
    "    x_lens = [x.size() for x in xx]\n",
    "    \n",
    "    if debug: print(yy)\n",
    "    y_lens = [y.size() for y in yy]\n",
    "    \n",
    "    if debug: print(zz)\n",
    "    z_lens = [z.size() for z in zz]\n",
    "    \n",
    "    if verbose: print(f\"x_lens: \\n {x_lens} \\n\\ny_lens: \\n {y_lens} \\n\\nz_lens: \\n {z_lens}\")\n",
    "    \n",
    "    # xx_pad = pad_sequence(xx, batch_first=True, padding_value=0)\n",
    "    # yy_pad = pad_sequence(yy, batch_first=True, padding_value=0)\n",
    "    # zz_pad = pad_sequence(zz, batch_first=True, padding_value=0)\n",
    "\n",
    "    # return xx_pad, yy_pad, x_lens, y_lens\n",
    "    return xx, yy, zz, x_lens, y_lens, z_lens"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e5fffee6",
   "metadata": {},
   "source": [
    "print(debug)\n",
    "print(verbose)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c669f2bc",
   "metadata": {},
   "source": [
    "dataset_mag_map.items()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "11fe8e72",
   "metadata": {},
   "source": [
    "dataloader_train = DataLoader(dataset_mag_map['train'],\n",
    "                            batch_size=64,\n",
    "                            shuffle=True,\n",
    "                            num_workers=4,\n",
    "                            #collate_fn=pad_collate,\n",
    "                            pin_memory=True)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d263880b",
   "metadata": {},
   "source": [
    "for data in dataloader_train:\n",
    "    print(data)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "42e0437d",
   "metadata": {},
   "source": [
    "dataset_result = { partition: DataLoader(ds,\n",
    "                                        batch_size=64,\n",
    "                                        shuffle=True,\n",
    "                                        num_workers=4,\n",
    "                                        collate_fn=pad_collate,\n",
    "                                        pin_memory=True) for partition, ds in dataset_mag_map.items()}"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6106ecff",
   "metadata": {},
   "source": [
    "for ele in dataset_result['train']:  \n",
    "    # print(ele.size())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6393423e",
   "metadata": {},
   "source": [
    "for data_, target_, mag_, data_len, target_len, mag_len in dataset_result['train']:  \n",
    "    print(f\"data_len      : {data_len}\\n\")\n",
    "    print(f\"data_         : {data_}\\n\")\n",
    "    print(f\"data_.shape   : {data_.shape}\\n\\n\")\n",
    "    \n",
    "    print(f\"target_len    : {target_len}\\n\")\n",
    "    print(f\"target_       : {target_}\\n\")\n",
    "    print(f\"target_.shape : {target_.shape}\\n\\n\")\n",
    "    \n",
    "    print(f\"mag_len      : {mag_len}\\n\")\n",
    "    print(f\"mag_         : {mag_}\\n\")\n",
    "    print(f\"mag_.shape   : {mag_.shape}\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f78c607",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "## ❌ FAKE PIPELINE for train BERT-based NETS\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "raw",
   "id": "scheduled-arkansas",
   "metadata": {},
   "source": [
    "for data_ in dataset_map['train']:  \n",
    "    print(f\"data_        : {data_['input_ids'].size()}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "continued-structure",
   "metadata": {},
   "source": [
    "From [here](https://github.com/huggingface/transformers/blob/master/examples/language-modeling/run_mlm.py) you can get an idea from were the code has been borrowed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "547a01ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data collator\n",
    "# This one will take care of randomly masking the tokens.\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=True, mlm_probability=0.15)\n",
    "\n",
    "train_dataset = dataset_mag_map['train']\n",
    "eval_dataset = dataset_mag_map['valid']\n",
    "\n",
    "# Inizialize TrainerArguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,           # [def.`tmp_trainer`] output directory\n",
    "    num_train_epochs=3,              # [def.   3 ] total # of training epochs\n",
    "    per_device_train_batch_size=8,  # [def.   8 ] batch size per device during training\n",
    "    per_device_eval_batch_size=8,   # [def.   8 ] batch size for evaluation\n",
    "    evaluation_strategy=\"no\",     # [def. 'no'] evaluation is done (and logged) every eval_steps\n",
    "    warmup_steps=0,                # [def.   0 ] number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0,               # [def.   0 ] strength of weight decay \n",
    "    learning_rate=5e-5,              # [def. 5e-5] \n",
    "    logging_dir='./logs',            # [def. runs/__id__] directory for storing logs. TensorBoard log directory.\n",
    ")\n",
    "\n",
    "# Initialize our Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e9e9b343",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33memanuelevivoli\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.10.27 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.20<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">./tmp_trainer/#2_1_scibert-s2orc</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/emanuelevivoli/huggingface\" target=\"_blank\">https://wandb.ai/emanuelevivoli/huggingface</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/emanuelevivoli/huggingface/runs/1ii7mmfk\" target=\"_blank\">https://wandb.ai/emanuelevivoli/huggingface/runs/1ii7mmfk</a><br/>\n",
       "                Run data is saved locally in <code>/home/vivoli/Thesis/notebooks/wandb/run-20210420_113550-1ii7mmfk</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='8994' max='8994' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [8994/8994 32:26, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.740000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.768300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>1.745800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>1.772000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>1.757900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>1.718100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>1.729900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>1.703800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>1.693300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>1.680800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>1.702900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>1.677500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>1.665100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>1.658600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>1.633800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>1.649800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>1.632200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "AttributeError",
     "evalue": "'Trainer' object has no attribute 'log_metrics'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-188bc1c4de1b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"train_samples\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_train_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"train\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"train\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Trainer' object has no attribute 'log_metrics'"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "\n",
    "checkpoint = None\n",
    "train_result = trainer.train(resume_from_checkpoint=checkpoint)\n",
    "trainer.save_model()  # Saves the tokenizer too for easy upload\n",
    "metrics = train_result.metrics\n",
    "\n",
    "max_train_samples = len(train_dataset)\n",
    "metrics[\"train_samples\"] = min(max_train_samples, len(train_dataset))\n",
    "\n",
    "trainer.log_metrics(\"train\", metrics)\n",
    "trainer.save_metrics(\"train\", metrics)\n",
    "trainer.save_state()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b63196ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "\n",
    "logger.info(\"*** Evaluate ***\")\n",
    "\n",
    "metrics = trainer.evaluate()\n",
    "\n",
    "max_val_samples = dataset_args.max_val_samples if dataset_args.max_val_samples is not None else len(eval_dataset)\n",
    "metrics[\"eval_samples\"] = min(max_val_samples, len(eval_dataset))\n",
    "perplexity = math.exp(metrics[\"eval_loss\"])\n",
    "metrics[\"perplexity\"] = perplexity\n",
    "\n",
    "trainer.log_metrics(\"eval\", metrics)\n",
    "trainer.save_metrics(\"eval\", metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "opened-sudan",
   "metadata": {},
   "source": [
    "---\n",
    "# 1. Introduction\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "supported-senior",
   "metadata": {},
   "source": [
    "The following datasets were downloaded from the internet (we try to provide links to those we have the right to do so). We divide the dataset based on the task they are mostly used for.\n",
    "\n",
    "## 1.1 Keyphrase task\n",
    "---\n",
    "\n",
    "SOTA: [keyphrase generation](https://arxiv.org/pdf/1704.06879.pdf).\n",
    "\n",
    "The Keyphrase datasets (***duc***, ***Inspect***, ***Krapivin***, ***NUS***, ***SemEval-2010***, ***KP20k dataset***, ***MagKP-CS***) are structured as follow:\n",
    "\n",
    "- title\n",
    "- abstract\n",
    "- fulltext\n",
    "- keywords\n",
    "\n",
    "The only dataset that variates is ***STACKEX*** that instead of having *abstract* and *keywords* has:\n",
    "\n",
    "- question (abstract)\n",
    "- tags (keywords)\n",
    "\n",
    "Here there is a list of the datasets previously cited, with some information:\n",
    "\n",
    "- **duc**, we haven't had much information on this dataset untill now.\n",
    "\n",
    "- **Inspec** [(Hulth, 2003)](https://www.aclweb.org/anthology/W03-1028.pdf), This dataset provides *2,000 paper abstracts*. We adopt the *500 testing* papers and their corresponding uncontrolled keyphrases for evaluation, and the remaining *1,500 papers* are used for *training* the supervised baseline models.\n",
    "\n",
    "- **Krapivin** [(Krapivin et al., 2008)](http://eprints.biblio.unitn.it/1671/1/disi09055-krapivin-autayeu-marchese.pdf): This dataset provides *2,304 papers with full-text* and *author-assigned keyphrases*. However, the author did not mention how to split testing data, so we selected the first *400 papers in alphabetical order as the testing data*, and the *remaining* papers are used to *train* the supervised baselines.\n",
    "\n",
    "- **NUS** [(Nguyen and Kan, 2007)](https://www.comp.nus.edu.sg/~kanmy/papers/icadl2007.pdf): We use both author-assigned and reader-assigned keyphrases and treat *all 211 papers as the testing data*. Since the NUS dataset did not specifically mention the ways of splitting training and testing data, the results of the supervised baseline models are obtained through a *five-fold cross-validation*.\n",
    "\n",
    "- **SemEval-2010** [(Kim et al., 2010)](https://www.aclweb.org/anthology/S10-1004.pdf): 288 articles were collected from the ACM Digital Library. 100 articles were used for testing and the rest were used for training supervised baselines.\n",
    "\n",
    "- **KP20k dataset** [(Meng et al., 2018)](https://arxiv.org/abs/1704.06879): They built a new testing dataset that contains the *titles, abstracts, and keyphrases* of *20,000 scientific articles* in computer science. They were *randomly selected from their obtained 567,830 articles*. Thus they took the 20,000 articles in the validation set to train the supervised baselines.\n",
    "\n",
    "- **MagKP-CS** (from OpenNMT-py and [OpenNMT-kpg-release](https://github.com/memray/OpenNMT-kpg-release)) that is available for download. \n",
    "\n",
    "- **STACKEX** (from [StackExchange](https://archive.org/details/stackexchange)) has been constructed from the computer science forums (CS/AI) at StackExchange using “title” + “body” as source text and “tags” as the target keyphrases. After removing questions without valid tags, they collected 330,965 questions. They randomly selected *16,000 for validation*, and another *16,000 as test set*. Note some questions in StackExchange forums contain large blocks of code, resulting in long texts (sometimes more than 10,000 tokens after tokenization), this is difficult for most neural models to handle. Consequently, the texts have been truncated to 300 tokens and 1,000 tokens for training and evaluation splits respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rural-enough",
   "metadata": {},
   "source": [
    "###### ⚠️ATTENTION\n",
    "> As we aren't going to use the Keyphrase dataset for now, we don't need any custom classes for managing this dataset. We will implement this functions and classes as we go, if there will be the needs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "warming-gates",
   "metadata": {},
   "source": [
    "## 1.2 Sentence embedding task\n",
    "---\n",
    "\n",
    "SOTA: [sBERT](https://arxiv.org/abs/1908.10084)\n",
    "\n",
    "- **SNLI** [(Bowman et al., 2015)](https://arxiv.org/abs/1508.05326) is a collection of *570,000 sentence pairs* annotated with the *labels contradiction, eintailment, and neutral*.\n",
    "\n",
    "- **MultiNLI** [(Williams et al., 2018)](https://arxiv.org/abs/1704.05426) contains *430,000 sentence pairs* and covers a *range of genres of spoken and written text*.\n",
    "\n",
    "- **SciTail** [(allenai)](http://ai2-website.s3.amazonaws.com/publications/scitail-aaai-2018_cameraready.pdf), the entailment dataset consists of 27k. In contrast to the SNLI and MultiNLI, it was not crowd-sourced but created from sentences that already exist “in the wild”. *Hypotheses* were created from *science questions* and the corresponding *answer candidates*, while relevant web sentences from a large corpus were used as premises. Models are evaluated based on accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incorrect-seeking",
   "metadata": {},
   "source": [
    "###### ❌ATTENTION\n",
    "> As we aren't going to use the NLI tasks dataset (for now), we don't need any custom classes for managing this dataset. We will implement this functions and classes as we go, if there will be the needs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "protected-constitutional",
   "metadata": {},
   "source": [
    "## 1.3 Generic NLP tasks\n",
    "---\n",
    "\n",
    "- **S2ORC** [(Lo et al., 2020)](https://github.com/allenai/s2orc) is a large corpus of *81.1M English-language academic papers* spanning many academic disciplines. The corpus consists of *rich metadata, paper abstracts, resolved bibliographic references*, as well as *structured full text for 8.1M open access papers*. Full text is annotated with automatically-detected inline mentions of citations, figures, and tables, each linked to their corresponding paper objects. In S2ORC, they aggregate papers from hundreds of academic publishers and digital archives into a unified source, and create the largest publicly-available collection of machine-readable academic text to date. Built for text mining over academic text.\n",
    "\n",
    "- **OAG** [(Tang et al., 2008)](http://keg.cs.tsinghua.edu.cn/jietang/publications/KDD08-Tang-et-al-ArnetMiner.pdf)  is a large knowledge graph unifying *two billion-scale academic graphs*: Microsoft Academic Graph (**MAG**) and **AMiner**. In mid 2017, they published OAG v1, which contains *166,192,182 papers from MAG and 154,771,162 papers from AMiner* and generated *64,639,608 linking (matching) relations between the two graphs*. This time, in OAG v2, author, venue and newer publication data and the corresponding matchings are available.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "impressive-scanner",
   "metadata": {},
   "source": [
    "###### ✅ATTENTION\n",
    "> We are going to use the S2ORC dataset as it contains full_text data as well as citation/reference informations. It contains also authorship - title - tables data that we will describe below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "activated-amendment",
   "metadata": {},
   "source": [
    "---\n",
    "# 2. S2ORC\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "color-indonesian",
   "metadata": {},
   "source": [
    "## 2.1 Description (s2orc)\n",
    "---\n",
    "The `S2ORC` dataset is in the `data` path under the folder `s2orc-full-20200705v1` (where `s2orc` is the name of the dataset, `full` is the type, as there is also a sample fingerprint; and `20200705v1` is the version). \n",
    "We can reach the data folder exiting by the project and entering in the data folder:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "chinese-simon",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "DATA_PATH = '/home/vivoli/Thesis/data' \n",
    "!ls $DATA_PATH"
   ]
  },
  {
   "cell_type": "raw",
   "id": "described-homework",
   "metadata": {},
   "source": [
    "s2orc_path = f\"{DATA_PATH}/s2orc-{s2orc_type}-20200705v1/{s2orc_type}\"\n",
    "!ls $s2orc_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "activated-chemistry",
   "metadata": {},
   "source": [
    "As you can see (going into `s2orc-full-20200705v1/full/`) there are the `metadata` folder and the `pdf_parses` folder. The main difference (as we can already get it from the names) is that in the `metadata` you only have some information about the dataset (retrieved from the published metadata), while in the `pdf_parses` you get all the extensive data conteined in the paper (if the paper was present, was correctly parsed and no restriction in the paper data were applied due to limited licence permition). For some reason, the `title` of the paper is contained only in the `metadata` file, but it can get from the `paper_id` field of the paper itself.\n",
    "\n",
    "More information about the `S2ORC` dataset can be read in the [README.md](https://github.com/allenai/s2orc/blob/master/README.md) of the project and in the [project repository](https://github.com/allenai/s2orc/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dangerous-horror",
   "metadata": {},
   "source": [
    "### mag field\n",
    "- MAG fields of study:\n",
    "\n",
    "| Field of study | All papers | Full text |\n",
    "|----------------|------------|-----------|\n",
    "| Medicine       | 12.8M      | 1.8M      |\n",
    "| Biology        | 9.6M       | 1.6M      |\n",
    "| Chemistry      | 8.7M       | 484k      |\n",
    "| n/a            | 7.7M       | 583k      |\n",
    "| Engineering    | 6.3M       | 228k      |\n",
    "| Comp Sci       | 6.0M       | 580k      |\n",
    "| Physics        | 4.9M       | 838k      |\n",
    "| Mat Sci        | 4.6M       | 213k      |\n",
    "| Math           | 3.9M       | 669k      |\n",
    "| Psychology     | 3.4M       | 316k      |\n",
    "| Economics      | 2.3M       | 198k      |\n",
    "| Poli Sci       | 1.8M       | 69k       |\n",
    "| Business       | 1.8M       | 94k       |\n",
    "| Geology        | 1.8M       | 115k      |\n",
    "| Sociology      | 1.6M       | 93k       |\n",
    "| Geography      | 1.4M       | 58k       |\n",
    "| Env Sci        | 766k       | 52k       |\n",
    "| Art            | 700k       | 16k       |\n",
    "| History        | 690k       | 22k       |\n",
    "| Philosophy     | 384k       | 15k       |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hungarian-recommendation",
   "metadata": {},
   "source": [
    "We need now a function that reads all the lines of the `jsonl` files inside both `metadata` and `pdf_parses` folders. Then we'll "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "commercial-continent",
   "metadata": {},
   "source": [
    "## `metadata` schema\n",
    "\n",
    "We recommend everyone work with `metadata/` as the starting point.  This is a JSONlines file (one line per paper) with the following keys:\n",
    "\n",
    "#### Identifier fields\n",
    "\n",
    "* `paper_id`: a `str`-valued field that is a unique identifier for each S2ORC paper.\n",
    "\n",
    "* `arxiv_id`: a `str`-valued field for papers on [arXiv.org](https://arxiv.org).\n",
    "\n",
    "* `acl_id`: a `str`-valued field for papers on [the ACL Anthology](https://www.aclweb.org/anthology/).\n",
    "\n",
    "* `pmc_id`: a `str`-valued field for papers on [PubMed Central](https://www.ncbi.nlm.nih.gov/pmc/articles).\n",
    "\n",
    "* `pubmed_id`: a `str`-valued field for papers on [PubMed](https://pubmed.ncbi.nlm.nih.gov/), which includes MEDLINE.  Also known as `pmid` on PubMed.\n",
    "\n",
    "* `mag_id`: a `str`-valued field for papers on [Microsoft Academic](https://academic.microsoft.com).\n",
    "\n",
    "* `doi`: a `str`-valued field for the [DOI](http://doi.org/).  \n",
    "\n",
    "Notably:\n",
    "\n",
    "* Resolved citation links are represented by the cited paper's `paper_id`.\n",
    "\n",
    "* The `paper_id` resolves to a Semantic Scholar paper page, which can be verified using the `s2_url` field.\n",
    "\n",
    "* We don't always have a value for every identifier field.  When missing, they take `null` value.\n",
    "\n",
    "\n",
    "#### Metadata fields\n",
    "\n",
    "* `title`: a `str`-valued field for the paper title.  Every S2ORC paper *must* have one, though the source can be from publishers or parsed from PDFs.  We prioritize publisher-provided values over parsed values.\n",
    "\n",
    "* `authors`: a `List[Dict]`-valued field for the paper authors.  Authors are listed in order.  Each dictionary has the keys `first`, `middle`, `last`, and `suffix` for the author name, which are all `str`-valued with exception of `middle`, which is a `List[str]`-valued field.  Every S2ORC paper *must* have at least one author.\n",
    "\n",
    "* `venue` and `journal`: `str`-valued fields for the published venue/journal.  *Please note that there is not often agreement as to what constitutes a \"venue\" versus a \"journal\". Consolidating these fields is being considered for future releases.*   \n",
    "\n",
    "* `year`: an `int`-valued field for the published year.  If a paper is preprinted in 2019 but published in 2020, we try to ensure the `venue/journal` and `year` fields agree & prefer non-preprint published info. *We know this decision prohibits certain types of analysis like comparing preprint & published versions of a paper.  We're looking into it for future releases.*  \n",
    "\n",
    "* `abstract`: a `str`-valued field for the abstract.  These are provided directly from gold sources (not parsed from PDFs).  We preserve newline breaks in structured abstracts, which are common in medical papers, by denoting breaks with `':::'`.     \n",
    "\n",
    "* `inbound_citations`: a `List[str]`-valued field containing `paper_id` of other S2ORC papers that cite the current paper.  *Currently derived from PDF-parsed bibliographies, but may have gold sources in the future.*\n",
    "\n",
    "* `outbound_citations`: a `List[str]`-valued field containing `paper_id` of other S2ORC papers that the current paper cites.  Same note as above.   \n",
    "\n",
    "* `has_inbound_citations`: a `bool`-valued field that is `true` if `inbound_citations` has at least one entry, and `false` otherwise.\n",
    "\n",
    "* `has_outbound_citations` a `bool`-valued field that is `true` if `outbound_citations` has at least one entry, and `false` otherwise.\n",
    "\n",
    "We don't always have a value for every metadata field.  When missing, `str` fields take `null` value, while `List` fields are empty lists.\n",
    "\n",
    "#### PDF parse-related metadata fields\n",
    "\n",
    "* `has_pdf_parse`:  a `bool`-valued field that is `true` if this paper has a corresponding entry in `pdf_parses/`, which means we had processed that paper's PDF(s) at some point.  The field is `false` otherwise.\n",
    "\n",
    "* `has_pdf_parsed_abstract`: a `bool`-valued field that is `true` if the paper's PDF parse contains a parsed abstract, and `false` otherwise.   \n",
    "\n",
    "* `has_pdf_parsed_body_text`: a `bool`-valued field that is `true` if the paper's PDF parse contains parsed body text, and `false` otherwise.\n",
    "\n",
    "* `has_pdf_parsed_bib_entries`: a `bool`-valued field that is `true` if the paper's PDF parse contains parsed bibliography entries, and `false` otherwise.\n",
    "\n",
    "* `has_pdf_parsed_ref_entries`: a `bool`-valued field that is `true` if the paper's PDF parse contains parsed reference entries (e.g. tables, figures), and `false` otherwise.\n",
    "\n",
    "Please note:\n",
    "\n",
    "* If `has_pdf_parse = false`, the other four fields will not be present in the JSON (trivially `false`).\n",
    "\n",
    "* If `has_pdf_parse = true` but `has_pdf_parsed_abstract`, `has_pdf_parsed_body_text`, or `has_pdf_parsed_ref_entries` are `false`, this can be because:\n",
    "\n",
    "    * Our PDF parser failed to extract that element\n",
    "    * Our PDF parser succeeded but that paper simply did not have that element (e.g. papers without abstracts)\n",
    "    * Our PDF parser succeeded but that element was removed because the paper is not identified as open-access.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "diverse-humidity",
   "metadata": {},
   "source": [
    "##### metadata_CLASS\n",
    "```python\n",
    "{\n",
    " \"paper_id\": (string), \n",
    " \"title\": (string), \n",
    " \"authors\": [\n",
    "     {\n",
    "         \"first\": (string), \n",
    "         \"middle\": [], \n",
    "         \"last\": (string), \n",
    "         \"suffix\": (string)\n",
    "     },\n",
    "     ...\n",
    "   ]: **Author_Class**, \n",
    " \"abstract\": (string), \n",
    " \"year\": (int), \n",
    " \"arxiv_id\": null, \n",
    " \"acl_id\": null, \n",
    " \"pmc_id\": null, \n",
    " \"pubmed_id\": null, \n",
    " \"doi\": null, \n",
    " \"venue\": null, \n",
    " \"journal\": (string), \n",
    " \"mag_id\": (string-number), \n",
    " \"mag_field_of_study\": [\n",
    "     \"Medicine\",\n",
    "     \"Computer Science\"\n",
    "   ]: **FieldOfStudy_Enum**, \n",
    " \"outbound_citations\": [], \n",
    " \"inbound_citations\": [], \n",
    " \"has_outbound_citations\": false, \n",
    " \"has_inbound_citations\": false, \n",
    " \"has_pdf_parse\": false, \n",
    " \"s2_url\": (string)\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "neural-centre",
   "metadata": {},
   "source": [
    "Here I represent Author_Class as an object of \n",
    "```python\n",
    "{\n",
    "    \"first\": (string), \n",
    "    \"middle\": [], \n",
    "    \"last\": (string), \n",
    "    \"suffix\": (string)\n",
    "}\n",
    "```\n",
    "and `FieldOfStudy_Enum` as an Enum of string such as `[ \"Medicine\", \"Computer Science\", \"Physics\", \"Mathematics\", ... ]`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aggregate-tsunami",
   "metadata": {},
   "source": [
    "\n",
    "## `pdf_parses` schema\n",
    "\n",
    "We view `pdf_parses/` as supplementary to the `metadata/` entries.  PDF parses are also represented as JSONlines file (one line per paper) with the following keys:\n",
    "\n",
    "* `paper_id`: a `str`-valued field which is the same S2ORC paper ID in `metadata/`\n",
    "\n",
    "* `_pdf_hash`: a `str`-valued field.  Internal usage only.  We use this for debugging.\n",
    "\n",
    "* `abstract` and `body_text` are `List[Dict]`-valued fields representing parsed text from the PDF.  Each `Dict` corresponds to a paragraph.  `List` preserves their original ordering.\n",
    "\n",
    "* `bib_entries` and `ref_entries` are `Dict`-valued fields representing extracted entities that can be referenced (inline) within the text.\n",
    "\n",
    "#### example 1\n",
    "\n",
    "One example paragraph in `abstract` or `body_text` might look like:\n",
    "\n",
    "```python\n",
    "{\n",
    "    \"section\": \"Introduction\",\n",
    "    \"text\": \"Dogs are happier cats [13, 15]. See Figure 3 for a diagram.\",\n",
    "    \"cite_spans\": [\n",
    "        {\"start\": 22, \"end\": 25, \"text\": \"[13\", \"ref_id\": \"BIBREF11\"},\n",
    "        {\"start\": 27, \"end\": 30, \"text\": \"15]\", \"ref_id\": \"BIBREF30\"},\n",
    "        ...\n",
    "    ],\n",
    "    \"ref_spans\": [\n",
    "        {\"start\": 36, \"end\": 44, \"text\": \"Figure 3\", \"ref_id\": \"FIGREF2\"},\n",
    "    ]\n",
    "}\n",
    "```\n",
    "\n",
    "and example `bib_entries` and `ref_entries` might look like:\n",
    "\n",
    "```python\n",
    "{\n",
    "    ...,\n",
    "    \"BIBREF11\": {\n",
    "        \"title\": \"Do dogs dream of electric humans?\",\n",
    "        \"authors\": [\n",
    "            {\"first\": \"Lucy\", \"middle\": [\"Lu\"], \"last\": \"Wang\", \"suffix\": \"\"}, \n",
    "            {\"first\": \"Mark\", \"middle\": [], \"last\": \"Neumann\", \"suffix\": \"V\"}\n",
    "        ],\n",
    "        \"year\": \"\", \n",
    "        \"venue\": \"barXiv\",\n",
    "        \"link\": null\n",
    "    },\n",
    "    ...\n",
    "}\n",
    "```\n",
    "\n",
    "```python\n",
    "{\n",
    "    \"TABREF4\": {\n",
    "        \"text\": \"Table 5. Clearly, we achieve SOTA here or something.\",\n",
    "        \"type\": \"table\"\n",
    "    }\n",
    "    ...,\n",
    "    \"FIGREF2\": {\n",
    "        \"text\": \"Figure 3. This is the caption of a pretty figure.\",\n",
    "        \"type\": \"figure\"\n",
    "    },\n",
    "    ...\n",
    "}\n",
    "```\n",
    "\n",
    "Notice: \n",
    "\n",
    "* Inline `spans` are represented by character start and end indices into the paragraph `text`\n",
    "* `spans` resolve to `BIBREF`, `TABREF` or `FIGREF` entries.\n",
    "* `BIBREF` are IDs of bibliographic elements of `bib_entries`.  Bib entries may be missing fields (e.g. `year`).  They can be linked to S2ORC papers, as specified by `link`, but we also preserve any unlinked entries by setting `link` to `null`.\n",
    "* `FIGREF` and `TABREF` are IDs of figure and table elements of `ref_entries`.  Ref entries contain the caption text of the corresponding object, and also indicate the type of object.\n",
    "\n",
    "\n",
    "#### example 2\n",
    "\n",
    "You may see empty `pdf_parses/` JSONs that look like: \n",
    "\n",
    "```python\n",
    "{\n",
    "    \"paper_id\": \"...\", \n",
    "    \"_pdf_hash\": \"...\", \n",
    "    \"abstract\": [], \n",
    "    \"body_text\": [], \n",
    "    \"bib_entries\": {}, \n",
    "    \"ref_entries\": {}\n",
    "}\n",
    "```\n",
    "\n",
    "We keep these around for our internal usage, but the way to interpret these is that there is no usable PDF parse here, despite the corresponding `metadata/` entry still displaying `has_pdf_parse = true`.\n",
    "\n",
    "These exist when (i) `bib_entries` does not successfully parse *and* (ii) the paper is not open-access, so we had to remove `abstract`, `body_text`, and `ref_entries`.   \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "golden-packet",
   "metadata": {},
   "source": [
    "##### pdf_parses_CLASS\n",
    "```python\n",
    "{\n",
    " \"paper_id\": (string), \n",
    " \"_pdf_hash\": (string-number), \n",
    " \"abstract\": [\n",
    "     {\n",
    "         \"section\": (string) \"Abstract\", \n",
    "         \"text\": (string), \n",
    "         \"cite_spans\": [\n",
    "             {\n",
    "                 \"start\": (int), \n",
    "                 \"end\": (int), \n",
    "                 \"text\": (string-number) \"[4, \n",
    "                 \"ref_id\": (string)\n",
    "             }\n",
    "           ]: **CiteSpan_Class**, \n",
    "         \"ref_spans\": []\n",
    "     },\n",
    "     ...\n",
    " ]: **TextSection_Class**, \n",
    " \"body_text\": [], \n",
    " \"bib_entries\": \n",
    "     {\n",
    "         \"BIBREF0\": \n",
    "             {\n",
    "              \"title\": (string), \n",
    "              \"authors\": [\n",
    "                  {\n",
    "                      \"first\": (string), \n",
    "                      \"middle\": [], \n",
    "                      \"last\": (string), \n",
    "                      \"suffix\": (string)\n",
    "                   }\n",
    "                 ], \n",
    "               \"year\": (int), \n",
    "               \"venue\": (string), \n",
    "               \"link\": (string-number)\n",
    "              }, \n",
    "          \"BIBREF1\": \n",
    "              {\n",
    "                  ...\n",
    "              }\n",
    "       }: **BIBREF_Class**, \n",
    " \"ref_entries\": {}\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "broke-complement",
   "metadata": {},
   "source": [
    "Here I represent `TextSection_Class` as an object of \n",
    "```python\n",
    "{\n",
    " \"section\": (string), \n",
    " \"text\": (string), \n",
    " \"cite_spans\": [\n",
    "     {\n",
    "         \"start\": (int), \n",
    "         \"end\": (int), \n",
    "         \"text\": (string-number) \"[4, \n",
    "         \"ref_id\": (string)\n",
    "     }\n",
    "   ], \n",
    " \"ref_spans\": []\n",
    "}\n",
    "```\n",
    "where `CiteSpan_Class` itself is another structured object:\n",
    "```python\n",
    "{\n",
    " \"start\": (int), \n",
    " \"end\": (int), \n",
    " \"text\": (string-number), \n",
    " \"ref_id\": (string)\n",
    "}\n",
    "```\n",
    "and `BIBREF_Class` as dictionary field with `BIBREF_#` as key and related to it an object as follow:\n",
    "```python\n",
    "\"BIBREF_#\": \n",
    " {\n",
    "  \"title\": (string), \n",
    "  \"authors\": [\n",
    "      {\n",
    "          \"first\": (string), \n",
    "          \"middle\": [] (list of string),\n",
    "          \"last\": (string), \n",
    "          \"suffix\": (string)\n",
    "       }\n",
    "     ], \n",
    "   \"year\": (int), \n",
    "   \"venue\": (string), \n",
    "   \"link\": null\n",
    "  }\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "polar-glass",
   "metadata": {},
   "source": [
    "## 2.3 Title Abstract - Full text  (s2orc)\n",
    "---\n",
    "We have loaded the `S2ORC` dataset, created our (one chunk) dataset parses and we want now starting creating our dataset objects (Classes and Loaders).\n",
    "\n",
    "Let's start with the datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hollywood-pitch",
   "metadata": {},
   "source": [
    "### Dataset creation\n",
    "We want to create the datasets for papers' title-abstract and fulltext-(title-abstract) generation. \n",
    "> we'd like also to create a KeyPhrase dataset, we are actualling waiting for the response from the `S2ORC` authors to understand where can we possibly obtain the keyphrases/keywords.\n",
    "\n",
    "In order to do this, we want to create the two datasets (saving them as `jsonl` files).\n",
    "We can organize the data folder as :\n",
    "```bash\n",
    "- data/\n",
    "    # keyphrase dataset \n",
    "    - keyphrase/\n",
    "        # (title - abstract - fulltext - keyphrase)\n",
    "        - s2orc/\n",
    "            - README.md\n",
    "            - chuncks_dataset_idx.json\n",
    "            - train/\n",
    "                - train_0.jsonl\n",
    "                - train_1.jsonl\n",
    "                - ...\n",
    "            - test/\n",
    "                - test_0.jsonl\n",
    "                - test_1.jsonl\n",
    "                - ...\n",
    "            - val/\n",
    "                - val_0.jsonl\n",
    "                - val_1.jsonl\n",
    "                - ...\n",
    "    \n",
    "    # sts datasets\n",
    "    - sts/ \n",
    "        # (title - abstract - cosine_similarity)\n",
    "        - s2orc_partial/\n",
    "            - README.md\n",
    "            - chuncks_dataset_idx.json\n",
    "            - train/\n",
    "                - train_0.jsonl\n",
    "                - train_1.jsonl\n",
    "                - ...\n",
    "            - test/\n",
    "                - test_0.jsonl\n",
    "                - test_1.jsonl\n",
    "                - ...\n",
    "            - val/\n",
    "                - val_0.jsonl\n",
    "                - val_1.jsonl\n",
    "                - ...\n",
    "                \n",
    "        # (title - abstract - fulltext - cosine_similarity)\n",
    "        - s2orc_full/\n",
    "            - README.md\n",
    "            - chuncks_dataset_idx.json\n",
    "            - train/\n",
    "                - train_0.jsonl\n",
    "                - train_1.jsonl\n",
    "                - ...\n",
    "            - test/\n",
    "                - test_0.jsonl\n",
    "                - test_1.jsonl\n",
    "                - ...\n",
    "            - val/\n",
    "                - val_0.jsonl\n",
    "                - val_1.jsonl\n",
    "                - ...\n",
    "```\n",
    "and in the `chuncks_dataset_idx.json` there is the dictionary that maps the `chuncks` (`metadata_{id}.jsonl, pdf_parses_{id}.jsonl for id in range(99)`) into the {train|test|validation}_{id}.\n",
    "\n",
    "A first step to not-using chuncks (neither metadata nor fulltext) anymore is to summarize the data we want into a new python structure (dict) as follow, and save them \n",
    "\n",
    "```python\n",
    "{\n",
    "    \"paper_id\": (string-int), \n",
    "    \"title\":  (string),\n",
    "    \"abstract\": (string), \n",
    "    \"fulltext\": (string), \n",
    "    \"keywords\": List[string],\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collaborative-conviction",
   "metadata": {},
   "source": [
    "1. get the training/validation dataset by extracting Title-Abstract from the `S2ORC` dataset, and getting the testing data from the `KeyPhrase` (*'inspec', 'krapivin', 'nus', 'semeval', 'kp20k', 'duc', 'stackexchange'*) datasets. We should have a pair of sentence (indicativelly a *title* and an *abstract*), possibly a *fulltext* and a *keywords* fields those can be\n",
    "\n",
    "    - completelly related (abstract and its corresponding title)\n",
    "    - someway related (abstract and a field-keyphrase related title {cs+(deep learning; metric learning; nlp; sts;)}\n",
    "    - unrelated but not far away (abstract and a field-**not**keyphrase related title {cs+(nlp; transformer;)-vs-(cv; attention)}\n",
    "    - completelly unrelated (abstract and title are field-keyphrase unrelated {cs+a -vs- phy+z})\n",
    "\n",
    "\n",
    "\n",
    "2. **🤗transformers**, we can see [here](https://huggingface.co/docs/datasets/loading_datasets.html#json-files) the dataset loader (from `jsonl` files) can be used to load train/validation datasets. As we have alrerady load the dataset as dictionary (it is called `multichunks_lists` now, depending on how many chuncks we need to load in one shot) we could also be using the example [here](https://huggingface.co/docs/datasets/loading_datasets.html#from-a-python-dictionary) in order to load the dataset from an existing dictionary. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "clear-values",
   "metadata": {},
   "source": [
    "1. **sentence-transformer**, [sBERT example for train](https://www.sbert.net/docs/training/overview.html#loss-functions) \n",
    "\n",
    "2. **🤗transformers**, we can see [here](https://huggingface.co/docs/datasets/loading_datasets.html#json-files) the dataset loader (from `jsonl` files) can be used to load train/validation datasets. As we have alrerady load the dataset as dictionary (it is called `multichunks_lists` now, depending on how many chuncks we need to load in one shot) we could also be using the example [here](https://huggingface.co/docs/datasets/loading_datasets.html#from-a-python-dictionary) in order to load the dataset from an existing dictionary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cubic-columbia",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# TADataset states for TitleAbstractDataset\n",
    "class TADataset(torch.utils.data.Dataset):\n",
    "    \n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "noted-nomination",
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_id = metadata_dict['paper_id']\n",
    "print(f\"Currently viewing S2ORC paper: {paper_id}\")\n",
    "\n",
    "# suppose we only care about ACL anthology papers\n",
    "if not metadata_dict['acl_id']:\n",
    "    continue\n",
    "\n",
    "# and we want only papers with resolved outbound citations\n",
    "if not metadata_dict['has_outbound_citations']:\n",
    "    continue\n",
    "\n",
    "# get citation context (paragraphs)!\n",
    "if paper_id in paper_id_to_pdf_parse:\n",
    "    # (1) get the full pdf parse from the previously computed lookup dict\n",
    "    pdf_parse = paper_id_to_pdf_parse[paper_id]\n",
    "\n",
    "    # (2) pull out fields we need from the pdf parse, including bibliography & text\n",
    "    bib_entries = pdf_parse['bib_entries']\n",
    "    paragraphs = pdf_parse['abstract'] + pdf_parse['body_text']\n",
    "\n",
    "    # (3) loop over paragraphs, grabbing citation contexts\n",
    "    for paragraph in paragraphs:\n",
    "\n",
    "        # (4) loop over each inline citation in this paragraph\n",
    "        for cite_span in paragraph['cite_spans']:\n",
    "\n",
    "            # (5) each inline citation can be resolved to a bib entry\n",
    "            cited_bib_entry = bib_entries[cite_span['ref_id']]\n",
    "\n",
    "            # (6) that bib entry *may* be linked to a S2ORC paper.  if so, grab paragraph\n",
    "            linked_paper_id = cited_bib_entry['link']\n",
    "            if linked_paper_id:\n",
    "                citation_contexts.append({\n",
    "                    'citing_paper_id': paper_id,\n",
    "                    'cited_paper_id': linked_paper_id,\n",
    "                    'context': paragraph['text'],\n",
    "                    'citation_mention_start': cite_span['start'],\n",
    "                    'citation_mention_end': cite_span['end'],\n",
    "                })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "express-tract",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "accessory-invalid",
   "metadata": {},
   "source": [
    "# 3. Computing Word Embeddings: `Continuous Bag-of-Words`\n",
    "\n",
    "The Continuous Bag-of-Words model (CBOW) is frequently used in NLP deep learning. It is a model that tries to predict words given the context of a few words before and a few words after the target word. This is distinct from language modeling, since CBOW is not sequential and does not have to be probabilistic. Typcially, CBOW is used to quickly train word embeddings, and these embeddings are used to initialize the embeddings of some more complicated model. Usually, this is referred to as pretraining embeddings. It almost always helps performance a couple of percent.\n",
    "\n",
    "The CBOW model is as follows. Given a target word $w_i$ and an $N$ context window on each side, $w_{i−1}, … , w_{i−N}$ and $w_{i+1},…,w_{i+N}$, referring to all context words collectively as $C$, CBOW tries to minimize:\n",
    "\n",
    "\n",
    "$$ −log p(w_i|C) = − log Softmax( A( \\sum_{w∈C}{}{q_w})+b) $$\n",
    "\n",
    "where $q_w$ is the embedding for word $w$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "closing-milan",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTEXT_SIZE = 2  # 2 words to the left, 2 to the right\n",
    "raw_text = \"\"\"We are about to study the idea of a computational process.\n",
    "Computational processes are abstract beings that inhabit computers.\n",
    "As they evolve, processes manipulate other abstract things called data.\n",
    "The evolution of a process is directed by a pattern of rules\n",
    "called a program. People create programs to direct processes. In effect,\n",
    "we conjure the spirits of the computer with our spells.\"\"\".split()\n",
    "\n",
    "# By deriving a set from `raw_text`, we deduplicate the array\n",
    "vocab = set(raw_text)\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "word_to_ix = {word: i for i, word in enumerate(vocab)}\n",
    "data = []\n",
    "for i in range(2, len(raw_text) - 2):\n",
    "    context = [raw_text[i - 2], raw_text[i - 1],\n",
    "               raw_text[i + 1], raw_text[i + 2]]\n",
    "    target = raw_text[i]\n",
    "    data.append((context, target))\n",
    "print(data[:5])\n",
    "\n",
    "\n",
    "class CBOW(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        pass\n",
    "\n",
    "# create your model and train.  here are some functions to help you make\n",
    "# the data ready for use by your module\n",
    "\n",
    "\n",
    "def make_context_vector(context, word_to_ix):\n",
    "    idxs = [word_to_ix[w] for w in context]\n",
    "    return torch.tensor(idxs, dtype=torch.long)\n",
    "\n",
    "\n",
    "make_context_vector(data[0][0], word_to_ix)  # example"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
