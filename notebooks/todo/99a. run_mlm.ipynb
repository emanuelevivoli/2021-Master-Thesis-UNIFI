{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "educated-format",
   "metadata": {},
   "source": [
    "---\n",
    "# Load dataset\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "fuzzy-youth",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vivoli/miniconda3/envs/arxiv-manipulation/lib/python3.7/site-packages/ipykernel_launcher.py:2: UserWarning: This overload of nonzero is deprecated:\n",
      "\tnonzero()\n",
      "Consider using one of the following signatures instead:\n",
      "\tnonzero(*, bool as_tuple) (Triggered internally at  /opt/conda/conda-bld/pytorch_1607370156314/work/torch/csrc/utils/python_arg_parser.cpp:882.)\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# If you print some element from `dataset_map['train'][element_index]['input_ids']` you'll see that lots of element\n",
    "vect = [ele[ele.nonzero()].size(0) for ele in dataset_mag_map['train'][:]['input_ids']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "unlike-return",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " max: 512 \n",
      " min: 2 \n",
      " avg: 162.88874527462752\n"
     ]
    }
   ],
   "source": [
    "max_vect = max(vect)\n",
    "min_vect = min(vect)\n",
    "sum_vect = sum(vect)\n",
    "len_vect = len(vect)\n",
    "\n",
    "print(f\" max: {max_vect} \\n min: {min_vect} \\n avg: {sum_vect/len_vect}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d9adb966",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  102, 11261,   669,  ...,     0,     0,     0],\n",
      "        [  102,  6773,   165,  ...,     0,     0,     0],\n",
      "        [  102,   833,   111,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [  102,  5005,  4994,  ...,     0,     0,     0],\n",
      "        [  102,   121,   238,  ...,     0,     0,     0],\n",
      "        [  102, 15794,   190,  ...,     0,     0,     0]])}\n"
     ]
    }
   ],
   "source": [
    "print(dataset_mag_map['train'][:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d9abadc",
   "metadata": {},
   "source": [
    "---\n",
    "# Train with Trainer\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "continued-structure",
   "metadata": {},
   "source": [
    "From [here](https://github.com/huggingface/transformers/blob/master/examples/language-modeling/run_mlm.py) you can get an idea from were the code has been borrowed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "489144e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data collator\n",
    "# This one will take care of randomly masking the tokens.\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=True, mlm_probability=0.15)\n",
    "\n",
    "train_dataset = dataset_mag_map['train']\n",
    "eval_dataset = dataset_mag_map['valid']\n",
    "\n",
    "# Inizialize TrainerArguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,           # [def.`tmp_trainer`] output directory\n",
    "    num_train_epochs=3,              # [def.   3 ] total # of training epochs\n",
    "    per_device_train_batch_size=16,   # [def.   8 ] batch size per device during training\n",
    "    per_device_eval_batch_size=16,    # [def.   8 ] batch size for evaluation\n",
    "    evaluation_strategy=\"no\",        # [def. 'no'] evaluation is done (and logged) every eval_steps\n",
    "    warmup_steps=0,                  # [def.   0 ] number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0,                  # [def.   0 ] strength of weight decay \n",
    "    learning_rate=5e-4,              # [def. 5e-5] \n",
    "    logging_dir='./logs',            # [def. runs/__id__] directory for storing logs. TensorBoard log directory.\n",
    ")\n",
    "\n",
    "# Initialize our Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "04c1e4d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13491' max='13491' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13491/13491 1:31:30, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>3.358500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>3.659500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>3.709500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>3.710400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>3.653800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>3.688000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>3.567900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>3.496200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>3.408400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>3.282000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>3.219500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>3.157700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>3.081200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>3.032900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>2.972900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>2.882400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>2.844500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>2.803700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>2.696700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>2.651000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>2.627800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>2.583700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>2.514300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>2.473600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12500</td>\n",
       "      <td>2.440500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>2.424400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "\n",
    "checkpoint = None\n",
    "train_result = trainer.train(resume_from_checkpoint=checkpoint)\n",
    "trainer.save_model()  # Saves the tokenizer too for easy upload\n",
    "metrics = train_result.metrics\n",
    "\n",
    "# max_train_samples = dataset_args.max_train_samples if dataset_args.max_train_samples is not None else len(train_dataset)\n",
    "max_train_samples = len(train_dataset)\n",
    "metrics[\"train_samples\"] = min(max_train_samples, len(train_dataset))\n",
    "\n",
    "trainer.log_metrics(\"train\", metrics)\n",
    "trainer.save_metrics(\"train\", metrics)\n",
    "trainer.save_state()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ef32f8b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='563' max='563' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [563/563 01:06]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Evaluation\n",
    "\n",
    "logger.info(\"*** Evaluate ***\")\n",
    "\n",
    "metrics = trainer.evaluate()\n",
    "\n",
    "# max_val_samples = dataset_args.max_val_samples if dataset_args.max_val_samples is not None else len(eval_dataset)\n",
    "max_val_samples = len(eval_dataset)\n",
    "metrics[\"eval_samples\"] = min(max_val_samples, len(eval_dataset))\n",
    "\n",
    "import math\n",
    "perplexity = math.exp(metrics[\"eval_loss\"])\n",
    "metrics[\"perplexity\"] = perplexity\n",
    "\n",
    "trainer.log_metrics(\"eval\", metrics)\n",
    "trainer.save_metrics(\"eval\", metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "3f6c6f4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 30782<br/>Program ended successfully."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.03MB of 0.03MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find user logs for this run at: <code>/home/vivoli/Thesis/notebooks/wandb/run-20210420_153523-2fgxlvex/logs/debug.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find internal logs for this run at: <code>/home/vivoli/Thesis/notebooks/wandb/run-20210420_153523-2fgxlvex/logs/debug-internal.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run summary:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>train/loss</td><td>2.4244</td></tr><tr><td>train/learning_rate</td><td>2e-05</td></tr><tr><td>train/epoch</td><td>3.0</td></tr><tr><td>train/global_step</td><td>13491</td></tr><tr><td>_runtime</td><td>6519</td></tr><tr><td>_timestamp</td><td>1618932242</td></tr><tr><td>_step</td><td>27</td></tr><tr><td>train/train_runtime</td><td>5491.643</td></tr><tr><td>train/train_samples_per_second</td><td>2.457</td></tr><tr><td>train/total_flos</td><td>7.290962682789888e+16</td></tr><tr><td>eval/loss</td><td>2.33759</td></tr><tr><td>eval/runtime</td><td>66.9983</td></tr><tr><td>eval/samples_per_second</td><td>134.242</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run history:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>train/loss</td><td>▆█████▇▇▆▆▅▅▅▄▄▃▃▃▂▂▂▂▁▁▁▁</td></tr><tr><td>train/learning_rate</td><td>██▇▇▇▇▆▆▆▅▅▅▅▄▄▄▄▃▃▃▂▂▂▂▁▁</td></tr><tr><td>train/epoch</td><td>▁▁▂▂▂▂▃▃▃▃▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇███</td></tr><tr><td>train/global_step</td><td>▁▁▂▂▂▂▃▃▃▃▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇███</td></tr><tr><td>_runtime</td><td>▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇██</td></tr><tr><td>_timestamp</td><td>▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇██</td></tr><tr><td>_step</td><td>▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇██</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>eval/loss</td><td>▁</td></tr><tr><td>eval/runtime</td><td>▁</td></tr><tr><td>eval/samples_per_second</td><td>▁</td></tr></table><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    <br/>Synced <strong style=\"color:#cdcd00\">zesty-pond-3</strong>: <a href=\"https://wandb.ai/emanuelevivoli/huggingface.secondrun/runs/2fgxlvex\" target=\"_blank\">https://wandb.ai/emanuelevivoli/huggingface.secondrun/runs/2fgxlvex</a><br/>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
