{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "broad-review",
   "metadata": {},
   "source": [
    "# ðŸš€ Thesis Roadmap\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intelligent-maldives",
   "metadata": {},
   "source": [
    "The thesis objective is to build a **pipeline** based on NLP state-of-the-art that starting from Pdf scientific papers pubblications obtains a papers' embedding and using a custom learned metric is able to find similar papers.\n",
    "\n",
    "The main argument will be\n",
    "> **Deep metric learning for scientific paper similarities**\n",
    "\n",
    "Based on this problem the roadmap we will follow in this thesis will be:\n",
    "\n",
    "## Datasets\n",
    "---\n",
    "1. [x] collection of fulltext/title-abstract datasets\n",
    "2. [ ] *(optional) creation of public journal crowler*\n",
    "3. [ ] *(optional) definition of the pdf-to-json pipeline*\n",
    "\n",
    "## Tasks\n",
    "---\n",
    "1. [ ] (papers similarity) bag-of-words of papers, defining a metric on those BoWs\n",
    "2. [x] (embedding analysis) summarization of {abstract} - obtaining {title}\n",
    "    - [ ] (papers similarity) based on this embeddings, cosine similarities of papers\n",
    "3. [ ] (embedding analysis) summarization of {intro,method,conclusion} - obtaining {abstract}\n",
    "    - [ ] (papers similarity) based on this embeddings, cosine similarities of papers\n",
    "4. [ ] (visualizzation) clustering of paper based on {keyword}\n",
    "    - (methods) we can do this either by\n",
    "        - [ ] (approximate) averaging the keywords (finding the mean keyword)\n",
    "    - or\n",
    "        - [ ] (graph) building a graph of keyphrases (or keyphrases embedding) that represents the paper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "matched-character",
   "metadata": {},
   "source": [
    "# What first ?\n",
    "---\n",
    "\n",
    "What left:\n",
    "1. [x] I'm stranslating the used functions in module to import and use them\n",
    "2. [x] need to check if tokenize the, before is usefull or we need some preprocess different for mlm in that file\n",
    "3. [ ] probably the custom tokenization function is good for sts\n",
    "\n",
    "\n",
    "As the main goal is to build something that works, functional and useful, the first tasks to build are:\n",
    "\n",
    "1. [x] collection of fulltext/title-abstract datasets\n",
    "2. [x] defining classes Dataset and DatasetLoader (using ðŸ¤—huggingface/pytorch)\n",
    "3. [x] (embedding analysis) summarization of {abstract} - obtaining {title}\n",
    "    1. [ ] (papers similarity) based on this embeddings, cosine similarities of papers\n",
    "4. [ ] (visualizzation) clustering of paper based on {keyword}\n",
    "    - (methods) we can do this either by\n",
    "        - [ ] (approximate) averaging the keywords (finding the mean keyword)\n",
    "    - or\n",
    "        - [ ] (graph) building a graph of keyphrases (or keyphrases embedding) that represents the paper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "focused-deputy",
   "metadata": {},
   "source": [
    "# What next ?\n",
    "---\n",
    "\n",
    "The main goal here is to build the pipelines:\n",
    "\n",
    "1. [ ] Data preparation\n",
    "    1. [ ] fulltext line-by-line\n",
    "    2. [x] title line-by-line\n",
    "    3. [x] abstract line-by-line\n",
    "2. [Â ] Data preparation\n",
    "    1. [x] S2ORC analysis extract batch (Computer science sub (full/not full))\n",
    "    2. [x] ADD mag_id\n",
    "    3. [x] train/test dataset text-abstract from Computer Science (not over 1M papers)\n",
    "    4. [ ] <span style=\"color:red\">Full text from s2orc</span>\n",
    "3. [ ] Train\n",
    "    1. [x] Train on S2ORC\n",
    "    2. [ ] Train on S2ORC sub Comp.Scie\n",
    "    3. [Â ] (?) vocab + bert pre-train \n",
    "4. [ ] Eval\n",
    "    1. [ ] 3.1 vs import scibert-uncased -> who wins?\n",
    "    2. [ ] 3.2 vs 3.1 -> who wins?\n",
    "    3. [Â ] 3.3 vs all\n",
    "5. [x] (embedding analysis) summarization of {abstract} - obtaining {title}\n",
    "    1. [ ] (papers similarity) based on this embeddings, cosine similarities of papers\n",
    "        0. [ ] (papers similarity) on conference\n",
    "            - [ ] take papers from conference Comp.Scie.\n",
    "            - [ ] embedds all\n",
    "            - [ ] n(n-1) similitudinies\n",
    "            - [ ] clustering\n",
    "                - [ ] sbert-wk (chiedere a francesco)\n",
    "                - [ ] s2orc\n",
    "                - [ ] s2orc fine tuned comp.sci.\n",
    "6. [ ] (visualizzation) clustering of paper based on {keyword}\n",
    "    - (methods) we can do this either by\n",
    "        - [ ] (approximate) averaging the keywords (finding the mean keyword)\n",
    "    - or\n",
    "        - [ ] (graph) building a graph of keyphrases (or keyphrases embedding) that represents the paper     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4351dea6-2d5f-439c-9c6f-78af6455fdbb",
   "metadata": {},
   "source": [
    "# what now?\n",
    "\n",
    "0. [ ] Utils\n",
    "    2. [ ] Dataset config\n",
    "        1. [ ] dataset Name (\"s2orc\", \"keyphrase\", and the others)\n",
    "        2. [ ] create generic DatasetConfig\n",
    "        3. [ ] create s2orc DatasetConfig\n",
    "        4. [ ] create keyphrase DatasetConfig\n",
    "1. [ ] <span style=\"color:red\">S2ORC dataset (train/test)</span>\n",
    "    1. [ ] full/sample\n",
    "    2. [ ] mag_field specification (e.g. \\[\"Computer Science\", \"Phisics\"\\] )\n",
    "    3. [ ] (only for full) chunk ids (e.g. \\[0, 1, 2\\] over the 99 we have downloaded)\n",
    "    4. [ ] data, target, classes (e.g. dictionary_input = { \"data\": \\[\"abstract\"\\], \"target\": \\[\"title\"\\], \"classes\": \\[\"mag_field_of_study\"\\]})\n",
    "2. [ ] <span style=\"color:blue\">KeyPhrase dataset (train)</span>\n",
    "    1. [ ] Find it !\n",
    "3. [ ] <span style=\"color:green\">KeyPhrase dataset (test)</span>\n",
    "    1. [ ] title/abstract/keyphrase\n",
    "    2. [ ] field specification (e.g. \\[\"Computer Science\", \"Phisics\"\\] )\n",
    "4. [ ] <span style=\"color:orange\">Fusion s2orc + keyphrase</span>\n",
    "    1. [ ] having fulltest + keyphrases (same papers title | paper_id | arxive_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef65c657",
   "metadata": {},
   "source": [
    "# What very next?\n",
    "---\n",
    "\n",
    "The things I'd like to do, next to the next, are (based on build the pipelines):\n",
    "\n",
    "0. [ ] <span style=\"color:lightblue\">Using</span>\n",
    "    1. [ ] Fast.ai \n",
    "    2. [ ] ðŸ¤— huggingface\n",
    "    3. [ ] wandb / Tensorboard+vscode / neptune.ai\n",
    "    4. [ ] nbdev\n",
    "1. [ ] <span style=\"color:blue\">Data preparation</span>\n",
    "    1. [ ] (s2orc) {title, abstract, mag} line-by-line\n",
    "    2. [ ] (keyphrase) {title, abstract, mag, keyphrases} line-by-line\n",
    "    3. [ ] (s2orc) {title, abstract, fulltext, mag} line-by-line\n",
    "    4. [ ] (s2orc+keyphrase) {title, abstract, fulltext, mag, keyphrases} line-by-line\n",
    "2. [ ] <span style=\"color:purple\">Vocab choice</span>\n",
    "    1. [ ] with the model\n",
    "    2. [ ] scibert\n",
    "    3. [ ] custom\n",
    "    4. [ ] ftidf\n",
    "3. [ ] <span style=\"color:magenta\">Embedding choice</span>\n",
    "    1. [ ] BoW\n",
    "    2. [ ] CBoW\n",
    "    3. [ ] tfidf\n",
    "    3. [ ] Random\n",
    "    4. [ ] Vocab\n",
    "4. [ ] <span style=\"color:red\">Encoding (Model) choice</span>\n",
    "    1. [ ] BERT-based\n",
    "        1. [ ] scibert\n",
    "        2. [ ] sbert\n",
    "        3. [Â ]Â rbert\n",
    "        4. [ ] Roberta\n",
    "    4. [Â ]Â old methods\n",
    "        1. [ ] Doc2Vec\n",
    "        2. [ ] GloVe\n",
    "        3. [ ] ELMo\n",
    "    5. [Â ]Â other methods\n",
    "        2. [ ] xlnet\n",
    "        3. [Â ]Â keyphrase\n",
    "5. [Â ]  <span style=\"color:orange\">Pre Processing</span>\n",
    "    1. [ ] Vocabulary\n",
    "    2. [ ] Tokenizer\n",
    "    3. [ ] Encoder\n",
    "    4. [ ] Dataset\n",
    "    5. [ ] DataLoader\n",
    "6. [ ]  <span style=\"color:#cc7722\">Pre Training (MLM)</span>\n",
    "    1. [ ] from scratch\n",
    "    2. [ ] from checkpoint\n",
    "7. [ ] <span style=\"color:#cc7722\">Pooling</span>\n",
    "    - [ ] mean\n",
    "    - [ ] max\n",
    "    - [ ] bert-wk\n",
    "8. [ ]  <span style=\"color:brown\">Evaluation (MLM)</span>\n",
    "    1. [ ] loss\n",
    "    2. [ ] accuracy\n",
    "9. [ ] <span style=\"color:black\">Fine Tuning</span>\n",
    "    1. [ ] (papers similarity) STS\n",
    "    2. [ ] (title generation) title from abstract\n",
    "    3. [ ] (abstract generation) abstract from fulltext\n",
    "10. [Â ] <span style=\"color:pink\">Evaluation</span>\n",
    "    1. [ ] (benchmarks) SentEval\n",
    "    1. [ ] (papers similarity) STS\n",
    "    2. [ ] (title generation) title from abstract\n",
    "    3. [ ] (abstract generation) abstract from fulltext\n",
    "11. [ ] <span style=\"color:lime\">Ablation studies</span>\n",
    "    1. [ ] ?\n",
    "    2. [ ] ?\n",
    "12. [ ] <span style=\"color:lime\">(visualizzation) clustering of paper based on {keyword}</span>\n",
    "    - (methods) we can do this either by\n",
    "        - [ ] (approximate) averaging the keywords (finding the mean keyword)\n",
    "    - or\n",
    "        - [ ] (graph) building a graph of keyphrases (or keyphrases embedding) that represents the paper     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dafbf9c7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
