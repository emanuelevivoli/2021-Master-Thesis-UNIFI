{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b2d74db",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2ee8483-fb89-4ac5-aac8-d3a19a174d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b05a4cf-dc56-4d40-a0fb-c652cfe17cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All Imports\n",
    "\n",
    "import argparse\n",
    "import logging\n",
    "import os\n",
    "import io\n",
    "import sys\n",
    "import math\n",
    "import random\n",
    "\n",
    "# iteration\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# remote logging\n",
    "# comet_ml stands before because of ImportError: import Comet before modules: torch\n",
    "import comet_ml \n",
    "import wandb\n",
    "\n",
    "# objects properties extraction\n",
    "from dataclasses import asdict\n",
    "\n",
    "# ðŸ¤— Datasets \n",
    "import datasets\n",
    "from datasets import load_dataset, load_metric\n",
    "\n",
    "# dataclasses and types\n",
    "from dataclasses import field, dataclass\n",
    "from typing import Dict, List, Union, Optional\n",
    "\n",
    "# dataset managements\n",
    "# from torch.utils.data.dataloader import DataLoader\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# data managements\n",
    "import json # load/write data\n",
    "import torch \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# accelerator for speed up experiments/runs\n",
    "# from accelerate import Accelerator\n",
    "\n",
    "# ðŸ¤— Tranformers\n",
    "import transformers\n",
    "from transformers import (\n",
    "    Trainer, # For using Trainer instead of our custom loop\n",
    "    AdamW,\n",
    "    AutoTokenizer, \n",
    "    AutoConfig,\n",
    "    HfArgumentParser,\n",
    "    TrainingArguments as HfTrainingArguments,\n",
    "    SchedulerType,\n",
    "    get_scheduler,\n",
    "    set_seed,\n",
    ")\n",
    "\n",
    "from transformers.trainer_callback import (\n",
    "    DefaultFlowCallback,\n",
    "    PrinterCallback, \n",
    "    ProgressCallback\n",
    ")\n",
    "\n",
    "from transformers.integrations import (\n",
    "    TensorBoardCallback,\n",
    "    WandbCallback,\n",
    "    CometCallback,\n",
    "    MLflowCallback,\n",
    "    AzureMLCallback\n",
    ")\n",
    "\n",
    "_callback_factory: Dict = {\n",
    "    \"DefaultFlowCallback\": DefaultFlowCallback,\n",
    "    \"PrinterCallback\": PrinterCallback, \n",
    "    \"ProgressCallback\": ProgressCallback,\n",
    "    \"TensorBoardCallback\": TensorBoardCallback,\n",
    "    \"WandbCallback\": WandbCallback,\n",
    "    \"CometCallback\": CometCallback,\n",
    "    \"MLflowCallback\": MLflowCallback,\n",
    "    \"AzureMLCallback\": AzureMLCallback\n",
    "}\n",
    "\n",
    "# tokenization utils\n",
    "from thesis.utils.tokenization.helper import _speed_tokenization\n",
    "\n",
    "# checkpoint utils\n",
    "from transformers.trainer_utils import get_last_checkpoint\n",
    "    \n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "95493e92-6fa4-4db1-b2a1-464103d64318",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports for mlm (mlm_task)\n",
    "\n",
    "\"\"\"\n",
    "Fine-tuning the library models for masked language modeling (BERT, ALBERT, RoBERTa...)\n",
    "on a text file or a dataset without using HuggingFace Trainer.\n",
    "Here is the full list of checkpoints on the hub that can be fine-tuned by this script:\n",
    "https://huggingface.co/models?filter=masked-lm\n",
    "\"\"\"\n",
    "\n",
    "# ðŸ¤— Tranformers\n",
    "from transformers import (\n",
    "    CONFIG_MAPPING,\n",
    "    MODEL_MAPPING,\n",
    "    AutoModel, \n",
    "    AutoModelForMaskedLM,\n",
    "    PreTrainedTokenizer, \n",
    "    DataCollatorForLanguageModeling, \n",
    "    BertForMaskedLM\n",
    ")\n",
    "from transformers.trainer_utils import get_last_checkpoint, is_main_process\n",
    "from transformers.utils import check_min_version\n",
    "\n",
    "# parsing \n",
    "from thesis.utils.parsers.args_parser import parse_args\n",
    "from thesis.utils.general import load_dataset_wrapper\n",
    "\n",
    "# logging\n",
    "from thesis.utils.config.execution import LogConfig\n",
    "\n",
    "MODEL_CONFIG_CLASSES = list(MODEL_MAPPING.keys())\n",
    "MODEL_TYPES = tuple(conf.model_type for conf in MODEL_CONFIG_CLASSES)\n",
    "\n",
    "DICTIONARY_FIELD_NAMES = dict(\n",
    "    train         = ['train'],\n",
    "    test          = ['test', 'debug', 'dev'],\n",
    "    validation    = ['validation', 'valid']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b408e802",
   "metadata": {},
   "source": [
    "# Imports for glue (seq_class)\n",
    "\n",
    "\"\"\" Finetuning a ðŸ¤— Transformers model for sequence classification on GLUE.\"\"\"\n",
    "\n",
    "# ðŸ¤— Tranformers\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    DataCollatorWithPadding,\n",
    "    PretrainedConfig,\n",
    "    default_data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eefa69c6",
   "metadata": {},
   "source": [
    "from lensNLP import (\n",
    "    mlm_task,\n",
    "    seq_class\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "06491123",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "COMET INFO: Comet API key is valid\n"
     ]
    }
   ],
   "source": [
    "#Â Logging on Comet.ml\n",
    "comet_ml.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e9916725-3c2a-4008-aed1-fe76f09ab9fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33memanuelevivoli\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: WANDB_WATCH=all\n"
     ]
    }
   ],
   "source": [
    "# Logging on WandB\n",
    "wandb.login()\n",
    "\n",
    "# Optional: log both gradients and parameters\n",
    "%env WANDB_WATCH=all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "90a0f32c-d2e5-4a49-9012-f0a443200a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(args_list):\n",
    "\n",
    "    # ------------------\n",
    "    # Parsing arguments\n",
    "    # ------------------\n",
    "    \n",
    "    # If we pass only one argument to the script and it's the path to a json file, it parses it to get our arguments.\n",
    "    # If we pass a list of pairs '--option', 'argument', it uses those to get arguments.\n",
    "    dataset_args, training_args, model_args, run_args, log_args, embedding_args = parse_args(args_list)\n",
    "    log_config = LogConfig(**asdict(log_args))\n",
    "    \n",
    "    # ------------------\n",
    "    # Checkpoints\n",
    "    # ------------------\n",
    "    \n",
    "    # Detecting last checkpoint.\n",
    "    last_checkpoint = None\n",
    "    if os.path.isdir(training_args.output_dir) and training_args.do_train and not training_args.overwrite_output_dir:\n",
    "        last_checkpoint = get_last_checkpoint(training_args.output_dir)\n",
    "        if last_checkpoint is None and len(os.listdir(training_args.output_dir)) > 0:\n",
    "            raise ValueError(\n",
    "                f\"Output directory ({training_args.output_dir}) already exists and is not empty. \"\n",
    "                \"Use --overwrite_output_dir to overcome.\"\n",
    "            )\n",
    "        elif last_checkpoint is not None and training_args.resume_from_checkpoint is None:\n",
    "            logger.info(\n",
    "                f\"Checkpoint detected, resuming training at {last_checkpoint}. To avoid this behavior, change \"\n",
    "                \"the `--output_dir` or add `--overwrite_output_dir` to train from scratch.\"\n",
    "            )\n",
    "    \n",
    "    # ------------------\n",
    "    # Logging definition\n",
    "    # ------------------\n",
    "\n",
    "    # Setup logging\n",
    "    logging.basicConfig(\n",
    "        format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n",
    "        datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "        handlers=[logging.StreamHandler(sys.stdout)],\n",
    "    )\n",
    "    logger.setLevel(logging.INFO if is_main_process(training_args.local_rank) else logging.WARN)\n",
    "\n",
    "    # Log on each process the small summary:\n",
    "    logger.warning(\n",
    "        f\"Process rank: {training_args.local_rank}, device: {training_args.device}, n_gpu: {training_args.n_gpu}\"\n",
    "        + f\"distributed training: {bool(training_args.local_rank != -1)}, 16-bits training: {training_args.fp16}\"\n",
    "    )\n",
    "    # Set the verbosity to info of the Transformers logger (on main process only):\n",
    "    if is_main_process(training_args.local_rank):\n",
    "        transformers.utils.logging.set_verbosity_info()\n",
    "        transformers.utils.logging.enable_default_handler()\n",
    "        transformers.utils.logging.enable_explicit_format()\n",
    "    logger.info(f\"Training/evaluation parameters {training_args}\")\n",
    "\n",
    "    # ------------------\n",
    "    # Setting seed\n",
    "    # ------------------\n",
    "    \n",
    "    # If passed along, set the training seed now.\n",
    "    if training_args.seed is not None:\n",
    "        set_seed(training_args.seed)\n",
    "\n",
    "    # ------------------\n",
    "    # Getting the datasets\n",
    "    # ------------------\n",
    "\n",
    "    # Get the datasets: you can either provide your own CSV/JSON/TXT training and evaluation files (see below)\n",
    "    # or just provide the name of one of the public datasets available on the hub at https://huggingface.co/datasets/\n",
    "    # (the dataset will be downloaded automatically from the datasets Hub).\n",
    "    # You can also provide the name of some other dataset ('S2orc', 'Keyphrase') that we customly support.\n",
    "    #\n",
    "    # For the 'S2orc' dataset we provide a caching mechanisms in order to speed-up the preprocessing.\n",
    "    # The cache files changes (and are recalculated) everytime the configurations changes.\n",
    "    #\n",
    "    # For CSV/JSON files, this script will use the column called 'text' or the first column if no column called\n",
    "    # 'text' is found. You can easily tweak this behavior (see below).\n",
    "    #\n",
    "    # In distributed training, the load_dataset function guarantee that only one local process can concurrently\n",
    "    # download the dataset.\n",
    "    if dataset_args.dataset_name is not None:\n",
    "        # Getting the load_dataset wrapper that manages huggingface dataset and the custom ones\n",
    "        custom_load_dataset = load_dataset_wrapper()\n",
    "        # Loading the raw data based on input (and default) values of arguments\n",
    "        raw_datasets = custom_load_dataset(dataset_args, training_args, model_args, run_args, log_args, embedding_args)\n",
    "    else:\n",
    "        # If the files 'train_file' and 'validation_file' are specified\n",
    "        # data_files is composed by those elements.\n",
    "        data_files = {}\n",
    "        if dataset_args.train_file is not None:\n",
    "            data_files[\"train\"] = dataset_args.train_file\n",
    "        if dataset_args.validation_file is not None:\n",
    "            data_files[\"validation\"] = dataset_args.validation_file\n",
    "        extension = dataset_args.train_file.split(\".\")[-1]\n",
    "        if extension == \"txt\":\n",
    "            extension = \"text\"\n",
    "        elif extension == \"jsonl\": # jsonl files are file with json element per row\n",
    "            extension = \"json\"\n",
    "        raw_datasets = load_dataset(extension, data_files=data_files)\n",
    "    # See more about loading any type of standard or custom dataset (from files, python dict, pandas DataFrame, etc) at\n",
    "    # https://huggingface.co/docs/datasets/loading_datasets.html.\n",
    "\n",
    "    # The Datasets in the raw form can have different form of key names (depending on the configuration).\n",
    "    # We need all datasets to contain 'train', 'test', 'validation' keys, if not we change the dictionary keys' name\n",
    "    # based on the `names_tuple` and conseguently on `names_map`.\n",
    "    def format_key_names(raw_datasets):\n",
    "        # The creation of `names_map` happens to be here\n",
    "        # For every element in the values lists, one dictionary entry is added \n",
    "        # with (k,v): k=Value of the list, v=Key such as 'train', etc.\n",
    "        def names_dict_generator(names_tuple: dict):\n",
    "            names_map = dict()\n",
    "            for key, values in names_tuple.items():\n",
    "                for value in values:\n",
    "                    names_map[value] = key\n",
    "            return names_map\n",
    "        names_map = names_dict_generator(DICTIONARY_FIELD_NAMES)\n",
    "        split_names = raw_datasets.keys()\n",
    "        for split_name in split_names:\n",
    "            new_split_name = names_map.get(split_name)\n",
    "            if split_name != new_split_name:\n",
    "                raw_datasets[new_split_name] = raw_datasets.pop(split_name)  \n",
    "        return raw_datasets\n",
    "\n",
    "    logger.info(f\"Formatting DatasetDict keys\")\n",
    "    datasets = format_key_names(raw_datasets)\n",
    "\n",
    "    # ------------------\n",
    "    # Load tokenizer and\n",
    "    # pretrained model\n",
    "    # ------------------\n",
    "\n",
    "    # In distributed training, the .from_pretrained methods guarantee that only one local process can concurrently\n",
    "    # download model & vocab.\n",
    "    config_kwargs = {\n",
    "        \"cache_dir\": model_args.cache_dir,\n",
    "        \"revision\": model_args.model_revision,\n",
    "        \"use_auth_token\": True if model_args.use_auth_token else None,\n",
    "    }\n",
    "    if model_args.config_name:\n",
    "        config = AutoConfig.from_pretrained(model_args.config_name, **config_kwargs)\n",
    "    elif model_args.model_name_or_path:\n",
    "        config = AutoConfig.from_pretrained(model_args.model_name_or_path, **config_kwargs)\n",
    "    else:\n",
    "        config = CONFIG_MAPPING[model_args.model_type]()\n",
    "        logger.warning(\"You are instantiating a new config instance from scratch.\")\n",
    "\n",
    "    tokenizer_kwargs = {\n",
    "        \"cache_dir\": model_args.cache_dir,\n",
    "        \"use_fast\": model_args.use_fast_tokenizer,\n",
    "        \"revision\": model_args.model_revision,\n",
    "        \"use_auth_token\": True if model_args.use_auth_token else None,\n",
    "    }\n",
    "    if model_args.tokenizer_name:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_args.tokenizer_name, **tokenizer_kwargs)\n",
    "    elif model_args.model_name_or_path:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_args.model_name_or_path, **tokenizer_kwargs)\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            \"You are instantiating a new tokenizer from scratch. This is not supported by this script.\"\n",
    "            \"You can do it from another script, save it, and load it from here, using --tokenizer_name.\"\n",
    "        )\n",
    "\n",
    "    if model_args.model_name_or_path:\n",
    "        model = AutoModelForMaskedLM.from_pretrained(\n",
    "            model_args.model_name_or_path,\n",
    "            from_tf=bool(\".ckpt\" in model_args.model_name_or_path),\n",
    "            config=config,\n",
    "            cache_dir=model_args.cache_dir,\n",
    "            revision=model_args.model_revision,\n",
    "            use_auth_token=True if model_args.use_auth_token else None,\n",
    "        )\n",
    "    else:\n",
    "        logger.info(\"Training new model from scratch\")\n",
    "        model = AutoModelForMaskedLM.from_config(config)\n",
    "\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "    # Preprocessing the datasets.\n",
    "    # First we tokenize all the texts.\n",
    "    if training_args.do_train:\n",
    "        column_names = datasets[\"train\"].column_names\n",
    "    else:\n",
    "        column_names = datasets[\"validation\"].column_names\n",
    "    text_column_name = \"text\" if \"text\" in column_names else column_names[0]\n",
    "\n",
    "    if dataset_args.max_seq_length is None:\n",
    "        max_seq_length = tokenizer.model_max_length\n",
    "        if max_seq_length > 1024:\n",
    "            logger.warning(\n",
    "                f\"The tokenizer picked seems to have a very large `model_max_length` ({tokenizer.model_max_length}). \"\n",
    "                \"Picking 1024 instead. You can change that default value by passing --max_seq_length xxx.\"\n",
    "            )\n",
    "            max_seq_length = 1024\n",
    "    else:\n",
    "        if dataset_args.max_seq_length > tokenizer.model_max_length:\n",
    "            logger.warning(\n",
    "                f\"The max_seq_length passed ({dataset_args.max_seq_length}) is larger than the maximum length for the\"\n",
    "                f\"model ({tokenizer.model_max_length}). Using max_seq_length={tokenizer.model_max_length}.\"\n",
    "            )\n",
    "        max_seq_length = min(dataset_args.max_seq_length, tokenizer.model_max_length)\n",
    "\n",
    "    tokenized_datasets = _speed_tokenization(\n",
    "        dataset_args,\n",
    "        training_args,\n",
    "        tokenizer,\n",
    "        max_seq_length,\n",
    "        datasets,\n",
    "        text_column_name,\n",
    "        column_names\n",
    "    ) \n",
    "\n",
    "    if training_args.do_train:\n",
    "        if \"train\" not in tokenized_datasets:\n",
    "            raise ValueError(\"--do_train requires a train dataset\")\n",
    "        train_dataset = tokenized_datasets[\"train\"]\n",
    "        if dataset_args.max_train_samples is not None:\n",
    "            train_dataset = train_dataset.select(range(dataset_args.max_train_samples))\n",
    "\n",
    "    if training_args.do_eval:\n",
    "        if \"validation\" not in tokenized_datasets:\n",
    "            raise ValueError(\"--do_eval requires a validation dataset\")\n",
    "        eval_dataset = tokenized_datasets[\"validation\"]\n",
    "        if dataset_args.max_eval_samples is not None:\n",
    "            eval_dataset = eval_dataset.select(range(dataset_args.max_eval_samples))\n",
    "\n",
    "    # Data collator\n",
    "    # This one will take care of randomly masking the tokens.\n",
    "    pad_to_multiple_of_8 = dataset_args.line_by_line and training_args.fp16 and not training_args.pad_to_max_length\n",
    "    data_collator = DataCollatorForLanguageModeling(\n",
    "        tokenizer=tokenizer,\n",
    "        mlm_probability=dataset_args.mlm_probability,\n",
    "        pad_to_multiple_of=8 if pad_to_multiple_of_8 else None,\n",
    "    )\n",
    "\n",
    "    # Initialize our Trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset if training_args.do_train else None,\n",
    "        eval_dataset=eval_dataset if training_args.do_eval else None,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "        callbacks=[_callback_factory[callback] for callback in log_config.callbacks]\n",
    "    )\n",
    "\n",
    "    logging.info(f\"trainer: {trainer}\")\n",
    "\n",
    "    # Training\n",
    "    if training_args.do_train:\n",
    "        checkpoint = None\n",
    "        if training_args.resume_from_checkpoint is not None:\n",
    "            checkpoint = training_args.resume_from_checkpoint\n",
    "        elif last_checkpoint is not None:\n",
    "            checkpoint = last_checkpoint\n",
    "        train_result = trainer.train(resume_from_checkpoint=checkpoint)\n",
    "        trainer.save_model()  # Saves the tokenizer too for easy upload\n",
    "        metrics = train_result.metrics\n",
    "\n",
    "        max_train_samples = (\n",
    "            dataset_args.max_train_samples if dataset_args.max_train_samples is not None else len(train_dataset)\n",
    "        )\n",
    "        metrics[\"train_samples\"] = min(max_train_samples, len(train_dataset))\n",
    "\n",
    "        trainer.log_metrics(\"train\", metrics)\n",
    "        trainer.save_metrics(\"train\", metrics)\n",
    "        trainer.save_state()\n",
    "\n",
    "    # Evaluation\n",
    "    if training_args.do_eval:\n",
    "        logger.info(\"*** Evaluate ***\")\n",
    "\n",
    "        metrics = trainer.evaluate()\n",
    "\n",
    "        max_eval_samples = dataset_args.max_eval_samples if dataset_args.max_eval_samples is not None else len(eval_dataset)\n",
    "        metrics[\"eval_samples\"] = min(max_eval_samples, len(eval_dataset))\n",
    "        perplexity = math.exp(metrics[\"eval_loss\"])\n",
    "        metrics[\"perplexity\"] = perplexity\n",
    "\n",
    "        trainer.log_metrics(\"eval\", metrics)\n",
    "        trainer.save_metrics(\"eval\", metrics)\n",
    "\n",
    "    if training_args.push_to_hub:\n",
    "        kwargs = {\"finetuned_from\": model_args.model_name_or_path, \"tags\": \"fill-mask\"}\n",
    "        if training_args.dataset_name is not None:\n",
    "            kwargs[\"dataset_tags\"] = training_args.dataset_name\n",
    "            if training_args.dataset_config_name is not None:\n",
    "                kwargs[\"dataset_args\"] = training_args.dataset_config_name\n",
    "                kwargs[\"dataset\"] = f\"{training_args.dataset_name} {training_args.dataset_config_name}\"\n",
    "            else:\n",
    "                kwargs[\"dataset\"] = training_args.dataset_name\n",
    "\n",
    "        trainer.push_to_hub(**kwargs)\n",
    "\n",
    "\n",
    "    wandb.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "25c83995-a704-4f2b-8af8-7202757910cc",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "05/15/2021 08:09:40 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
      "05/15/2021 08:09:40 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(output_dir='/home/vivoli/Thesis/output', overwrite_output_dir=True, do_train=True, do_eval=True, do_predict=True, evaluation_strategy=<IntervalStrategy.NO: 'no'>, prediction_loss_only=False, per_device_train_batch_size=8, per_device_eval_batch_size=8, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=5e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=1, max_steps=-1, lr_scheduler_type=<SchedulerType.LINEAR: 'linear'>, warmup_ratio=0.0, warmup_steps=0, logging_dir='./logs', logging_strategy=<IntervalStrategy.STEPS: 'steps'>, logging_first_step=False, logging_steps=500, save_strategy=<IntervalStrategy.STEPS: 'steps'>, save_steps=500, save_total_limit=None, no_cuda=False, seed=1234, fp16=False, fp16_opt_level='O1', fp16_backend='auto', fp16_full_eval=False, local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=[], dataloader_drop_last=False, eval_steps=500, dataloader_num_workers=0, past_index=-1, run_name='scibert-s2orc', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, sharded_ddp=[], deepspeed=None, label_smoothing_factor=0.0, adafactor=False, group_by_length=False, length_column_name='length', report_to=['comet_ml', 'tensorboard', 'wandb'], ddp_find_unused_parameters=None, dataloader_pin_memory=True, skip_memory_metrics=False, use_legacy_prediction_loop=False, push_to_hub=False, resume_from_checkpoint=None, mp_parameters='', max_train_steps=None, num_warmup_steps=0)\n",
      "loaded result from cache for function S2orcConfig.get_filenames.<locals>._get_filenames    \n",
      "loaded result from cache for function S2orcConfig.get_completed_filenames.<locals>._get_completed_filenames    \n",
      "loaded result from cache for function S2orcConfig.get_toread_chunks.<locals>._get_toread_chunks    \n",
      "loaded result from cache for function s2orc_multichunk_read.<locals>._s2orc_multichunk_read    \n",
      "loaded result from cache for function get_dataset.<locals>._get_dataset    \n",
      "05/15/2021 08:09:52 - INFO - __main__ -   Formatting DatasetDict keys\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|configuration_utils.py:517] 2021-05-15 08:09:52,784 >> loading configuration file https://huggingface.co/allenai/scibert_scivocab_uncased/resolve/main/config.json from cache at /home/vivoli/.cache/huggingface/transformers/858852fd2471ce39075378592ddc87f5a6551e64c6825d1b92c8dab9318e0fc3.03ff9e9f998b9a9d40647a2148a202e3fb3d568dc0f170dda9dda194bab4d5dd\n",
      "[INFO|configuration_utils.py:553] 2021-05-15 08:09:52,784 >> Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.6.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 31090\n",
      "}\n",
      "\n",
      "[INFO|configuration_utils.py:517] 2021-05-15 08:09:53,141 >> loading configuration file https://huggingface.co/allenai/scibert_scivocab_uncased/resolve/main/config.json from cache at /home/vivoli/.cache/huggingface/transformers/858852fd2471ce39075378592ddc87f5a6551e64c6825d1b92c8dab9318e0fc3.03ff9e9f998b9a9d40647a2148a202e3fb3d568dc0f170dda9dda194bab4d5dd\n",
      "[INFO|configuration_utils.py:553] 2021-05-15 08:09:53,144 >> Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.6.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 31090\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-05-15 08:09:55,028 >> loading file https://huggingface.co/allenai/scibert_scivocab_uncased/resolve/main/vocab.txt from cache at /home/vivoli/.cache/huggingface/transformers/33593020f507d72099bd84ea6cd2296feb424fecd62d4a8edcc2a02899af6e29.38339d84e6e392addd730fd85fae32652c4cc7c5423633d6fa73e5f7937bbc38\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-05-15 08:09:55,031 >> loading file https://huggingface.co/allenai/scibert_scivocab_uncased/resolve/main/tokenizer.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-05-15 08:09:55,032 >> loading file https://huggingface.co/allenai/scibert_scivocab_uncased/resolve/main/added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-05-15 08:09:55,034 >> loading file https://huggingface.co/allenai/scibert_scivocab_uncased/resolve/main/special_tokens_map.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-05-15 08:09:55,035 >> loading file https://huggingface.co/allenai/scibert_scivocab_uncased/resolve/main/tokenizer_config.json from cache at None\n",
      "[INFO|modeling_utils.py:1155] 2021-05-15 08:09:55,419 >> loading weights file https://huggingface.co/allenai/scibert_scivocab_uncased/resolve/main/pytorch_model.bin from cache at /home/vivoli/.cache/huggingface/transformers/de14937a851e8180a2bc5660c0041d385f8a0c62b1b2ccafa46df31043a2390c.74830bb01a0ffcdeaed8be9916312726d0c4cd364ac6fc15b375f789eaff4cbb\n",
      "[WARNING|modeling_utils.py:1331] 2021-05-15 08:09:56,246 >> Some weights of the model checkpoint at allenai/scibert_scivocab_uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "[INFO|modeling_utils.py:1348] 2021-05-15 08:09:56,246 >> All the weights of BertForMaskedLM were initialized from the model checkpoint at allenai/scibert_scivocab_uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForMaskedLM for predictions without further training.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f68dc402b754c4c8feb166a83363b17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=72.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82ffbda0852b409f9026a6bf666c6f37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=9.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83510318a70a4ff6b6140cf36059787e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=9.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b50d9117b2341ee96586a0b491243e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=72.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbaef689da464674910b468ba8c94e55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=9.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f4ccc11258a461da775d8824f709d7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=9.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING|trainer_callback.py:306] 2021-05-15 08:11:26,311 >> You are adding a <class 'transformers.integrations.WandbCallback'> to the callbacks of this Trainer, but there is already one. The currentlist of callbacks is\n",
      ":DefaultFlowCallback\n",
      "CometCallback\n",
      "TensorBoardCallback\n",
      "WandbCallback\n",
      "[WARNING|trainer_callback.py:306] 2021-05-15 08:11:26,311 >> You are adding a <class 'transformers.integrations.CometCallback'> to the callbacks of this Trainer, but there is already one. The currentlist of callbacks is\n",
      ":DefaultFlowCallback\n",
      "CometCallback\n",
      "TensorBoardCallback\n",
      "WandbCallback\n",
      "WandbCallback\n",
      "[WARNING|trainer_callback.py:306] 2021-05-15 08:11:26,312 >> You are adding a <class 'transformers.integrations.TensorBoardCallback'> to the callbacks of this Trainer, but there is already one. The currentlist of callbacks is\n",
      ":DefaultFlowCallback\n",
      "CometCallback\n",
      "TensorBoardCallback\n",
      "WandbCallback\n",
      "WandbCallback\n",
      "CometCallback\n",
      "[INFO|trainer.py:516] 2021-05-15 08:11:26,401 >> The following columns in the training set  don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
      "[INFO|trainer.py:1156] 2021-05-15 08:11:26,408 >> ***** Running training *****\n",
      "[INFO|trainer.py:1157] 2021-05-15 08:11:26,408 >>   Num examples = 23155\n",
      "[INFO|trainer.py:1158] 2021-05-15 08:11:26,409 >>   Num Epochs = 1\n",
      "[INFO|trainer.py:1159] 2021-05-15 08:11:26,409 >>   Instantaneous batch size per device = 8\n",
      "[INFO|trainer.py:1160] 2021-05-15 08:11:26,410 >>   Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "[INFO|trainer.py:1161] 2021-05-15 08:11:26,410 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:1162] 2021-05-15 08:11:26,411 >>   Total optimization steps = 2895\n",
      "COMET WARNING: As you are running in a Jupyter environment, you will need to call `experiment.end()` when finished to ensure all metrics and code are logged before exiting.\n",
      "COMET INFO: Experiment is live on comet.ml https://www.comet.ml/emanuelevivoli/huggingface/d146c630642f4130a8ce0ebd5ec7d9ae\n",
      "\n",
      "[INFO|integrations.py:788] 2021-05-15 08:11:31,433 >> Automatic Comet.ml online logging enabled\n",
      "[INFO|integrations.py:675] 2021-05-15 08:11:31,446 >> Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.30<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">scibert-s2orc</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/emanuelevivoli/huggingface\" target=\"_blank\">https://wandb.ai/emanuelevivoli/huggingface</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/emanuelevivoli/huggingface/runs/2c4orm5k\" target=\"_blank\">https://wandb.ai/emanuelevivoli/huggingface/runs/2c4orm5k</a><br/>\n",
       "                Run data is saved locally in <code>/home/vivoli/Thesis/examples/language-modeling/wandb/run-20210515_081131-2c4orm5k</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|integrations.py:675] 2021-05-15 08:11:33,158 >> Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "You can only call `wandb.watch` once per model.  Pass a new instance of the model if you need to call wandb.watch again in your code.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-7c0ffebf68c5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     63\u001b[0m     ]\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-7-5a7c9eab5cee>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(args_list)\u001b[0m\n\u001b[1;32m    252\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mlast_checkpoint\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m             \u001b[0mcheckpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlast_checkpoint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 254\u001b[0;31m         \u001b[0mtrain_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    255\u001b[0m         \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Saves the tokenizer too for easy upload\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m         \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_result\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/thesis/lib/python3.6/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, **kwargs)\u001b[0m\n\u001b[1;32m   1214\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1215\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1216\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallback_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m         \u001b[0;31m# Skip the first epochs_trained epochs to get the random state of the dataloader at the right point.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/thesis/lib/python3.6/site-packages/transformers/trainer_callback.py\u001b[0m in \u001b[0;36mon_train_begin\u001b[0;34m(self, args, state, control)\u001b[0m\n\u001b[1;32m    338\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_train_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTrainingArguments\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTrainerState\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontrol\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTrainerControl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m         \u001b[0mcontrol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_training_stop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 340\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"on_train_begin\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontrol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    342\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_train_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTrainingArguments\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTrainerState\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontrol\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTrainerControl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/thesis/lib/python3.6/site-packages/transformers/trainer_callback.py\u001b[0m in \u001b[0;36mcall_event\u001b[0;34m(self, event, args, state, control, **kwargs)\u001b[0m\n\u001b[1;32m    386\u001b[0m                 \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m                 \u001b[0meval_dataloader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval_dataloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 388\u001b[0;31m                 \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    389\u001b[0m             )\n\u001b[1;32m    390\u001b[0m             \u001b[0;31m# A Callback can skip the return of `control` if it doesn't change it.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/thesis/lib/python3.6/site-packages/transformers/integrations.py\u001b[0m in \u001b[0;36mon_train_begin\u001b[0;34m(self, args, state, control, model, **kwargs)\u001b[0m\n\u001b[1;32m    715\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wandb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinish\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    716\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialized\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 717\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    718\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    719\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_train_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontrol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/thesis/lib/python3.6/site-packages/transformers/integrations.py\u001b[0m in \u001b[0;36msetup\u001b[0;34m(self, args, state, model, **kwargs)\u001b[0m\n\u001b[1;32m    705\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_torch_tpu_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetenv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"WANDB_WATCH\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"false\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    706\u001b[0m                 self._wandb.watch(\n\u001b[0;32m--> 707\u001b[0;31m                     \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetenv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"WANDB_WATCH\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"gradients\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_freq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogging_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    708\u001b[0m                 )\n\u001b[1;32m    709\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/thesis/lib/python3.6/site-packages/wandb/sdk/wandb_watch.py\u001b[0m in \u001b[0;36mwatch\u001b[0;34m(models, criterion, log, log_freq, idx)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         graph = wandb.wandb_torch.TorchGraph.hook_torch(\n\u001b[0;32m---> 94\u001b[0;31m             \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgraph_idx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mglobal_idx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m         )\n\u001b[1;32m     96\u001b[0m         \u001b[0mgraphs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/thesis/lib/python3.6/site-packages/wandb/wandb_torch.py\u001b[0m in \u001b[0;36mhook_torch\u001b[0;34m(cls, model, criterion, graph_idx)\u001b[0m\n\u001b[1;32m    321\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mhook_torch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgraph_idx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m         \u001b[0mgraph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTorchGraph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 323\u001b[0;31m         \u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhook_torch_modules\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgraph_idx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgraph_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    324\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/thesis/lib/python3.6/site-packages/wandb/wandb_torch.py\u001b[0m in \u001b[0;36mhook_torch_modules\u001b[0;34m(self, module, criterion, prefix, graph_idx)\u001b[0m\n\u001b[1;32m    373\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_wandb_watch_called\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wandb_watch_called\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    374\u001b[0m             raise ValueError(\n\u001b[0;32m--> 375\u001b[0;31m                 \u001b[0;34m\"You can only call `wandb.watch` once per model.  Pass a new instance of the model if you need to call wandb.watch again in your code.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    376\u001b[0m             )\n\u001b[1;32m    377\u001b[0m         \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wandb_watch_called\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: You can only call `wandb.watch` once per model.  Pass a new instance of the model if you need to call wandb.watch again in your code."
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    args_list = [\n",
    "\n",
    "        \"--overwrite_output_dir\"         , 'True',\n",
    "        \n",
    "        \"--do_train\"                     , 'True',\n",
    "        \"--do_eval\"                      , 'True',\n",
    "        \"--do_predict\"                   , 'True', \n",
    "\n",
    "          # DatasetArguments\n",
    "        \"--model_name_or_path\"           , \"allenai/scibert_scivocab_uncased\",\n",
    "        \"--dataset_name\"                 , \"s2orc\", # \"keyphrase\",\n",
    "        \"--dataset_config_name\"          , \"full\",  # \"inspec\",\n",
    "        \n",
    "        # TrainingArguments        \n",
    "            # seed for reproducibility of experiments\n",
    "        \"--seed\"                         , '1234', \n",
    "        \"--output_dir\"                   , \"/home/vivoli/Thesis/output\",\n",
    "        # \"--debug\"                        , \"\", # it is not what we think it is\n",
    "        \n",
    "        \"--run_name\"                     , \"scibert-s2orc\", # \"scibert-keyph\",\n",
    "        \"--num_train_epochs\"             , '1',\n",
    "        \"--per_device_train_batch_size\"  , \"8\", # 16 and 32 end with \"RuntimeError: CUDA out of memory.\"\n",
    "        \"--per_device_eval_batch_size\"   , \"8\", # 16 and 32 end with \"RuntimeError: CUDA out of memory.\"\n",
    "            # custom added\n",
    "        \"--max_seq_length\"               , '512',\n",
    "        \n",
    "        # S2orcArguments & KeyPhArguments\n",
    "        \"--dataset_path\"                 , \"/home/vivoli/Thesis/data\",\n",
    "        \"--data\"                         , \"abstract\",\n",
    "        \"--target\"                       , \"title\",             \n",
    "        \"--classes\"                      , \"mag_field_of_study\", # \"keywords\",\n",
    "        \n",
    "        # S2orcArguments\n",
    "        \"--idxs\"                         , '0',\n",
    "        \"--zipped\"                       , 'True',\n",
    "            # list\n",
    "        \"--mag_field_of_study\"           , \"Computer Science\",    \n",
    "        #     # list\n",
    "        # \"--data\"                         , \"abstract\",\n",
    "        #     # list\n",
    "        # \"--target\"                       , \"title\",             \n",
    "        #     # list\n",
    "        # \"--classes\"                      , \"mag_field_of_study\",\n",
    "        \"--keep_none_papers\"             , 'False',\n",
    "        \"--keep_unused_columns\"          , 'False',\n",
    "        \n",
    "        # RunArguments\n",
    "        #Â \"--run_name\"                     , \"scibert-s2orc\",\n",
    "        \"--run_number\"                   , '0',\n",
    "        \"--run_iteration\"                , '0',\n",
    "        \n",
    "        # LoggingArguments\n",
    "        \"--verbose\"                      , 'True',\n",
    "        \"--debug_log\"                    , 'True',\n",
    "        \"--time\"                         , 'False',\n",
    "        \"--callbacks\"                     , \"WandbCallback,CometCallback,TensorBoardCallback\",\n",
    "        \n",
    "        # EmbeddingArguments\n",
    "        # \"--max_seq_length\"               , '512',\n",
    "        # \"--pooling\"                      , 'none',\n",
    "        # \"--batch_size\"                   , '32'\n",
    "    ]\n",
    "    \n",
    "    main(args_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "thesis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
