{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d772399-19b7-4f5b-96a2-cd975edb2a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2ee8483-fb89-4ac5-aac8-d3a19a174d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b05a4cf-dc56-4d40-a0fb-c652cfe17cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All Imports\n",
    "\n",
    "import argparse\n",
    "import logging\n",
    "import os\n",
    "import io\n",
    "import sys\n",
    "import math\n",
    "import random\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# ðŸ¤— Datasets \n",
    "import datasets\n",
    "from datasets import load_dataset, load_metric\n",
    "\n",
    "# dataclasses and types\n",
    "from dataclasses import field, dataclass\n",
    "from typing import Dict, List, Union, Optional\n",
    "\n",
    "# dataset managements\n",
    "# from torch.utils.data.dataloader import DataLoader\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# data managements\n",
    "import json # load/write data\n",
    "import torch \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# accelerator for speed up experiments/runs\n",
    "# from accelerate import Accelerator\n",
    "\n",
    "# ðŸ¤— Tranformers\n",
    "import transformers\n",
    "from transformers import (\n",
    "    Trainer, # For using Trainer instead of our custom loop\n",
    "    AdamW,\n",
    "    AutoTokenizer, \n",
    "    AutoConfig,\n",
    "    HfArgumentParser,\n",
    "    TrainingArguments as HfTrainingArguments,\n",
    "    SchedulerType,\n",
    "    get_scheduler,\n",
    "    set_seed,\n",
    ")\n",
    "# checkpoint utils\n",
    "from transformers.trainer_utils import get_last_checkpoint\n",
    "    \n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "95493e92-6fa4-4db1-b2a1-464103d64318",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports for mlm (mlm_task)\n",
    "\n",
    "\"\"\"\n",
    "Fine-tuning the library models for masked language modeling (BERT, ALBERT, RoBERTa...)\n",
    "on a text file or a dataset without using HuggingFace Trainer.\n",
    "Here is the full list of checkpoints on the hub that can be fine-tuned by this script:\n",
    "https://huggingface.co/models?filter=masked-lm\n",
    "\"\"\"\n",
    "\n",
    "# ðŸ¤— Tranformers\n",
    "from transformers import (\n",
    "    CONFIG_MAPPING,\n",
    "    MODEL_MAPPING,\n",
    "    AutoModel, \n",
    "    AutoModelForMaskedLM,\n",
    "    PreTrainedTokenizer, \n",
    "    DataCollatorForLanguageModeling, \n",
    "    BertForMaskedLM\n",
    ")\n",
    "from transformers.trainer_utils import get_last_checkpoint, is_main_process\n",
    "from transformers.utils import check_min_version\n",
    "\n",
    "# parsing \n",
    "from thesis.utils.parsers.args_parser import parse_args\n",
    "from thesis.utils.general import load_dataset_wrapper\n",
    "\n",
    "MODEL_CONFIG_CLASSES = list(MODEL_MAPPING.keys())\n",
    "MODEL_TYPES = tuple(conf.model_type for conf in MODEL_CONFIG_CLASSES)\n",
    "\n",
    "DICTIONARY_FIELD_NAMES = dict(\n",
    "    train         = ['train'],\n",
    "    test          = ['test', 'debug', 'dev'],\n",
    "    validation    = ['validation', 'valid']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "643560ec",
   "metadata": {},
   "source": [
    "# Imports for glue (seq_class)\n",
    "\n",
    "\"\"\" Finetuning a ðŸ¤— Transformers model for sequence classification on GLUE.\"\"\"\n",
    "\n",
    "# ðŸ¤— Tranformers\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    DataCollatorWithPadding,\n",
    "    PretrainedConfig,\n",
    "    default_data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00560e1d",
   "metadata": {},
   "source": [
    "from lensNLP import (\n",
    "    mlm_task,\n",
    "    seq_class\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e9916725-3c2a-4008-aed1-fe76f09ab9fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33memanuelevivoli\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: WANDB_NOTEBOOK_NAME=\"run_mlm\"\n",
      "env: WANDB_WATCH=all\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.30<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">happy-dew-8</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/emanuelevivoli/run_mlm\" target=\"_blank\">https://wandb.ai/emanuelevivoli/run_mlm</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/emanuelevivoli/run_mlm/runs/1j7jizr0\" target=\"_blank\">https://wandb.ai/emanuelevivoli/run_mlm/runs/1j7jizr0</a><br/>\n",
       "                Run data is saved locally in <code>/home/vivoli/Thesis/examples/language-modeling/wandb/run-20210514_131107-1j7jizr0</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h1>Run(1j7jizr0)</h1><iframe src=\"https://wandb.ai/emanuelevivoli/run_mlm/runs/1j7jizr0\" style=\"border:none;width:100%;height:400px\"></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7f5992fe89e8>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Logging on WandB\n",
    "import wandb\n",
    "wandb.login()\n",
    "\n",
    "# manually setting the name of this notebook\n",
    "%env WANDB_NOTEBOOK_NAME \"run_mlm\"\n",
    "# Optional: log both gradients and parameters\n",
    "%env WANDB_WATCH=all\n",
    "\n",
    "wandb.init(project=\"run_mlm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "90a0f32c-d2e5-4a49-9012-f0a443200a28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe src=\"https://wandb.ai/emanuelevivoli/run_mlm/runs/1j7jizr0?jupyter=true\" style=\"border:none;width:100%;height:420px\">\n",
       "                </iframe>"
      ],
      "text/plain": [
       "<wandb.jupyter.Run at 0x7f5a4065cb70>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%wandb\n",
    "\n",
    "def main(args_list):\n",
    "\n",
    "    # ------------------\n",
    "    # Parsing arguments\n",
    "    # ------------------\n",
    "    \n",
    "    # If we pass only one argument to the script and it's the path to a json file, it parses it to get our arguments.\n",
    "    # If we pass a list of pairs '--option', 'argument', it uses those to get arguments.\n",
    "    dataset_args, training_args, model_args, run_args, log_args, embedding_args = parse_args(args_list)\n",
    "    \n",
    "    # ------------------\n",
    "    # Checkpoints\n",
    "    # ------------------\n",
    "    \n",
    "    # Detecting last checkpoint.\n",
    "    last_checkpoint = None\n",
    "    if os.path.isdir(training_args.output_dir) and training_args.do_train and not training_args.overwrite_output_dir:\n",
    "        last_checkpoint = get_last_checkpoint(training_args.output_dir)\n",
    "        if last_checkpoint is None and len(os.listdir(training_args.output_dir)) > 0:\n",
    "            raise ValueError(\n",
    "                f\"Output directory ({training_args.output_dir}) already exists and is not empty. \"\n",
    "                \"Use --overwrite_output_dir to overcome.\"\n",
    "            )\n",
    "        elif last_checkpoint is not None and training_args.resume_from_checkpoint is None:\n",
    "            logger.info(\n",
    "                f\"Checkpoint detected, resuming training at {last_checkpoint}. To avoid this behavior, change \"\n",
    "                \"the `--output_dir` or add `--overwrite_output_dir` to train from scratch.\"\n",
    "            )\n",
    "    \n",
    "    # ------------------\n",
    "    # Logging definition\n",
    "    # ------------------\n",
    "\n",
    "    # Setup logging\n",
    "    logging.basicConfig(\n",
    "        format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n",
    "        datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "        handlers=[logging.StreamHandler(sys.stdout)],\n",
    "    )\n",
    "    logger.setLevel(logging.INFO if is_main_process(training_args.local_rank) else logging.WARN)\n",
    "\n",
    "    # Log on each process the small summary:\n",
    "    logger.warning(\n",
    "        f\"Process rank: {training_args.local_rank}, device: {training_args.device}, n_gpu: {training_args.n_gpu}\"\n",
    "        + f\"distributed training: {bool(training_args.local_rank != -1)}, 16-bits training: {training_args.fp16}\"\n",
    "    )\n",
    "    # Set the verbosity to info of the Transformers logger (on main process only):\n",
    "    if is_main_process(training_args.local_rank):\n",
    "        transformers.utils.logging.set_verbosity_info()\n",
    "        transformers.utils.logging.enable_default_handler()\n",
    "        transformers.utils.logging.enable_explicit_format()\n",
    "    logger.info(f\"Training/evaluation parameters {training_args}\")\n",
    "\n",
    "    # ------------------\n",
    "    # Setting seed\n",
    "    # ------------------\n",
    "    \n",
    "    # If passed along, set the training seed now.\n",
    "    if training_args.seed is not None:\n",
    "        set_seed(training_args.seed)\n",
    "\n",
    "    # ------------------\n",
    "    # Getting the datasets\n",
    "    # ------------------\n",
    "\n",
    "    # Get the datasets: you can either provide your own CSV/JSON/TXT training and evaluation files (see below)\n",
    "    # or just provide the name of one of the public datasets available on the hub at https://huggingface.co/datasets/\n",
    "    # (the dataset will be downloaded automatically from the datasets Hub).\n",
    "    # You can also provide the name of some other dataset ('S2orc', 'Keyphrase') that we customly support.\n",
    "    #\n",
    "    # For the 'S2orc' dataset we provide a caching mechanisms in order to speed-up the preprocessing.\n",
    "    # The cache files changes (and are recalculated) everytime the configurations changes.\n",
    "    #\n",
    "    # For CSV/JSON files, this script will use the column called 'text' or the first column if no column called\n",
    "    # 'text' is found. You can easily tweak this behavior (see below).\n",
    "    #\n",
    "    # In distributed training, the load_dataset function guarantee that only one local process can concurrently\n",
    "    # download the dataset.\n",
    "    if dataset_args.dataset_name is not None:\n",
    "        # Getting the load_dataset wrapper that manages huggingface dataset and the custom ones\n",
    "        custom_load_dataset = load_dataset_wrapper()\n",
    "        # Loading the raw data based on input (and default) values of arguments\n",
    "        raw_datasets = custom_load_dataset(dataset_args, training_args, model_args, run_args, log_args, embedding_args)\n",
    "    else:\n",
    "        # If the files 'train_file' and 'validation_file' are specified\n",
    "        # data_files is composed by those elements.\n",
    "        data_files = {}\n",
    "        if dataset_args.train_file is not None:\n",
    "            data_files[\"train\"] = dataset_args.train_file\n",
    "        if dataset_args.validation_file is not None:\n",
    "            data_files[\"validation\"] = dataset_args.validation_file\n",
    "        extension = dataset_args.train_file.split(\".\")[-1]\n",
    "        if extension == \"txt\":\n",
    "            extension = \"text\"\n",
    "        elif extension == \"jsonl\": # jsonl files are file with json element per row\n",
    "            extension = \"json\"\n",
    "        raw_datasets = load_dataset(extension, data_files=data_files)\n",
    "    # See more about loading any type of standard or custom dataset (from files, python dict, pandas DataFrame, etc) at\n",
    "    # https://huggingface.co/docs/datasets/loading_datasets.html.\n",
    "\n",
    "    # The Datasets in the raw form can have different form of key names (depending on the configuration).\n",
    "    # We need all datasets to contain 'train', 'test', 'validation' keys, if not we change the dictionary keys' name\n",
    "    # based on the `names_tuple` and conseguently on `names_map`.\n",
    "    def format_key_names(raw_datasets):\n",
    "        # The creation of `names_map` happens to be here\n",
    "        # For every element in the values lists, one dictionary entry is added \n",
    "        # with (k,v): k=Value of the list, v=Key such as 'train', etc.\n",
    "        def names_dict_generator(names_tuple: dict):\n",
    "            names_map = dict()\n",
    "            for key, values in names_tuple.items():\n",
    "                for value in values:\n",
    "                    names_map[value] = key\n",
    "            return names_map\n",
    "        names_map = names_dict_generator(DICTIONARY_FIELD_NAMES)\n",
    "        split_names = raw_datasets.keys()\n",
    "        for split_name in split_names:\n",
    "            new_split_name = names_map.get(split_name)\n",
    "            if split_name != new_split_name:\n",
    "                raw_datasets[new_split_name] = raw_datasets.pop(split_name)  \n",
    "        return raw_datasets\n",
    "\n",
    "    logger.info(f\"Formatting DatasetDict keys\")\n",
    "    datasets = format_key_names(raw_datasets)\n",
    "\n",
    "    # ------------------\n",
    "    # Load tokenizer and\n",
    "    # pretrained model\n",
    "    # ------------------\n",
    "\n",
    "    # In distributed training, the .from_pretrained methods guarantee that only one local process can concurrently\n",
    "    # download model & vocab.\n",
    "    config_kwargs = {\n",
    "        \"cache_dir\": model_args.cache_dir,\n",
    "        \"revision\": model_args.model_revision,\n",
    "        \"use_auth_token\": True if model_args.use_auth_token else None,\n",
    "    }\n",
    "    if model_args.config_name:\n",
    "        config = AutoConfig.from_pretrained(model_args.config_name, **config_kwargs)\n",
    "    elif model_args.model_name_or_path:\n",
    "        config = AutoConfig.from_pretrained(model_args.model_name_or_path, **config_kwargs)\n",
    "    else:\n",
    "        config = CONFIG_MAPPING[model_args.model_type]()\n",
    "        logger.warning(\"You are instantiating a new config instance from scratch.\")\n",
    "\n",
    "    tokenizer_kwargs = {\n",
    "        \"cache_dir\": model_args.cache_dir,\n",
    "        \"use_fast\": model_args.use_fast_tokenizer,\n",
    "        \"revision\": model_args.model_revision,\n",
    "        \"use_auth_token\": True if model_args.use_auth_token else None,\n",
    "    }\n",
    "    if model_args.tokenizer_name:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_args.tokenizer_name, **tokenizer_kwargs)\n",
    "    elif model_args.model_name_or_path:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_args.model_name_or_path, **tokenizer_kwargs)\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            \"You are instantiating a new tokenizer from scratch. This is not supported by this script.\"\n",
    "            \"You can do it from another script, save it, and load it from here, using --tokenizer_name.\"\n",
    "        )\n",
    "\n",
    "    if model_args.model_name_or_path:\n",
    "        model = AutoModelForMaskedLM.from_pretrained(\n",
    "            model_args.model_name_or_path,\n",
    "            from_tf=bool(\".ckpt\" in model_args.model_name_or_path),\n",
    "            config=config,\n",
    "            cache_dir=model_args.cache_dir,\n",
    "            revision=model_args.model_revision,\n",
    "            use_auth_token=True if model_args.use_auth_token else None,\n",
    "        )\n",
    "    else:\n",
    "        logger.info(\"Training new model from scratch\")\n",
    "        model = AutoModelForMaskedLM.from_config(config)\n",
    "\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "    # Preprocessing the datasets.\n",
    "    # First we tokenize all the texts.\n",
    "    if training_args.do_train:\n",
    "        column_names = datasets[\"train\"].column_names\n",
    "    else:\n",
    "        column_names = datasets[\"validation\"].column_names\n",
    "    text_column_name = \"text\" if \"text\" in column_names else column_names[0]\n",
    "\n",
    "    if dataset_args.max_seq_length is None:\n",
    "        max_seq_length = tokenizer.model_max_length\n",
    "        if max_seq_length > 1024:\n",
    "            logger.warning(\n",
    "                f\"The tokenizer picked seems to have a very large `model_max_length` ({tokenizer.model_max_length}). \"\n",
    "                \"Picking 1024 instead. You can change that default value by passing --max_seq_length xxx.\"\n",
    "            )\n",
    "            max_seq_length = 1024\n",
    "    else:\n",
    "        if dataset_args.max_seq_length > tokenizer.model_max_length:\n",
    "            logger.warning(\n",
    "                f\"The max_seq_length passed ({dataset_args.max_seq_length}) is larger than the maximum length for the\"\n",
    "                f\"model ({tokenizer.model_max_length}). Using max_seq_length={tokenizer.model_max_length}.\"\n",
    "            )\n",
    "        max_seq_length = min(dataset_args.max_seq_length, tokenizer.model_max_length)\n",
    "\n",
    "    if dataset_args.line_by_line:\n",
    "        # When using line_by_line, we just tokenize each nonempty line.\n",
    "        padding = \"max_length\" if training_args.pad_to_max_length else False\n",
    "\n",
    "        def tokenize_function(examples):\n",
    "            # Remove empty lines\n",
    "            examples[\"text\"] = [line for line in examples[\"text\"] if len(line) > 0 and not line.isspace()]\n",
    "            return tokenizer(\n",
    "                examples[\"text\"],\n",
    "                padding=padding,\n",
    "                truncation=True,\n",
    "                max_length=max_seq_length,\n",
    "                # We use this option because DataCollatorForLanguageModeling (see below) is more efficient when it\n",
    "                # receives the `special_tokens_mask`.\n",
    "                return_special_tokens_mask=True,\n",
    "            )\n",
    "\n",
    "        tokenized_datasets = datasets.map(\n",
    "            tokenize_function,\n",
    "            batched=True,\n",
    "            num_proc=dataset_args.preprocessing_num_workers,\n",
    "            remove_columns=[text_column_name],\n",
    "            load_from_cache_file=not dataset_args.overwrite_cache,\n",
    "        )\n",
    "    else:\n",
    "        # Otherwise, we tokenize every text, then concatenate them together before splitting them in smaller parts.\n",
    "        # We use `return_special_tokens_mask=True` because DataCollatorForLanguageModeling (see below) is more\n",
    "        # efficient when it receives the `special_tokens_mask`.\n",
    "        def tokenize_function(examples):\n",
    "            return tokenizer(examples[text_column_name], return_special_tokens_mask=True)\n",
    "\n",
    "        tokenized_datasets = datasets.map(\n",
    "            tokenize_function,\n",
    "            batched=True,\n",
    "            num_proc=dataset_args.preprocessing_num_workers,\n",
    "            remove_columns=column_names,\n",
    "            load_from_cache_file=not dataset_args.overwrite_cache,\n",
    "        )\n",
    "\n",
    "        # Main data processing function that will concatenate all texts from our dataset and generate chunks of\n",
    "        # max_seq_length.\n",
    "        def group_texts(examples):\n",
    "            # Concatenate all texts.\n",
    "            concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "            total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "            # We drop the small remainder, we could add padding if the model supported it instead of this drop, you can\n",
    "            # customize this part to your needs.\n",
    "            total_length = (total_length // max_seq_length) * max_seq_length\n",
    "            # Split by chunks of max_len.\n",
    "            result = {\n",
    "                k: [t[i : i + max_seq_length] for i in range(0, total_length, max_seq_length)]\n",
    "                for k, t in concatenated_examples.items()\n",
    "            }\n",
    "            return result\n",
    "\n",
    "        # Note that with `batched=True`, this map processes 1,000 texts together, so group_texts throws away a\n",
    "        # remainder for each of those groups of 1,000 texts. You can adjust that batch_size here but a higher value\n",
    "        # might be slower to preprocess.\n",
    "        #\n",
    "        # To speed up this part, we use multiprocessing. See the documentation of the map method for more information:\n",
    "        # https://huggingface.co/docs/datasets/package_reference/main_classes.html#datasets.Dataset.map\n",
    "\n",
    "        tokenized_datasets = tokenized_datasets.map(\n",
    "            group_texts,\n",
    "            batched=True,\n",
    "            num_proc=dataset_args.preprocessing_num_workers,\n",
    "            load_from_cache_file=not dataset_args.overwrite_cache,\n",
    "        )\n",
    "\n",
    "    if training_args.do_train:\n",
    "        if \"train\" not in tokenized_datasets:\n",
    "            raise ValueError(\"--do_train requires a train dataset\")\n",
    "        train_dataset = tokenized_datasets[\"train\"]\n",
    "        if dataset_args.max_train_samples is not None:\n",
    "            train_dataset = train_dataset.select(range(dataset_args.max_train_samples))\n",
    "\n",
    "    if training_args.do_eval:\n",
    "        if \"validation\" not in tokenized_datasets:\n",
    "            raise ValueError(\"--do_eval requires a validation dataset\")\n",
    "        eval_dataset = tokenized_datasets[\"validation\"]\n",
    "        if dataset_args.max_eval_samples is not None:\n",
    "            eval_dataset = eval_dataset.select(range(dataset_args.max_eval_samples))\n",
    "\n",
    "    # Data collator\n",
    "    # This one will take care of randomly masking the tokens.\n",
    "    pad_to_multiple_of_8 = dataset_args.line_by_line and training_args.fp16 and not training_args.pad_to_max_length\n",
    "    data_collator = DataCollatorForLanguageModeling(\n",
    "        tokenizer=tokenizer,\n",
    "        mlm_probability=dataset_args.mlm_probability,\n",
    "        pad_to_multiple_of=8 if pad_to_multiple_of_8 else None,\n",
    "    )\n",
    "\n",
    "    # Initialize our Trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset if training_args.do_train else None,\n",
    "        eval_dataset=eval_dataset if training_args.do_eval else None,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "    )\n",
    "\n",
    "    # Training\n",
    "    if training_args.do_train:\n",
    "        checkpoint = None\n",
    "        if training_args.resume_from_checkpoint is not None:\n",
    "            checkpoint = training_args.resume_from_checkpoint\n",
    "        elif last_checkpoint is not None:\n",
    "            checkpoint = last_checkpoint\n",
    "        train_result = trainer.train(resume_from_checkpoint=checkpoint)\n",
    "        trainer.save_model()  # Saves the tokenizer too for easy upload\n",
    "        metrics = train_result.metrics\n",
    "\n",
    "        max_train_samples = (\n",
    "            dataset_args.max_train_samples if dataset_args.max_train_samples is not None else len(train_dataset)\n",
    "        )\n",
    "        metrics[\"train_samples\"] = min(max_train_samples, len(train_dataset))\n",
    "\n",
    "        trainer.log_metrics(\"train\", metrics)\n",
    "        trainer.save_metrics(\"train\", metrics)\n",
    "        trainer.save_state()\n",
    "\n",
    "    # Evaluation\n",
    "    if training_args.do_eval:\n",
    "        logger.info(\"*** Evaluate ***\")\n",
    "\n",
    "        metrics = trainer.evaluate()\n",
    "\n",
    "        max_eval_samples = dataset_args.max_eval_samples if dataset_args.max_eval_samples is not None else len(eval_dataset)\n",
    "        metrics[\"eval_samples\"] = min(max_eval_samples, len(eval_dataset))\n",
    "        perplexity = math.exp(metrics[\"eval_loss\"])\n",
    "        metrics[\"perplexity\"] = perplexity\n",
    "\n",
    "        trainer.log_metrics(\"eval\", metrics)\n",
    "        trainer.save_metrics(\"eval\", metrics)\n",
    "\n",
    "    if training_args.push_to_hub:\n",
    "        kwargs = {\"finetuned_from\": model_args.model_name_or_path, \"tags\": \"fill-mask\"}\n",
    "        if training_args.dataset_name is not None:\n",
    "            kwargs[\"dataset_tags\"] = training_args.dataset_name\n",
    "            if training_args.dataset_config_name is not None:\n",
    "                kwargs[\"dataset_args\"] = training_args.dataset_config_name\n",
    "                kwargs[\"dataset\"] = f\"{training_args.dataset_name} {training_args.dataset_config_name}\"\n",
    "            else:\n",
    "                kwargs[\"dataset\"] = training_args.dataset_name\n",
    "\n",
    "        trainer.push_to_hub(**kwargs)\n",
    "\n",
    "\n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "25c83995-a704-4f2b-8af8-7202757910cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "05/14/2021 13:11:52 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
      "05/14/2021 13:11:52 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(output_dir='output', overwrite_output_dir=True, do_train=True, do_eval=True, do_predict=True, evaluation_strategy=<IntervalStrategy.NO: 'no'>, prediction_loss_only=False, per_device_train_batch_size=8, per_device_eval_batch_size=8, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=5e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=1, max_steps=-1, lr_scheduler_type=<SchedulerType.LINEAR: 'linear'>, warmup_ratio=0.0, warmup_steps=0, logging_dir='./logs', logging_strategy=<IntervalStrategy.STEPS: 'steps'>, logging_first_step=False, logging_steps=500, save_strategy=<IntervalStrategy.STEPS: 'steps'>, save_steps=500, save_total_limit=None, no_cuda=False, seed=1234, fp16=False, fp16_opt_level='O1', fp16_backend='auto', fp16_full_eval=False, local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=[], dataloader_drop_last=False, eval_steps=500, dataloader_num_workers=0, past_index=-1, run_name='scibert-s2orc', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, sharded_ddp=[], deepspeed=None, label_smoothing_factor=0.0, adafactor=False, group_by_length=False, length_column_name='length', report_to=['wandb'], ddp_find_unused_parameters=None, dataloader_pin_memory=True, skip_memory_metrics=False, use_legacy_prediction_loop=False, push_to_hub=False, resume_from_checkpoint=None, mp_parameters='', max_train_steps=None, num_warmup_steps=0, max_seq_length=512, line_by_line=False, preprocessing_num_workers=None, overwrite_cache=False, mlm_probability=0.15)\n",
      "loaded result from cache for function S2orcConfig.get_filenames.<locals>._get_filenames    \n",
      "loaded result from cache for function S2orcConfig.get_completed_filenames.<locals>._get_completed_filenames    \n",
      "loaded result from cache for function S2orcConfig.get_toread_chunks.<locals>._get_toread_chunks    \n",
      "loaded result from cache for function s2orc_multichunk_read.<locals>._s2orc_multichunk_read    \n",
      "loaded result from cache for function get_dataset.<locals>._get_dataset    \n",
      "05/14/2021 13:12:03 - INFO - __main__ -   Formatting DatasetDict keys\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|configuration_utils.py:517] 2021-05-14 13:12:03,614 >> loading configuration file https://huggingface.co/allenai/scibert_scivocab_uncased/resolve/main/config.json from cache at /home/vivoli/.cache/huggingface/transformers/858852fd2471ce39075378592ddc87f5a6551e64c6825d1b92c8dab9318e0fc3.03ff9e9f998b9a9d40647a2148a202e3fb3d568dc0f170dda9dda194bab4d5dd\n",
      "[INFO|configuration_utils.py:553] 2021-05-14 13:12:03,640 >> Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.6.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 31090\n",
      "}\n",
      "\n",
      "[INFO|configuration_utils.py:517] 2021-05-14 13:12:04,023 >> loading configuration file https://huggingface.co/allenai/scibert_scivocab_uncased/resolve/main/config.json from cache at /home/vivoli/.cache/huggingface/transformers/858852fd2471ce39075378592ddc87f5a6551e64c6825d1b92c8dab9318e0fc3.03ff9e9f998b9a9d40647a2148a202e3fb3d568dc0f170dda9dda194bab4d5dd\n",
      "[INFO|configuration_utils.py:553] 2021-05-14 13:12:04,025 >> Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.6.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 31090\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-05-14 13:12:05,883 >> loading file https://huggingface.co/allenai/scibert_scivocab_uncased/resolve/main/vocab.txt from cache at /home/vivoli/.cache/huggingface/transformers/33593020f507d72099bd84ea6cd2296feb424fecd62d4a8edcc2a02899af6e29.38339d84e6e392addd730fd85fae32652c4cc7c5423633d6fa73e5f7937bbc38\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-05-14 13:12:05,921 >> loading file https://huggingface.co/allenai/scibert_scivocab_uncased/resolve/main/tokenizer.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-05-14 13:12:05,924 >> loading file https://huggingface.co/allenai/scibert_scivocab_uncased/resolve/main/added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-05-14 13:12:05,926 >> loading file https://huggingface.co/allenai/scibert_scivocab_uncased/resolve/main/special_tokens_map.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-05-14 13:12:05,927 >> loading file https://huggingface.co/allenai/scibert_scivocab_uncased/resolve/main/tokenizer_config.json from cache at None\n",
      "[INFO|modeling_utils.py:1155] 2021-05-14 13:12:06,323 >> loading weights file https://huggingface.co/allenai/scibert_scivocab_uncased/resolve/main/pytorch_model.bin from cache at /home/vivoli/.cache/huggingface/transformers/de14937a851e8180a2bc5660c0041d385f8a0c62b1b2ccafa46df31043a2390c.74830bb01a0ffcdeaed8be9916312726d0c4cd364ac6fc15b375f789eaff4cbb\n",
      "[WARNING|modeling_utils.py:1331] 2021-05-14 13:12:07,168 >> Some weights of the model checkpoint at allenai/scibert_scivocab_uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "[INFO|modeling_utils.py:1348] 2021-05-14 13:12:07,171 >> All the weights of BertForMaskedLM were initialized from the model checkpoint at allenai/scibert_scivocab_uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForMaskedLM for predictions without further training.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e8e88ffa74b46df962d263996e66422",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=72.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f26fc27034af4461a527b5635411e0e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=9.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "094acab6d0e34d27b7849cafb4e63c79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=9.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c38868ed71842cebee4562e35617144",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=72.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8534af02a17b4a53883f84b9abe2ae9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=9.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99bc9d0ec0bd4d6cab818854d077007e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=9.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'TrainingArguments' object has no attribute 'max_train_samples'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-c436a12ff168>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     63\u001b[0m     ]\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-6-8e1a933cb2ee>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(args_list)\u001b[0m\n\u001b[1;32m    271\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"--do_train requires a train dataset\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m         \u001b[0mtrain_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenized_datasets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"train\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 273\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mtraining_args\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_train_samples\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    274\u001b[0m             \u001b[0mtrain_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_args\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_train_samples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'TrainingArguments' object has no attribute 'max_train_samples'"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    args_list = [\n",
    "\n",
    "        \"--do_train\"                     , 'True',\n",
    "        \"--do_eval\"                      , 'True',\n",
    "        \"--do_predict\"                   , 'True', \n",
    "        \n",
    "        \"--overwrite_output_dir\"         , 'True', \n",
    "        \n",
    "          # DatasetArguments\n",
    "        \"--model_name_or_path\"           , \"allenai/scibert_scivocab_uncased\",\n",
    "        \"--dataset_name\"                 , \"s2orc\", # \"keyphrase\",\n",
    "        \"--dataset_config_name\"          , \"full\",  # \"inspec\",\n",
    "        \n",
    "        # TrainingArguments        \n",
    "            # seed for reproducibility of experiments\n",
    "        \"--seed\"                         , '1234', \n",
    "        \"--output_dir\"                   , \"output\",\n",
    "        # \"--debug\"                        , \"\", # it is not what we think it is\n",
    "        \n",
    "        \"--run_name\"                     , \"scibert-s2orc\", # \"scibert-keyph\",\n",
    "        \"--num_train_epochs\"             , '1',\n",
    "        \"--per_device_train_batch_size\"  , \"8\", # 16 and 32 end with \"RuntimeError: CUDA out of memory.\"\n",
    "        \"--per_device_eval_batch_size\"   , \"8\", # 16 and 32 end with \"RuntimeError: CUDA out of memory.\"\n",
    "            # custom added\n",
    "        \"--max_seq_length\"               , '512',\n",
    "        \n",
    "        # S2orcArguments & KeyPhArguments\n",
    "        \"--dataset_path\"                 , \"/home/vivoli/Thesis/data\",\n",
    "        \"--data\"                         , \"abstract\",\n",
    "        \"--target\"                       , \"title\",             \n",
    "        \"--classes\"                      , \"mag_field_of_study\", # \"keywords\",\n",
    "        \n",
    "        # S2orcArguments\n",
    "        \"--idxs\"                         , '0',\n",
    "        \"--zipped\"                       , 'True',\n",
    "            # list\n",
    "        \"--mag_field_of_study\"           , \"Computer Science\",    \n",
    "        #     # list\n",
    "        # \"--data\"                         , \"abstract\",\n",
    "        #     # list\n",
    "        # \"--target\"                       , \"title\",             \n",
    "        #     # list\n",
    "        # \"--classes\"                      , \"mag_field_of_study\",\n",
    "        \"--keep_none_papers\"             , 'False',\n",
    "        \"--keep_unused_columns\"          , 'False',\n",
    "        \n",
    "        # RunArguments\n",
    "        \"--run_name\"                     , \"scibert-s2orc\",\n",
    "        \"--run_number\"                   , '0',\n",
    "        \"--run_iteration\"                , '0',\n",
    "        \n",
    "        # LoggingArguments\n",
    "        \"--verbose\"                      , 'True',\n",
    "        \"--debug_log\"                    , 'True',\n",
    "        \"--time\"                         , 'False',\n",
    "        \"--callback\"                     , \"WandbCallback\",\n",
    "        \n",
    "        # EmbeddingArguments\n",
    "        # \"--max_seq_length\"               , '512',\n",
    "        # \"--pooling\"                      , 'none',\n",
    "        # \"--batch_size\"                   , '32'\n",
    "    ]\n",
    "    \n",
    "    main(args_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f8811f9-6b8d-4199-baef-319eac7193e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "thesis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
