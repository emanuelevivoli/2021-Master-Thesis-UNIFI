{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# S2orc (exploration, clustering & visualization)\n",
    "---\n",
    "---\n",
    "For presenting some results we need to analyze (and rapidly compare) some of the methods we used untill now in order to discriminates between paper's `field_of_study` based on their `title` and `abstract`.\n",
    "This notebook is an extention of some previous work done by Master's students from University of Florence (cite here)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "From each scientific paper we took the `title` and the `abstract`, as well as a property identifying the field in witch the article pertrains.\n",
    "The dataset (only 1000 elements) has been selected randomly from a full-version of 80M papers from different fields.\n",
    "The field of studies (that are called in the dataset `mag_field_of_study`) are the following:\n",
    "\n",
    "| Field of study | All papers | Full text |\n",
    "|----------------|------------|-----------|\n",
    "| Medicine       | 12.8M      | 1.8M      |\n",
    "| Biology        | 9.6M       | 1.6M      |\n",
    "| Chemistry      | 8.7M       | 484k      |\n",
    "| n/a            | 7.7M       | 583k      |\n",
    "| Engineering    | 6.3M       | 228k      |\n",
    "| Comp Sci       | 6.0M       | 580k      |\n",
    "| Physics        | 4.9M       | 838k      |\n",
    "| Mat Sci        | 4.6M       | 213k      |\n",
    "| Math           | 3.9M       | 669k      |\n",
    "| Psychology     | 3.4M       | 316k      |\n",
    "| Economics      | 2.3M       | 198k      |\n",
    "| Poli Sci       | 1.8M       | 69k       |\n",
    "| Business       | 1.8M       | 94k       |\n",
    "| Geology        | 1.8M       | 115k      |\n",
    "| Sociology      | 1.6M       | 93k       |\n",
    "| Geography      | 1.4M       | 58k       |\n",
    "| Env Sci        | 766k       | 52k       |\n",
    "| Art            | 700k       | 16k       |\n",
    "| History        | 690k       | 22k       |\n",
    "| Philosophy     | 384k       | 15k       |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note for reproducibility: `data` is a `DatasetDict` object composed by `Dataset` object for every key (in `train`, `test`, `valid`):\n",
    "\n",
    "```python\n",
    "{ \n",
    "    \"train\": Dataset,\n",
    "    \"test\" : Dataset,\n",
    "    \"valid\": Dataset\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAIN_PATH = '/home/vivoli/Thesis'\n",
    "DATA_PATH = '/home/vivoli/Thesis/data'\n",
    "OUT_PATH  = '/home/vivoli/Thesis/outputs/'\n",
    "ARGS_PATH = '/home/vivoli/Thesis/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "from thesis.utils.general import load_dataset_wrapper\n",
    "from thesis.utils.parsers.args_parser import parse_args\n",
    "\n",
    "DICTIONARY_FIELD_NAMES = dict(\n",
    "    train         = ['train'],\n",
    "    test          = ['test', 'debug', 'dev'],\n",
    "    validation    = ['validation', 'valid']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the dataset\n",
    "---\n",
    "In order to get the dataset we need to create a dictionary with the DatasetArguments (params) and use our \"library\" called `thesis`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model_name_or_path': 'allenai/scibert_scivocab_uncased', 'dataset_name': 's2orc', 'dataset_config_name': 'full', 'seed': '1234', 'output_dir': '/home/vivoli/Thesis/output', 'num_train_epochs': '1', 'per_device_train_batch_size': '8', 'per_device_eval_batch_size': '8', 'max_seq_length': '512', 'dataset_path': '/home/vivoli/Thesis/data', 'data': 'abstract', 'target': 'title', 'classes': 'mag_field_of_study', 'idxs': '0', 'zipped': 'True', 'mag_field_of_study': None, 'keep_none_papers': 'False', 'keep_unused_columns': 'False', 'run_name': 'scibert-s2orc', 'run_number': '0', 'run_iteration': '0', 'verbose': 'True', 'debug_log': 'False', 'time': 'True', 'callbacks': 'WandbCallback,CometCallback,TensorBoardCallback'}\n"
     ]
    }
   ],
   "source": [
    "# ------------------\n",
    "# Creating Arguments\n",
    "# ------------------\n",
    "\n",
    "# create arguments dictionary\n",
    "args = dict(\n",
    "    \n",
    "    # nocache = \"True\",\n",
    "\n",
    "    # DatasetArguments\n",
    "    model_name_or_path           = \"allenai/scibert_scivocab_uncased\",\n",
    "    dataset_name                 = \"s2orc\", # \"keyphrase\",\n",
    "    dataset_config_name          = \"full\",  # \"inspec\",\n",
    "\n",
    "    # TrainingArguments        \n",
    "    seed                         = '1234', \n",
    "    output_dir                   = \"/home/vivoli/Thesis/output\",\n",
    "    \n",
    "    num_train_epochs             = '1',\n",
    "    per_device_train_batch_size  = \"8\", # 16 and 32 end with \"RuntimeError: CUDA out of memory.\"\n",
    "    per_device_eval_batch_size   = \"8\", # 16 and 32 end with \"RuntimeError: CUDA out of memory.\"\n",
    "    max_seq_length               = '512',\n",
    "\n",
    "    # S2orcArguments & KeyPhArguments\n",
    "    dataset_path                 = \"/home/vivoli/Thesis/data\",\n",
    "    data                         = \"abstract\",\n",
    "    target                       = \"title\",             \n",
    "    classes                      = \"mag_field_of_study\", # \"keywords\",\n",
    "\n",
    "    # S2orcArguments\n",
    "    idxs                         = '0',\n",
    "    zipped                       = True,\n",
    "    mag_field_of_study           = None, # \"Computer Science\" but we want all\n",
    "    keep_none_papers             = False,\n",
    "    keep_unused_columns          = False,\n",
    "\n",
    "    # RunArguments\n",
    "    run_name                     = \"scibert-s2orc\",\n",
    "    run_number                   = '0',\n",
    "    run_iteration                = '0',\n",
    "\n",
    "    # LoggingArguments\n",
    "    verbose                      = True,\n",
    "    debug_log                    = False,\n",
    "    time                         = True,\n",
    "    callbacks                    = \"WandbCallback,CometCallback,TensorBoardCallback\",\n",
    ")\n",
    "\n",
    "# save dictionary to file\n",
    "import json\n",
    "import os\n",
    "\n",
    "ARGS_FILE = 'arguments.json'\n",
    "\n",
    "with open(os.path.join(ARGS_PATH, ARGS_FILE), 'w') as fp:\n",
    "    json.dump(args, fp)\n",
    "    \n",
    "print(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s2orc\n"
     ]
    }
   ],
   "source": [
    "# ------------------\n",
    "# Parsing the Arguments\n",
    "# ------------------\n",
    "\n",
    "dataset_args, training_args, model_args, run_args, log_args, embedding_args = parse_args(os.path.join(ARGS_PATH, ARGS_FILE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------\n",
    "# Getting the datasets\n",
    "# ------------------\n",
    "\n",
    "# Getting the load_dataset wrapper that manages huggingface dataset and the custom ones\n",
    "custom_load_dataset = load_dataset_wrapper()\n",
    "# Loading the raw data based on input (and default) values of arguments\n",
    "raw_datasets = custom_load_dataset(dataset_args, training_args, model_args, run_args, log_args, embedding_args)\n",
    "\n",
    "\n",
    "# The Datasets in the raw form can have different form of key names (depending on the configuration).\n",
    "# We need all datasets to contain 'train', 'test', 'validation' keys, if not we change the dictionary keys' name\n",
    "# based on the `names_tuple` and conseguently on `names_map`.\n",
    "def format_key_names(raw_datasets):\n",
    "    # The creation of `names_map` happens to be here\n",
    "    # For every element in the values lists, one dictionary entry is added \n",
    "    # with (k,v): k=Value of the list, v=Key such as 'train', etc.\n",
    "    def names_dict_generator(names_tuple: dict):\n",
    "        names_map = dict()\n",
    "        for key, values in names_tuple.items():\n",
    "            for value in values:\n",
    "                names_map[value] = key\n",
    "        return names_map\n",
    "    names_map = names_dict_generator(DICTIONARY_FIELD_NAMES)\n",
    "    split_names = raw_datasets.keys()\n",
    "    for split_name in split_names:\n",
    "        new_split_name = names_map.get(split_name)\n",
    "        if split_name != new_split_name:\n",
    "            raw_datasets[new_split_name] = raw_datasets.pop(split_name)  \n",
    "    return raw_datasets\n",
    "\n",
    "logger.info(f\"Formatting DatasetDict keys\")\n",
    "datasets = format_key_names(raw_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "thesis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
