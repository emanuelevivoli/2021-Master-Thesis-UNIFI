{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "abc507c2",
   "metadata": {},
   "source": [
    "# Datasets Francesco\n",
    "---\n",
    "\n",
    "In this notebook we'll build/implement the Dataset classes we need to work with all the dataset we have.\n",
    "First we will introduce the datasets, then we will separate those based on the usage we are going to make of them, then we will use/build/implement our classes in order to manage those different datasets and tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd92220f",
   "metadata": {},
   "source": [
    "# 0.0 Utils\n",
    "---\n",
    "\n",
    "We will be using the ðŸ¤—*Datasets* library, so first we need to install it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a34fd34c",
   "metadata": {},
   "source": [
    "> !pip install datasets > datasets_installation.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cccb1d7",
   "metadata": {},
   "source": [
    "We'll be using also the ðŸ¤— *Tranformers* library, as we need a tokenizer and a vocab."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8286fe39",
   "metadata": {},
   "source": [
    "> !pip install transformers > transformers_installation.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f23b1ec",
   "metadata": {},
   "source": [
    "Let's define all the `imports` and `hyperparameters` in one place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "612a48d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import time\n",
    "import os\n",
    "import json\n",
    "from transformers import AutoTokenizer, AutoModel, PreTrainedTokenizer, DataCollatorForLanguageModeling, Trainer, BertForMaskedLM, TrainingArguments\n",
    "import torch\n",
    "import transformers\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from typing import Dict, List, Union\n",
    "from datasets import load_dataset, DatasetDict\n",
    "from enum import Enum\n",
    "#Â from pydash.arrays import pull_at\n",
    "import numpy as np\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c7e901d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------- #\n",
    "#           Hyperparameters\n",
    "# ----------------------------------- #\n",
    "RUN_NAME = 'scibert-s2orc'\n",
    "RUN_NUMBER = 2\n",
    "RUN_ITER = 1\n",
    "\n",
    "\n",
    "# --------- logging         --------- #\n",
    "verbose = True # logging function description start and end\n",
    "debug = False # logging element values\n",
    "print_debug = False\n",
    "time_debug = True\n",
    "\n",
    "# to avoid warning messages from huggingface-tranformers you must comment this line\n",
    "transformers.logging.set_verbosity_info()\n",
    "\n",
    "# --------- preprocessing   --------- #\n",
    "# in **partial_prepare_data**\n",
    "remove_None_papers = True # if True, remove papers with None eather in abstract or title\n",
    "remove_Unused_columns = True\n",
    "# in **preprocess**\n",
    "clean_None_data = False # if True, changes all the None (abstract of title) to ''\n",
    "remove_None_data = False # if True (and clean_None_data set False), remove all the None abstract/title and the correspond title/abstract\n",
    "\n",
    "# --------- paths           --------- #\n",
    "# data folder path\n",
    "data_base_dir = '/home/vivoli/Thesis/data'\n",
    "# If you choose 'full' without setting N, the system will use the first chunk\n",
    "s2orc_type = 'full'\n",
    "N = None\n",
    "\n",
    "# --------- model/tokenizer --------- #\n",
    "# hugginface model/tokenizer name\n",
    "MODEL_PATH = 'allenai/scibert_scivocab_uncased'\n",
    "model_name_or_path = MODEL_PATH\n",
    "\n",
    "# --------- checkpoint model -------- #\n",
    "from transformers.trainer_utils import get_last_checkpoint\n",
    "\n",
    "# seed for reproducibility of experiments\n",
    "SEED = 1234"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58c51cbc",
   "metadata": {},
   "source": [
    "# 0.1 KeyPhrase Dataset\n",
    "---\n",
    "\n",
    "These are testing datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7cc6fa0",
   "metadata": {},
   "source": [
    "- Keyphrase paths\n",
    "\n",
    "```python\n",
    "dataset_names = ['inspec', 'krapivin', 'nus', 'semeval', 'kp20k', 'duc', 'stackexchange']\n",
    "json_base_dir = data_base_dir + '/keyphrase/json/'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "563babb8",
   "metadata": {},
   "source": [
    "- Keyphrase read json file\n",
    "\n",
    "```python\n",
    "def json_keyphrase_read( dataset_name, json_base_dir, file_name=None ):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        dataset_name (string): Directory name with the json file.\n",
    "        json_base_dir (string): Path to the Dataset directory.\n",
    "        file_name (string): (Optional) Json file name.\n",
    "        \n",
    "    Return:\n",
    "        json_list (list of dict): List of dictionaries, each one with the fields \n",
    "            - 'title' (string)\n",
    "            - 'abstract' (string)\n",
    "            - 'fulltext' (string | '')\n",
    "            - 'keywords' (list)\n",
    "    \"\"\"\n",
    "    if verbose: print(dataset_name)\n",
    "\n",
    "    input_json_path = os.path.join(json_base_dir, dataset_name, \n",
    "                                   '%s_test.json' % dataset_name if file_name is None else file_name)\n",
    "\n",
    "    json_list_of_dict = []\n",
    "    with open(input_json_path, 'r') as input_json:\n",
    "        for json_line in input_json:\n",
    "            json_dict = json.loads(json_line)\n",
    "\n",
    "            if dataset_name == 'stackexchange':\n",
    "                json_dict['abstract'] = json_dict['question']\n",
    "                json_dict['keywords'] = json_dict['tags']            \n",
    "                del json_dict['question']\n",
    "                del json_dict['tags']\n",
    "\n",
    "            keywords = json_dict['keywords']\n",
    "\n",
    "            if isinstance(keywords, str):\n",
    "                keywords = keywords.split(';')\n",
    "                json_dict['keywords'] = keywords\n",
    "\n",
    "            json_list_of_dict.append(json_dict)\n",
    "                \n",
    "    return json_list_of_dict\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b7c33a",
   "metadata": {},
   "source": [
    "This keyphrase dataset could be useful for testing some model on keyphrase task or abstract-title summarization/generation/embedding.\n",
    "\n",
    "For now, we can avoid implementing the Dataset's and DataLoader's classes for this objects.\n",
    "\n",
    "Although, the dataset and Dataloader would be simple as follow:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "515e440d",
   "metadata": {},
   "source": [
    "- Keyphrase process data -> tensor\n",
    "\n",
    "```python\n",
    "def data_keyphrase_process(json_list_of_dict, tokenizer, debug=False):\n",
    "    data = []\n",
    "    for json_dict in json_list_of_dict:\n",
    "        title_tensor_ = torch.tensor(tokenizer.encode(json_dict['title']),\n",
    "                                dtype=torch.long)\n",
    "        if debug: print(title_tensor_)\n",
    "        abstract_tensor_ = torch.tensor(tokenizer.encode(json_dict['abstract']),\n",
    "                                dtype=torch.long)\n",
    "        if debug: print(abstract_tensor_)\n",
    "        fulltext_tensor_ = torch.tensor(tokenizer.encode(json_dict['fulltext']),\n",
    "                                dtype=torch.long)\n",
    "        if debug: print(fulltext_tensor_)\n",
    "        keywords_tensor_ = torch.tensor(tokenizer(json_dict['keywords'], padding=True)['input_ids'], \n",
    "                                dtype=torch.long)\n",
    "        if debug: print(keywords_tensor_)\n",
    "\n",
    "        data.append((title_tensor_, abstract_tensor_, fulltext_tensor_, keywords_tensor_))\n",
    "    return data\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62bb227f",
   "metadata": {},
   "source": [
    "- Defining model and vocab\n",
    "\n",
    "```python\n",
    "# we need to get `vocab` and the `tokenizer`, all comes with *AutoTokenizer*\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "model = AutoModel.from_pretrained(MODEL_PATH)\n",
    "```\n",
    "\n",
    "- Extracting tokenized data\n",
    "```python\n",
    "# now we can use them\n",
    "json_list_of_dict = json_keyphrase_read( dataset_names[0], json_base_dir )\n",
    "data = data_keyphrase_process( json_list_of_dict, tokenizer )\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bbb2453",
   "metadata": {},
   "source": [
    "The `data` object is composed by `500 tuples`, each one composed by 4 objects:\n",
    "- `title_tensor_` is the title embedding (composed by integers values)\n",
    "- `abstract_tensor_` is the abstract embedding (composed by integers values)\n",
    "- `fulltext_tensor_` is the fulltext embedding (composed by integers values)\n",
    "- `keywords_tensor_` is the keywords embedding (composed by integers values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c83c7103",
   "metadata": {},
   "source": [
    "# 0.2 S2ORC Dataset\n",
    "---\n",
    "\n",
    "To build a generic loading function we take inspiration from [here](https://discuss.huggingface.co/t/pipeline-with-custom-dataset-tokenizer-when-to-save-load-manually/1084/11)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "354cb819",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARNING] You set 'full' but no bucket index was specified. \n",
      "         We'll use the index 0, so the first bucket will be used.\n"
     ]
    }
   ],
   "source": [
    "if s2orc_type == 'sample':\n",
    "    metadata_filename = 'sample'\n",
    "    pdf_parses_filename = 'sample'\n",
    "    \n",
    "    \n",
    "elif s2orc_type == 'full':\n",
    "    \n",
    "    if N is None:\n",
    "        print(f\"[WARNING] You set 'full' but no bucket index was specified. \\n \\\n",
    "        We'll use the index 0, so the first bucket will be used.\")\n",
    "        N = 0\n",
    "        \n",
    "    metadata_filename = f\"metadata_{N}\"\n",
    "    pdf_parses_filename = f\"pdf_parses_{N}\"\n",
    "    \n",
    "else:\n",
    "    raise NameError(f\"You must select an existed S2ORC dataset \\n \\\n",
    "                You selected {s2orc_type}, but options are ['sample' or 'full']\")\n",
    "\n",
    "meta_s2orc = data_base_dir +f'/s2orc-{s2orc_type}-20200705v1/{s2orc_type}/metadata/{metadata_filename}.jsonl'\n",
    "pdfs_s2orc = data_base_dir +f'/s2orc-{s2orc_type}-20200705v1/{s2orc_type}/pdf_/{pdf_parses_filename}.jsonl'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8a5ca2c",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "## âŒ PARTIAL PREPARE\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "cc4a5947",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint DOESN'T exist, model load from scratch at allenai/scibert_scivocab_uncased.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/allenai/scibert_scivocab_uncased/resolve/main/config.json from cache at /home/vivoli/.cache/huggingface/transformers/858852fd2471ce39075378592ddc87f5a6551e64c6825d1b92c8dab9318e0fc3.03ff9e9f998b9a9d40647a2148a202e3fb3d568dc0f170dda9dda194bab4d5dd\n",
      "Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.3.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 31090\n",
      "}\n",
      "\n",
      "Model name 'allenai/scibert_scivocab_uncased' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming 'allenai/scibert_scivocab_uncased' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "loading file https://huggingface.co/allenai/scibert_scivocab_uncased/resolve/main/vocab.txt from cache at /home/vivoli/.cache/huggingface/transformers/33593020f507d72099bd84ea6cd2296feb424fecd62d4a8edcc2a02899af6e29.38339d84e6e392addd730fd85fae32652c4cc7c5423633d6fa73e5f7937bbc38\n",
      "loading file https://huggingface.co/allenai/scibert_scivocab_uncased/resolve/main/tokenizer.json from cache at None\n",
      "loading file https://huggingface.co/allenai/scibert_scivocab_uncased/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/allenai/scibert_scivocab_uncased/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/allenai/scibert_scivocab_uncased/resolve/main/tokenizer_config.json from cache at None\n"
     ]
    }
   ],
   "source": [
    "# in case you need is necessary to set output_dir\n",
    "output_dir=f'./tmp_trainer/#{RUN_NUMBER}_{RUN_ITER}_{RUN_NAME}'\n",
    "\n",
    "# tokenizer from 'allenai/scibert_scivocab_uncased' or from checkpoint\n",
    "PRETRAINED = MODEL_PATH\n",
    "print(f\"Checkpoint DOESN'T exist, model load from scratch at {PRETRAINED}.\")\n",
    "    \n",
    "tokenizer = AutoTokenizer.from_pretrained(PRETRAINED) \n",
    "# model = BertForMaskedLM.from_pretrained(PRETRAINED) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "004f4adf",
   "metadata": {},
   "source": [
    "Model BertConfig object for \n",
    "\n",
    "```python\n",
    "Model config BertConfig {\n",
    "  \"attention_probs_dropout_prob\": 0.1,\n",
    "  \"gradient_checkpointing\": false,\n",
    "  \"hidden_act\": \"gelu\",\n",
    "  \"hidden_dropout_prob\": 0.1,\n",
    "  \"hidden_size\": 768,\n",
    "  \"initializer_range\": 0.02,\n",
    "  \"intermediate_size\": 3072,\n",
    "  \"layer_norm_eps\": 1e-12,\n",
    "  \"max_position_embeddings\": 512,\n",
    "  \"model_type\": \"bert\",\n",
    "  \"num_attention_heads\": 12,\n",
    "  \"num_hidden_layers\": 12,\n",
    "  \"pad_token_id\": 0,\n",
    "  \"position_embedding_type\": \"absolute\",\n",
    "  \"transformers_version\": \"4.3.2\",\n",
    "  \"type_vocab_size\": 2,\n",
    "  \"use_cache\": true,\n",
    "  \"vocab_size\": 31090\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db1a0411",
   "metadata": {},
   "source": [
    "Max sequence length from tokenizer, model and input might be differents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2596aa08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512\n"
     ]
    }
   ],
   "source": [
    "max_seq_length = model.config.max_position_embeddings\n",
    "print(max_seq_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7218b201",
   "metadata": {},
   "source": [
    "Here we use only the `meta_s2orc`, so we have the data that is structured such as:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac97bb4",
   "metadata": {},
   "source": [
    "## 2.1 Description (s2orc)\n",
    "---\n",
    "The `S2ORC` dataset is in the `data` path under the folder `s2orc-full-20200705v1` (where `s2orc` is the name of the dataset, `full` is the type, as there is also a sample fingerprint; and `20200705v1` is the version). \n",
    "We can reach the data folder exiting by the project and entering in the data folder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9edbe6ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keyphrase  s2orc-full-20200705v1  s2orc-sample-20200705v1  sentence-tranformers\r\n",
      "README.md  s2orc-mini\t\t  scibert\t\t   snli_1.0\r\n"
     ]
    }
   ],
   "source": [
    "DATA_PATH = '/home/vivoli/Thesis/data' \n",
    "!ls $DATA_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cd6dd862",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metadata  pdf_parses\r\n"
     ]
    }
   ],
   "source": [
    "custom_path = f\"{DATA_PATH}/s2orc-full-20200705v1/full\"\n",
    "!ls $custom_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80dce2bf",
   "metadata": {},
   "source": [
    "As you can see (going into `s2orc-full-20200705v1/full/`) there are the `metadata` folder and the `pdf_parses` folder. The main difference (as we can already get it from the names) is that in the `metadata` you only have some information about the dataset (retrieved from the published metadata), while in the `pdf_parses` you get all the extensive data conteined in the paper (if the paper was present, was correctly parsed and no restriction in the paper data were applied due to limited licence permition). For some reason, the `title` of the paper is contained only in the `metadata` file, but it can get from the `paper_id` field of the paper itself.\n",
    "\n",
    "More information about the `S2ORC` dataset can be read in the [README.md](https://github.com/allenai/s2orc/blob/master/README.md) of the project and in the [project repository](https://github.com/allenai/s2orc/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02aa52a7",
   "metadata": {},
   "source": [
    "### mag field\n",
    "- MAG fields of study:\n",
    "\n",
    "| class | Field of study | All papers | Full text |\n",
    "|-------|----------------|------------|-----------|\n",
    "|0      | Medicine       | 12.8M      | 1.8M      |\n",
    "|1      | Biology        | 9.6M       | 1.6M      |\n",
    "|2      | Chemistry      | 8.7M       | 484k      |\n",
    "|3      | n/a            | 7.7M       | 583k      |\n",
    "|4      | Engineering    | 6.3M       | 228k      |\n",
    "|5      | Comp Sci       | 6.0M       | 580k      |\n",
    "|6      | Physics        | 4.9M       | 838k      |\n",
    "|7      | Mat Sci        | 4.6M       | 213k      |\n",
    "|8      | Math           | 3.9M       | 669k      |\n",
    "|9      | Psychology     | 3.4M       | 316k      |\n",
    "|10     | Economics      | 2.3M       | 198k      |\n",
    "|11     | Poli Sci       | 1.8M       | 69k       |\n",
    "|12     | Business       | 1.8M       | 94k       |\n",
    "|13     | Geology        | 1.8M       | 115k      |\n",
    "|14     | Sociology      | 1.6M       | 93k       |\n",
    "|15     | Geography      | 1.4M       | 58k       |\n",
    "|16     | Env Sci        | 766k       | 52k       |\n",
    "|17     | Art            | 700k       | 16k       |\n",
    "|18     | History        | 690k       | 22k       |\n",
    "|19     | Philosophy     | 384k       | 15k       |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6021b325",
   "metadata": {},
   "source": [
    "## `metadata` schema\n",
    "\n",
    "We recommend everyone work with `metadata/` as the starting point.  This is a JSONlines file (one line per paper) with the following keys:\n",
    "\n",
    "#### Identifier fields\n",
    "\n",
    "* `paper_id`: a `str`-valued field that is a unique identifier for each S2ORC paper.\n",
    "\n",
    "* `arxiv_id`: a `str`-valued field for papers on [arXiv.org](https://arxiv.org).\n",
    "\n",
    "* `acl_id`: a `str`-valued field for papers on [the ACL Anthology](https://www.aclweb.org/anthology/).\n",
    "\n",
    "* `pmc_id`: a `str`-valued field for papers on [PubMed Central](https://www.ncbi.nlm.nih.gov/pmc/articles).\n",
    "\n",
    "* `pubmed_id`: a `str`-valued field for papers on [PubMed](https://pubmed.ncbi.nlm.nih.gov/), which includes MEDLINE.  Also known as `pmid` on PubMed.\n",
    "\n",
    "* `mag_id`: a `str`-valued field for papers on [Microsoft Academic](https://academic.microsoft.com).\n",
    "\n",
    "* `doi`: a `str`-valued field for the [DOI](http://doi.org/).  \n",
    "\n",
    "Notably:\n",
    "\n",
    "* Resolved citation links are represented by the cited paper's `paper_id`.\n",
    "\n",
    "* The `paper_id` resolves to a Semantic Scholar paper page, which can be verified using the `s2_url` field.\n",
    "\n",
    "* We don't always have a value for every identifier field.  When missing, they take `null` value.\n",
    "\n",
    "\n",
    "#### Metadata fields\n",
    "\n",
    "* `title`: a `str`-valued field for the paper title.  Every S2ORC paper *must* have one, though the source can be from publishers or parsed from PDFs.  We prioritize publisher-provided values over parsed values.\n",
    "\n",
    "* `authors`: a `List[Dict]`-valued field for the paper authors.  Authors are listed in order.  Each dictionary has the keys `first`, `middle`, `last`, and `suffix` for the author name, which are all `str`-valued with exception of `middle`, which is a `List[str]`-valued field.  Every S2ORC paper *must* have at least one author.\n",
    "\n",
    "* `venue` and `journal`: `str`-valued fields for the published venue/journal.  *Please note that there is not often agreement as to what constitutes a \"venue\" versus a \"journal\". Consolidating these fields is being considered for future releases.*   \n",
    "\n",
    "* `year`: an `int`-valued field for the published year.  If a paper is preprinted in 2019 but published in 2020, we try to ensure the `venue/journal` and `year` fields agree & prefer non-preprint published info. *We know this decision prohibits certain types of analysis like comparing preprint & published versions of a paper.  We're looking into it for future releases.*  \n",
    "\n",
    "* `abstract`: a `str`-valued field for the abstract.  These are provided directly from gold sources (not parsed from PDFs).  We preserve newline breaks in structured abstracts, which are common in medical papers, by denoting breaks with `':::'`.     \n",
    "\n",
    "* `inbound_citations`: a `List[str]`-valued field containing `paper_id` of other S2ORC papers that cite the current paper.  *Currently derived from PDF-parsed bibliographies, but may have gold sources in the future.*\n",
    "\n",
    "* `outbound_citations`: a `List[str]`-valued field containing `paper_id` of other S2ORC papers that the current paper cites.  Same note as above.   \n",
    "\n",
    "* `has_inbound_citations`: a `bool`-valued field that is `true` if `inbound_citations` has at least one entry, and `false` otherwise.\n",
    "\n",
    "* `has_outbound_citations` a `bool`-valued field that is `true` if `outbound_citations` has at least one entry, and `false` otherwise.\n",
    "\n",
    "We don't always have a value for every metadata field.  When missing, `str` fields take `null` value, while `List` fields are empty lists.\n",
    "\n",
    "#### PDF parse-related metadata fields\n",
    "\n",
    "* `has_pdf_parse`:  a `bool`-valued field that is `true` if this paper has a corresponding entry in `pdf_parses/`, which means we had processed that paper's PDF(s) at some point.  The field is `false` otherwise.\n",
    "\n",
    "* `has_pdf_parsed_abstract`: a `bool`-valued field that is `true` if the paper's PDF parse contains a parsed abstract, and `false` otherwise.   \n",
    "\n",
    "* `has_pdf_parsed_body_text`: a `bool`-valued field that is `true` if the paper's PDF parse contains parsed body text, and `false` otherwise.\n",
    "\n",
    "* `has_pdf_parsed_bib_entries`: a `bool`-valued field that is `true` if the paper's PDF parse contains parsed bibliography entries, and `false` otherwise.\n",
    "\n",
    "* `has_pdf_parsed_ref_entries`: a `bool`-valued field that is `true` if the paper's PDF parse contains parsed reference entries (e.g. tables, figures), and `false` otherwise.\n",
    "\n",
    "Please note:\n",
    "\n",
    "* If `has_pdf_parse = false`, the other four fields will not be present in the JSON (trivially `false`).\n",
    "\n",
    "* If `has_pdf_parse = true` but `has_pdf_parsed_abstract`, `has_pdf_parsed_body_text`, or `has_pdf_parsed_ref_entries` are `false`, this can be because:\n",
    "\n",
    "    * Our PDF parser failed to extract that element\n",
    "    * Our PDF parser succeeded but that paper simply did not have that element (e.g. papers without abstracts)\n",
    "    * Our PDF parser succeeded but that element was removed because the paper is not identified as open-access.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b755e27",
   "metadata": {},
   "source": [
    "##### metadata_CLASS\n",
    "```python\n",
    "{\n",
    " \"paper_id\": (string), \n",
    " \"title\": (string), \n",
    " \"authors\": [\n",
    "     {\n",
    "         \"first\": (string), \n",
    "         \"middle\": [], \n",
    "         \"last\": (string), \n",
    "         \"suffix\": (string)\n",
    "     },\n",
    "     ...\n",
    "   ]: **Author_Class**, \n",
    " \"abstract\": (string), \n",
    " \"year\": (int), \n",
    " \"arxiv_id\": null, \n",
    " \"acl_id\": null, \n",
    " \"pmc_id\": null, \n",
    " \"pubmed_id\": null, \n",
    " \"doi\": null, \n",
    " \"venue\": null, \n",
    " \"journal\": (string), \n",
    " \"mag_id\": (string-number), \n",
    " \"mag_field_of_study\": [\n",
    "     \"Medicine\",\n",
    "     \"Computer Science\"\n",
    "   ]: **FieldOfStudy_Enum**, \n",
    " \"outbound_citations\": [], \n",
    " \"inbound_citations\": [], \n",
    " \"has_outbound_citations\": false, \n",
    " \"has_inbound_citations\": false, \n",
    " \"has_pdf_parse\": false, \n",
    " \"s2_url\": (string)\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3768706",
   "metadata": {},
   "source": [
    "Here I represent Author_Class as an object of \n",
    "```python\n",
    "{\n",
    "    \"first\": (string), \n",
    "    \"middle\": [], \n",
    "    \"last\": (string), \n",
    "    \"suffix\": (string)\n",
    "}\n",
    "```\n",
    "and `FieldOfStudy_Enum` as an Enum of string such as `[ \"Medicine\", \"Computer Science\", \"Physics\", \"Mathematics\", ... ]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "98ef1785",
   "metadata": {},
   "outputs": [],
   "source": [
    "def partial_prepare_data(dataset_f: str,\n",
    "                tokenizer: PreTrainedTokenizer,\n",
    "                max_seq_length: int = None,\n",
    "                batch_size: int = 64,\n",
    "                num_workers: int = 4,\n",
    "                seed: int = SEED,\n",
    "                data_field: List[str] =  [\"title\", \"abstract\"]) -> Dict[str, DataLoader]:\n",
    "    \"\"\"Given an input file, prepare the train, test, validation dataloaders.\n",
    "    :param dataset_f: input file (format: .txt; line by line)\n",
    "    :param tokenizer: pretrained tokenizer that will prepare the data, i.e. convert tokens into IDs\n",
    "    :param max_seq_length: maximal sequence length. Longer sequences will be truncated\n",
    "    :param batch_size: batch size for the dataloaders\n",
    "    :param num_workers: number of CPU workers to use during dataloading. On Windows this must be zero\n",
    "    :return: a dictionary containing train, test, validation dataloaders\n",
    "    \"\"\"\n",
    "    print_all_debug = False\n",
    "    time_debug = True\n",
    "    print_some_debug = True\n",
    "\n",
    "    ## ------------------ ##\n",
    "    ## -- LOAD DATASET -- ##\n",
    "    ## ------------------ ##\n",
    "    if time_debug: start = time.time()\n",
    "    if time_debug: start_load = time.time()\n",
    "        \n",
    "    ## execution\n",
    "    max_seq_length = tokenizer.model_max_length if not max_seq_length else max_seq_length\n",
    "    if print_some_debug: print(max_seq_length)\n",
    "    dataset_dict = load_dataset(\"json\", data_files=dataset_f)\n",
    "\n",
    "    if time_debug: end_load = time.time()\n",
    "    if time_debug: print(f\"[TIME] load_dataset: {end_load - start_load}\")\n",
    "    \n",
    "    ## ------------------ ##\n",
    "    ## ---- MANAGING ---- ##\n",
    "    ## ------------------ ##\n",
    "    if time_debug: start_selection = time.time()\n",
    "    \n",
    "    ## execution\n",
    "    dataset = dataset_dict['train']\n",
    "    \n",
    "    if time_debug: end_selection = time.time()\n",
    "    if time_debug: print(f\"[TIME] dataset_train selection: {end_selection - start_selection}\")\n",
    "    if print_all_debug: print(dataset)\n",
    "   \n",
    "    ## ------------------ ##\n",
    "    ## --- REMOVE none -- ##\n",
    "    ## ------------------ ##\n",
    "    if time_debug: start_removing = time.time()\n",
    "    # clean input removing papers with **None** as abstract/title\n",
    "    if remove_None_papers:\n",
    "\n",
    "        ## --------------------- ##\n",
    "        ## --- REMOVE.indexes -- ##\n",
    "        ## --------------------- ##\n",
    "        if time_debug: start_removing_indexes = time.time()\n",
    "        if print_all_debug: print(data_field)\n",
    "        \n",
    "        ## execution\n",
    "        none_papers_indexes = {}\n",
    "        for field in data_field:\n",
    "            none_indexes = [ idx_s for idx_s, s in enumerate(dataset[f\"{field}\"]) if s is None]\n",
    "            none_papers_indexes = {**none_papers_indexes, **dict.fromkeys(none_indexes , False)}\n",
    "\n",
    "        if time_debug: end_removing_indexes = time.time()\n",
    "        if time_debug: print(f\"[TIME] remove.indexes: {end_removing_indexes - start_removing_indexes}\")\n",
    "        if print_all_debug: print(none_papers_indexes)\n",
    "        \n",
    "        ## --------------------- ##\n",
    "        ## --- REMOVE.concat --- ##\n",
    "        ## --------------------- ##\n",
    "        if time_debug: start_removing_concat = time.time()\n",
    "        \n",
    "        ## execution\n",
    "        to_remove_indexes = list(none_papers_indexes.keys())\n",
    "\n",
    "        if time_debug: end_removing_concat = time.time()\n",
    "        if time_debug: print(f\"[TIME] remove.concat: {end_removing_concat - start_removing_concat}\")\n",
    "        if print_all_debug: print(to_remove_indexes)\n",
    "        if print_all_debug: print([ dataset[\"abstract\"][i] for i in to_remove_indexes])\n",
    "\n",
    "        ## --------------------- ##\n",
    "        ## --- REMOVE.filter --- ##\n",
    "        ## --------------------- ##\n",
    "        if time_debug: start_removing_filter = time.time()\n",
    "        \n",
    "        ## execution\n",
    "        dataset = dataset.filter((lambda x, ids: none_papers_indexes.get(ids, True)), with_indices=True)\n",
    "        \n",
    "        if time_debug: end_removing_filter = time.time()\n",
    "        if time_debug: print(f\"[TIME] remove.filter: {end_removing_filter - start_removing_filter}\")\n",
    "        if print_all_debug: print(dataset)\n",
    "\n",
    "        \n",
    "    if time_debug: end_removing = time.time()\n",
    "    if time_debug: print(f\"[TIME] remove None fields: {end_removing - start_removing}\")\n",
    "\n",
    "    ## --------------------- ##\n",
    "    ## --- REMOVE.column --- ##\n",
    "    ## --------------------- ##\n",
    "    if time_debug: start_remove_unused_columns = time.time()\n",
    "    if remove_Unused_columns:\n",
    "        \n",
    "        for column in dataset.column_names:\n",
    "            if column not in data_field:\n",
    "                if debug: print(f\"{column}\")\n",
    "                dataset.remove_columns_(column)\n",
    "\n",
    "    if time_debug: end_remove_unused_columns = time.time()\n",
    "    if time_debug: print(f\"[TIME] remove.column: {end_remove_unused_columns - start_remove_unused_columns}\")\n",
    "        \n",
    "    ## ------------------ ##\n",
    "    ## --- SPLIT 1.    -- ##\n",
    "    ## ------------------ ##\n",
    "    if time_debug: start_first_split = time.time()\n",
    "    \n",
    "    # 80% (train), 20% (test + validation)\n",
    "    ## execution\n",
    "    train_testvalid = dataset.train_test_split(test_size=0.2, seed=SEED)\n",
    "    \n",
    "    if time_debug: end_first_split = time.time()\n",
    "    if time_debug: print(f\"[TIME] first [train-(test-val)] split: {end_first_split - start_first_split}\")\n",
    "\n",
    "    ## ------------------ ##\n",
    "    ## --- SPLIT 2.    -- ##\n",
    "    ## ------------------ ##\n",
    "    if time_debug: start_second_split = time.time()\n",
    "    \n",
    "    # 10% of total (test), 10% of total (validation)\n",
    "    ## execution\n",
    "    test_valid = train_testvalid[\"test\"].train_test_split(test_size=0.5, seed=SEED)\n",
    "\n",
    "    if time_debug: end_second_split = time.time()\n",
    "    if time_debug: print(f\"[TIME] second [test-val] split: {end_second_split - start_second_split}\")\n",
    "\n",
    "    ## execution\n",
    "    dataset = DatasetDict({\"train\": train_testvalid[\"train\"],\n",
    "                          \"test\": test_valid[\"test\"],\n",
    "                          \"valid\": test_valid[\"train\"]})\n",
    "    if time_debug: end = time.time()\n",
    "    if time_debug: print(f\"[TIME] TOTAL: {end - start}\") \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bbc18349",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2 Âµs, sys: 1 Âµs, total: 3 Âµs\n",
      "Wall time: 4.05 Âµs\n",
      "512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-d0f4b114df4979ef\n",
      "Reusing dataset json (/home/vivoli/.cache/huggingface/datasets/json/default-d0f4b114df4979ef/0.0.0/83d5b3a2f62630efc6b5315f00f20209b4ad91a00ac586597caee3a4da0bef02)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TIME] load_dataset: 1.4133775234222412\n",
      "[TIME] dataset_train selection: 7.152557373046875e-07\n",
      "[TIME] remove.indexes: 5.205094814300537\n",
      "[TIME] remove.concat: 0.0026721954345703125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/vivoli/.cache/huggingface/datasets/json/default-d0f4b114df4979ef/0.0.0/83d5b3a2f62630efc6b5315f00f20209b4ad91a00ac586597caee3a4da0bef02/cache-91c1b95631f59966.arrow\n",
      "remove_columns_ is deprecated and will be removed in the next major version of datasets. Use the dataset.remove_columns method instead.\n",
      "Loading cached split indices for dataset at /home/vivoli/.cache/huggingface/datasets/json/default-d0f4b114df4979ef/0.0.0/83d5b3a2f62630efc6b5315f00f20209b4ad91a00ac586597caee3a4da0bef02/cache-a5d01c93d0c6f581.arrow and /home/vivoli/.cache/huggingface/datasets/json/default-d0f4b114df4979ef/0.0.0/83d5b3a2f62630efc6b5315f00f20209b4ad91a00ac586597caee3a4da0bef02/cache-fb2c374f9e35d33e.arrow\n",
      "Loading cached split indices for dataset at /home/vivoli/.cache/huggingface/datasets/json/default-d0f4b114df4979ef/0.0.0/83d5b3a2f62630efc6b5315f00f20209b4ad91a00ac586597caee3a4da0bef02/cache-dd364e56fbe42c75.arrow and /home/vivoli/.cache/huggingface/datasets/json/default-d0f4b114df4979ef/0.0.0/83d5b3a2f62630efc6b5315f00f20209b4ad91a00ac586597caee3a4da0bef02/cache-cfcd188a54a5ffc4.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TIME] remove.filter: 1.443756341934204\n",
      "[TIME] remove None fields: 6.651668548583984\n",
      "[TIME] remove.column: 0.010538339614868164\n",
      "[TIME] first [train-(test-val)] split: 0.02309584617614746\n",
      "[TIME] second [test-val] split: 0.0066204071044921875\n",
      "[TIME] TOTAL: 8.105465173721313\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "\n",
    "dictionary_input = { \"data\": [\"abstract\"], \"target\": [\"title\"], \"classes\": [\"mag_field_of_study\"]}\n",
    "dictionary_columns = sum(dictionary_input.values(), [])\n",
    "\n",
    "# here we use meta_s2orc for speed, \n",
    "dataset = partial_prepare_data(meta_s2orc, tokenizer, data_field=dictionary_columns, max_seq_length=max_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5e26a139",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['title', 'abstract', 'mag_field_of_study'],\n",
      "        num_rows: 609048\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['title', 'abstract', 'mag_field_of_study'],\n",
      "        num_rows: 76132\n",
      "    })\n",
      "    valid: Dataset({\n",
      "        features: ['title', 'abstract', 'mag_field_of_study'],\n",
      "        num_rows: 76131\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "afb97c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(*sentences_by_column, data, target, classes):\n",
    "    \"\"\"Preprocess the raw input sentences from the text file.\n",
    "    :param sentences: a list of sentences (strings)\n",
    "    :return: a dictionary of \"input_ids\"\n",
    "    \"\"\"\n",
    "    print_all_debug = False\n",
    "    time_debug = False\n",
    "    print_some_debug = False\n",
    "\n",
    "    if debug: print(f\"[INFO-START] Preprocess on data: {data}, target: {target}\") \n",
    "    \n",
    "    assert data == ['abstract'], \"data should be ['abstract']\"\n",
    "    if debug: print(data)\n",
    "    assert target == ['title'], \"target should be ['title']\"\n",
    "    if debug: print(target)\n",
    "        \n",
    "    data_columns_len = len(data)\n",
    "    target_columns_len = len(target)\n",
    "    columns_len = data_columns_len + target_columns_len\n",
    "    \n",
    "    assert data_columns_len == 1, \"data length should be 1\"\n",
    "    if debug: print(data_columns_len)\n",
    "    assert target_columns_len == 1, \"target length should be 1\"\n",
    "    if debug: print(target_columns_len)\n",
    "        \n",
    "    sentences_by_column = np.asarray(sentences_by_column)\n",
    "    input_columns_len = len(sentences_by_column)\n",
    "    \n",
    "    if debug: print(f'all sentences (len {input_columns_len}): {sentences_by_column}')\n",
    "    \n",
    "    if target_columns_len == 0:\n",
    "        raise NameError(\"No target variable selected, \\\n",
    "                    are you sure you don't want any target?\")\n",
    "        \n",
    "    data_sentences = sentences_by_column[0]\n",
    "    target_sentences = sentences_by_column[1] # if columns_len == input_columns_len else sentences_by_column[data_columns_len:-1]\n",
    "    \n",
    "    if debug: print(data_sentences)\n",
    "    if debug: print(target_sentences)\n",
    "\n",
    "    \"\"\"\n",
    "    # clean input removing **None**, converting them to **''**\n",
    "    if clean_None_data:\n",
    "        data_sentences = np.asarray([ s if s is not None else '' for s in data_sentences])\n",
    "        target_sentences = np.asarray([ s if s is not None else '' for s in target_sentences])\n",
    "\n",
    "    # clean input removing papers with **None** as abstract/title\n",
    "    elif remove_None_data:\n",
    "        none_data_indexes = np.asarray([ idx_s for idx_s, s in enumerate(data_sentences) if s is None])\n",
    "        none_target_indexes = np.asarray([ idx_s for idx_s, s in enumerate(target_sentences) if s is None])\n",
    "\n",
    "        if debug: print(none_data_indexes)\n",
    "        if debug: print(none_target_indexes)\n",
    "\n",
    "        to_removed_indexes = np.unique(none_data_indexes, none_target_indexes)\n",
    "\n",
    "        if debug: print(to_removed_indexes)\n",
    "\n",
    "        data_sentences = np.delete(data_sentences, to_removed_indexes)\n",
    "        target_sentences = np.delete(target_sentences, to_removed_indexes)\n",
    "    \n",
    "    if debug: print(data_sentences)\n",
    "    if debug: print(target_sentences)\n",
    "    \"\"\"\n",
    "    \n",
    "    #Â sentences = [s for s in sentences if s is not None]\n",
    "    #Â tokens = [s.strip().split() for s in sentences]\n",
    "    #Â tokens = [t[:max_seq_length - 1] + [tokenizer.eos_token] for t in tokens]\n",
    "\n",
    "    # The sequences are not padded here. we leave that to the dataloader in a collate_fn\n",
    "    # ----------------------------------------------- #\n",
    "    # -------- TODO include the `collate_fn` -------- #\n",
    "    # ----------------------------------------------- #\n",
    "    # That means: a bit slower processing, but a smaller saved dataset size\n",
    "    if print_some_debug: print(max_seq_length)\n",
    "        \n",
    "    data_encoded_d = tokenizer(\n",
    "                        text=data_sentences.tolist(),\n",
    "                        # add_special_tokens=False,\n",
    "                        # is_pretokenized=True,\n",
    "                        padding=True, truncation=True, max_length=max_seq_length,\n",
    "                        return_token_type_ids=False,\n",
    "                        return_attention_mask=False,\n",
    "                        # We use this option because DataCollatorForLanguageModeling (see below) is more efficient when it\n",
    "                        # receives the `special_tokens_mask`.\n",
    "                        return_special_tokens_mask=True,\n",
    "                        return_tensors='np'\n",
    "    )\n",
    "    \n",
    "    target_encoded_d = tokenizer(\n",
    "                        text=target_sentences.tolist(),\n",
    "                        # add_special_tokens=False,\n",
    "                        #Â is_pretokenized=True,\n",
    "                        padding=True, truncation=True, max_length=max_seq_length,\n",
    "                        return_token_type_ids=False,\n",
    "                        return_attention_mask=False,\n",
    "                        # We use this option because DataCollatorForLanguageModeling (see below) is more efficient when it\n",
    "                        # receives the `special_tokens_mask`.\n",
    "                        return_special_tokens_mask=True,\n",
    "                        return_tensors='np'\n",
    "    )\n",
    "\n",
    "                            \n",
    "\n",
    "    if debug: print(data_encoded_d[\"input_ids\"].shape)\n",
    "    if debug: print(target_encoded_d[\"input_ids\"].shape)\n",
    "    # return encoded_d\n",
    "    \n",
    "    return {\"data_input_ids\": data_encoded_d[\"input_ids\"], \"target_input_ids\": target_encoded_d[\"input_ids\"]}\n",
    "    # return {\"input_ids\": sum(encoded_d['input_ids'], [])} "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89e1855c",
   "metadata": {},
   "source": [
    "print an example\n",
    "```python \n",
    "print(dataset['train'][:10]['title'], dataset['train'][:10]['abstract'])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cf6ffa21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PAD]: 0\n",
      "[UNK]: 101\n",
      "[SEP]: 103\n",
      "[CLS]: 102\n",
      "0: [PAD]\n",
      "1: [unused0]\n",
      "2: [unused1]\n",
      "99: [unused98]\n",
      "100: [unused99]\n",
      "101: [UNK]\n"
     ]
    }
   ],
   "source": [
    "vocab = tokenizer.get_vocab()\n",
    "print(f\"[PAD]: {vocab['[PAD]']}\")\n",
    "print(f\"[UNK]: {vocab['[UNK]']}\")\n",
    "print(f\"[SEP]: {vocab['[SEP]']}\")\n",
    "print(f\"[CLS]: {vocab['[CLS]']}\")\n",
    "print(f\"0: {tokenizer.convert_ids_to_tokens(0)}\")\n",
    "print(f\"1: {tokenizer.convert_ids_to_tokens(1)}\")\n",
    "print(f\"2: {tokenizer.convert_ids_to_tokens(2)}\")\n",
    "print(f\"99: {tokenizer.convert_ids_to_tokens(99)}\")\n",
    "print(f\"100: {tokenizer.convert_ids_to_tokens(100)}\")\n",
    "print(f\"101: {tokenizer.convert_ids_to_tokens(101)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9d9354db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PreTrainedTokenizerFast(name_or_path='allenai/scibert_scivocab_uncased', vocab_size=31090, model_max_len=1000000000000000019884624838656, is_fast=True, padding_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'})"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "750bd01f",
   "metadata": {},
   "source": [
    "Finally, I found [this](https://huggingface.co/docs/datasets/package_reference/main_classes.html?highlight=datasetdict#datasets.DatasetDict.map) documentation for the function `DatasetDict.map` from the `dataset` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "25fd4dd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vivoli/miniconda3/envs/arxiv-manipulation/lib/python3.7/site-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return array(a, dtype, copy=False, order=order)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3556ec4080694bfdbb8b975050f386f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=610.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73c19331f64843139a9f28010f4e3e44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=77.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef3294ae7e924fccb78e73434fed2598",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=77.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "dataset_map = dataset.map(preprocess, input_columns= dictionary_columns, fn_kwargs= dictionary_input, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93cefe6c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataset_map' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-3d3e29e0571b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdataset_map\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'dataset_map' is not defined"
     ]
    }
   ],
   "source": [
    "dataset_map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f9ef5f0",
   "metadata": {},
   "source": [
    "##### adding also the `mag_index`: the  `mag_field_of_study`  index present in the table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7804f13b",
   "metadata": {},
   "outputs": [],
   "source": [
    "mag_field_dict: Dict = {\n",
    "    \"Medicine\":    0,\n",
    "    \"Biology\":     1,\n",
    "    \"Chemistry\":   2,\n",
    "    \"Engineering\": 4,\n",
    "    \"Computer Science\":    5,\n",
    "    \"Physics\":     6,\n",
    "    \"Materials Science\":     7,\n",
    "    \"Mathematics\":        8,\n",
    "    \"Psychology\":  9,\n",
    "    \"Economics\":   10,\n",
    "    \"Political Science\":    11,\n",
    "    \"Business\":    12,\n",
    "    \"Geology\":     13,\n",
    "    \"Sociology\":   14,\n",
    "    \"Geography\":   15,\n",
    "    \"Environmental Science\":     16,\n",
    "    \"Art\":         17,\n",
    "    \"History\":     18,\n",
    "    \"Philosophy\":  19\n",
    "    # \"null\":         3, \n",
    "}\n",
    "\n",
    "# The key null is actually null, not \"null\":str\n",
    "#\n",
    "#      real_mag_field_value = paper_metadata['mag_field_of_study']\n",
    "#\n",
    "# so we could return the id 3 if it not contained as key of dictionary\n",
    "#\n",
    "#      mag_field_dict.get(real_mag_field_value, 3)\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "6bee7271",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mag_preprocessing(*mags):\n",
    "    \"\"\"Preprocess the raw input sentences from the text file.\n",
    "    :param sentences: a list of sentences (strings)\n",
    "    :return: a dictionary of \"input_ids\"\n",
    "    \"\"\"\n",
    "    debug = False\n",
    "    \n",
    "    if debug: print(f\"[INFO-START] Mag Preprocess\") \n",
    "        \n",
    "    mag_field = np.array(mags)\n",
    "    input_columns_len = mag_field.shape\n",
    "    if debug: print(f'pre flatten (len {input_columns_len}): {mag_field}')\n",
    "    if debug: print(f'pre types: {[type(ele) for ele in mag_field]}')\n",
    "    if debug: print(f'pre types: {type(mag_field)}')\n",
    "    \n",
    "    mag_field = mag_field.flatten()\n",
    "    input_columns_len = mag_field.shape\n",
    "    if debug: print(f'after flatten (len {input_columns_len}): {mag_field}')\n",
    "    if debug: print(f'after types: {[type(ele) for ele in mag_field]}')\n",
    "    if debug: print(f'after types: {type(mag_field)}')\n",
    "        \n",
    "    mag_field = np.array([ele if type(ele) == str else list(ele)[0] for ele in mag_field])\n",
    "        \n",
    "    if input_columns_len == 0:\n",
    "        raise NameError(\"No mag variable selected, \\\n",
    "                    are you sure you don't want any target?\")\n",
    "    \n",
    "    if debug: print(mag_field)\n",
    "    if debug: print(mag_field_dict)\n",
    "    if debug: print([mag_field_dict.get(real_mag_field_value, 3) for real_mag_field_value in mag_field])\n",
    "        \n",
    "    mag_index = np.asarray([mag_field_dict.get(real_mag_field_value, 3) for real_mag_field_value in mag_field])\n",
    "    \n",
    "    if debug: print(mag_index)\n",
    "    \n",
    "    return {\"mag_index\": mag_index}\n",
    "    # return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "6f1a4a97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "670670cca7db470ab0c4adcd24bec1a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=610.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vivoli/miniconda3/envs/arxiv-manipulation/lib/python3.7/site-packages/ipykernel_launcher.py:10: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e0eaf9ff9b943c297713f260c60814a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=77.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cb18b9b54b14f68814d6513520b007f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=77.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "dataset_mag_map = dataset_map.map(mag_preprocessing, input_columns= dictionary_input['classes'], batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "38c3a490",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['abstract', 'data_input_ids', 'mag_field_of_study', 'mag_index', 'target_input_ids', 'title'],\n",
       "        num_rows: 609048\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['abstract', 'data_input_ids', 'mag_field_of_study', 'mag_index', 'target_input_ids', 'title'],\n",
       "        num_rows: 76132\n",
       "    })\n",
       "    valid: Dataset({\n",
       "        features: ['abstract', 'data_input_ids', 'mag_field_of_study', 'mag_index', 'target_input_ids', 'title'],\n",
       "        num_rows: 76131\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_mag_map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f90a1374",
   "metadata": {},
   "source": [
    "# Rename it as you want\n",
    "---\n",
    "\n",
    "- `dataset_map.rename_column` ,method for renaming\n",
    "- `dataset_map.set_format`, method for define what columns need to be returned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "477fea92",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_map = dataset_map.rename_column(\"data_input_ids\", \"input_ids\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72488b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_map.set_format(\"torch\", columns=[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d0af8bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset_map['train'][1]['input_ids'].size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb00f389",
   "metadata": {},
   "source": [
    "Then, if you want to store it, it will be stored in the conda environment you are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c123f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%store dataset_map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e802b904",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "## âŒ FAKE PIPELINE for train BERT-based NETS\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb0e9945",
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r dataset_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f799db0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['abstract', 'input_ids', 'target_input_ids', 'title'],\n",
       "        num_rows: 612900\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['abstract', 'input_ids', 'target_input_ids', 'title'],\n",
       "        num_rows: 76613\n",
       "    })\n",
       "    valid: Dataset({\n",
       "        features: ['abstract', 'input_ids', 'target_input_ids', 'title'],\n",
       "        num_rows: 76613\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a6fcdae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer: we already have it\n",
    "# model: we already have it\n",
    "\n",
    "# If you print some element from `dataset_map['train'][element_index]['input_ids']` you'll see that lots of element\n",
    "vect = [ele[ele.nonzero()].size(0) for ele in dataset_map['train'][:]['input_ids']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "879c41f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " max: 512 \n",
      " min: 2 \n",
      " avg: 204.6087681514113\n"
     ]
    }
   ],
   "source": [
    "max_vect = max(vect)\n",
    "min_vect = min(vect)\n",
    "sum_vect = sum(vect)\n",
    "len_vect = len(vect)\n",
    "\n",
    "print(f\" max: {max_vect} \\n min: {min_vect} \\n avg: {sum_vect/len_vect}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a30e333",
   "metadata": {},
   "source": [
    "From [here](https://github.com/huggingface/transformers/blob/master/examples/language-modeling/run_mlm.py) you can get an idea from were the code has been borrowed."
   ]
  },
  {
   "cell_type": "raw",
   "id": "3894d6ed",
   "metadata": {},
   "source": [
    "# Data collator\n",
    "# This one will take care of randomly masking the tokens.\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=True, mlm_probability=0.15)\n",
    "\n",
    "train_dataset = dataset_map['train']\n",
    "eval_dataset = dataset_map['valid']\n",
    "\n",
    "# Inizialize TrainerArguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,           # [def.`tmp_trainer`] output directory\n",
    "    num_train_epochs=3,              # [def.   3 ] total # of training epochs\n",
    "    per_device_train_batch_size=64,  # [def.   8 ] batch size per device during training\n",
    "    per_device_eval_batch_size=64,   # [def.   8 ] batch size for evaluation\n",
    "    evaluation_strategy=\"steps\",     # [def. 'no'] evaluation is done (and logged) every eval_steps\n",
    "    warmup_steps=500,                # [def.   0 ] number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,               # [def.   0 ] strength of weight decay \n",
    "    learning_rate=5e-5,              # [def. 5e-5] \n",
    "    logging_dir='./logs',            # [def. runs/__id__] directory for storing logs. TensorBoard log directory.\n",
    ")\n",
    "\n",
    "# Initialize our Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "429a62e0",
   "metadata": {},
   "source": [
    "# Training\n",
    "train_result = trainer.train()\n",
    "trainer.save_model()  # Saves the tokenizer too for easy upload\n",
    "metrics = train_result.metrics\n",
    "\n",
    "max_train_samples = len(train_dataset)\n",
    "metrics[\"train_samples\"] = min(max_train_samples, len(train_dataset))\n",
    "\n",
    "trainer.log_metrics(\"train\", metrics)\n",
    "trainer.save_metrics(\"train\", metrics)\n",
    "trainer.save_state()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0a83fbfc",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "\n",
    "#logger.info\n",
    "print(\"*** Evaluate ***\")\n",
    "\n",
    "metrics = trainer.evaluate()\n",
    "\n",
    "max_val_samples = len(eval_dataset)\n",
    "metrics[\"eval_samples\"] = min(max_val_samples, len(eval_dataset))\n",
    "perplexity = math.exp(metrics[\"eval_loss\"])\n",
    "metrics[\"perplexity\"] = perplexity\n",
    "\n",
    "trainer.log_metrics(\"eval\", metrics)\n",
    "trainer.save_metrics(\"eval\", metrics)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
