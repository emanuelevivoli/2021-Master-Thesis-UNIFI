{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "smooth-studio",
   "metadata": {},
   "source": [
    "# Datasets\n",
    "---\n",
    "\n",
    "In this notebook we'll build/implement the Dataset classes we need to work with all the dataset we have.\n",
    "First we will introduce the datasets, then we will separate those based on the usage we are going to make of them, then we will use/build/implement our classes in order to manage those different datasets and tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rolled-latter",
   "metadata": {},
   "source": [
    "# 0.0 Utils\n",
    "---\n",
    "\n",
    "We will be using the 🤗*Datasets* library, so first we need to install it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "controversial-terrace",
   "metadata": {},
   "source": [
    "> !pip install datasets > datasets_installation.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "effective-atmosphere",
   "metadata": {},
   "source": [
    "We'll be using also the 🤗 *Tranformers* library, as we need a tokenizer and a vocab."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vocal-dublin",
   "metadata": {},
   "source": [
    "> !pip install transformers > transformers_installation.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collect-airfare",
   "metadata": {},
   "source": [
    "We'll be using also `pydash`, a python library inspired on lodash."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dependent-superintendent",
   "metadata": {},
   "source": [
    "> !pip install pydash > pydash_installation.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "employed-assets",
   "metadata": {},
   "source": [
    "We'll be using (for loggin) Weigths and Biases (`wandb`) so we are going to install it, independently from Hugging face, and use it within it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "overhead-accessory",
   "metadata": {},
   "source": [
    "> !pip install wandb -qq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "outdoor-organizer",
   "metadata": {},
   "source": [
    "Let's define all the `imports` and `hyperparameters` in one place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "viral-flesh",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import time\n",
    "import os\n",
    "import json\n",
    "from transformers import AutoTokenizer, AutoModel, PreTrainedTokenizer, DataCollatorForLanguageModeling, Trainer, BertForMaskedLM, TrainingArguments\n",
    "import torch\n",
    "import transformers\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from typing import Dict, List, Union\n",
    "from datasets import load_dataset, DatasetDict\n",
    "from enum import Enum\n",
    "# from pydash.arrays import pull_at\n",
    "import numpy as np\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import math\n",
    "\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "average-society",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33memanuelevivoli\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: WANDB_WATCH=all\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------- #\n",
    "#           Hyperparameters\n",
    "# ----------------------------------- #\n",
    "RUN_NAME = 'scibert-s2orc'\n",
    "RUN_NUMBER = 2\n",
    "RUN_ITER = 1\n",
    "\n",
    "\n",
    "# --------- logging         --------- #\n",
    "verbose = True # logging function description start and end\n",
    "debug = False # logging element values\n",
    "print_debug = False\n",
    "time_debug = True\n",
    "\n",
    "transformers.logging.set_verbosity_info()\n",
    "\n",
    "wandb.login()\n",
    "# Optional: log both gradients and parameters\n",
    "%env WANDB_WATCH=all\n",
    "\n",
    "# --------- preprocessing   --------- #\n",
    "# in **partial_prepare_data**\n",
    "remove_None_papers = True # if True, remove papers with None eather in abstract or title\n",
    "remove_Unused_columns = True\n",
    "# in **preprocess**\n",
    "clean_None_data = False # if True, changes all the None (abstract of title) to ''\n",
    "remove_None_data = False # if True (and clean_None_data set False), remove all the None abstract/title and the correspond title/abstract\n",
    "\n",
    "# --------- paths           --------- #\n",
    "# data folder path\n",
    "data_base_dir = '/home/vivoli/Thesis/data'\n",
    "s2orc_type = 'full'\n",
    "N = None\n",
    "\n",
    "# --------- model/tokenizer --------- #\n",
    "# hugginface model/tokenizer name\n",
    "MODEL_PATH = 'allenai/scibert_scivocab_uncased'\n",
    "model_name_or_path = MODEL_PATH\n",
    "\n",
    "# --------- checkpoint model -------- #\n",
    "from transformers.trainer_utils import get_last_checkpoint\n",
    "\n",
    "# seed for reproducibility of experiments\n",
    "SEED = 1234"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "timely-attitude",
   "metadata": {},
   "source": [
    "# 0.1 KeyPhrase Dataset\n",
    "---\n",
    "\n",
    "These are testing datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arabic-baptist",
   "metadata": {},
   "source": [
    "- Keyphrase paths\n",
    "\n",
    "```python\n",
    "dataset_names = ['inspec', 'krapivin', 'nus', 'semeval', 'kp20k', 'duc', 'stackexchange']\n",
    "json_base_dir = data_base_dir + '/keyphrase/json/'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "removed-seafood",
   "metadata": {},
   "source": [
    "- Keyphrase read json file\n",
    "\n",
    "```python\n",
    "def json_keyphrase_read( dataset_name, json_base_dir, file_name=None ):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        dataset_name (string): Directory name with the json file.\n",
    "        json_base_dir (string): Path to the Dataset directory.\n",
    "        file_name (string): (Optional) Json file name.\n",
    "        \n",
    "    Return:\n",
    "        json_list (list of dict): List of dictionaries, each one with the fields \n",
    "            - 'title' (string)\n",
    "            - 'abstract' (string)\n",
    "            - 'fulltext' (string | '')\n",
    "            - 'keywords' (list)\n",
    "    \"\"\"\n",
    "    if verbose: print(dataset_name)\n",
    "\n",
    "    input_json_path = os.path.join(json_base_dir, dataset_name, \n",
    "                                   '%s_test.json' % dataset_name if file_name is None else file_name)\n",
    "\n",
    "    json_list_of_dict = []\n",
    "    with open(input_json_path, 'r') as input_json:\n",
    "        for json_line in input_json:\n",
    "            json_dict = json.loads(json_line)\n",
    "\n",
    "            if dataset_name == 'stackexchange':\n",
    "                json_dict['abstract'] = json_dict['question']\n",
    "                json_dict['keywords'] = json_dict['tags']            \n",
    "                del json_dict['question']\n",
    "                del json_dict['tags']\n",
    "\n",
    "            keywords = json_dict['keywords']\n",
    "\n",
    "            if isinstance(keywords, str):\n",
    "                keywords = keywords.split(';')\n",
    "                json_dict['keywords'] = keywords\n",
    "\n",
    "            json_list_of_dict.append(json_dict)\n",
    "                \n",
    "    return json_list_of_dict\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "liberal-atlantic",
   "metadata": {},
   "source": [
    "This keyphrase dataset could be useful for testing some model on keyphrase task or abstract-title summarization/generation/embedding.\n",
    "\n",
    "For now, we can avoid implementing the Dataset's and DataLoader's classes for this objects.\n",
    "\n",
    "Although, the dataset and Dataloader would be simple as follow:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "capital-synthesis",
   "metadata": {},
   "source": [
    "- Keyphrase process data -> tensor\n",
    "\n",
    "```python\n",
    "def data_keyphrase_process(json_list_of_dict, tokenizer, debug=False):\n",
    "    data = []\n",
    "    for json_dict in json_list_of_dict:\n",
    "        title_tensor_ = torch.tensor(tokenizer.encode(json_dict['title']),\n",
    "                                dtype=torch.long)\n",
    "        if debug: print(title_tensor_)\n",
    "        abstract_tensor_ = torch.tensor(tokenizer.encode(json_dict['abstract']),\n",
    "                                dtype=torch.long)\n",
    "        if debug: print(abstract_tensor_)\n",
    "        fulltext_tensor_ = torch.tensor(tokenizer.encode(json_dict['fulltext']),\n",
    "                                dtype=torch.long)\n",
    "        if debug: print(fulltext_tensor_)\n",
    "        keywords_tensor_ = torch.tensor(tokenizer(json_dict['keywords'], padding=True)['input_ids'], \n",
    "                                dtype=torch.long)\n",
    "        if debug: print(keywords_tensor_)\n",
    "\n",
    "        data.append((title_tensor_, abstract_tensor_, fulltext_tensor_, keywords_tensor_))\n",
    "    return data\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adolescent-beads",
   "metadata": {},
   "source": [
    "- Defining model and vocab\n",
    "\n",
    "```python\n",
    "# we need to get `vocab` and the `tokenizer`, all comes with *AutoTokenizer*\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "model = AutoModel.from_pretrained(MODEL_PATH)\n",
    "```\n",
    "\n",
    "- Extracting tokenized data\n",
    "```python\n",
    "# now we can use them\n",
    "json_list_of_dict = json_keyphrase_read( dataset_names[0], json_base_dir )\n",
    "data = data_keyphrase_process( json_list_of_dict, tokenizer )\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "changing-clinic",
   "metadata": {},
   "source": [
    "The `data` object is composed by `500 tuples`, each one composed by 4 objects:\n",
    "- `title_tensor_` is the title embedding (composed by integers values)\n",
    "- `abstract_tensor_` is the abstract embedding (composed by integers values)\n",
    "- `fulltext_tensor_` is the fulltext embedding (composed by integers values)\n",
    "- `keywords_tensor_` is the keywords embedding (composed by integers values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "initial-innocent",
   "metadata": {},
   "source": [
    "# 0.2 S2ORC Dataset\n",
    "---\n",
    "\n",
    "To build a generic loading function we take inspiration from [here](https://discuss.huggingface.co/t/pipeline-with-custom-dataset-tokenizer-when-to-save-load-manually/1084/11)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "relevant-knitting",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARNING] You set 'full' but no bucket index was specified. \n",
      "         We'll use the index 0, so the first bucket will be used.\n"
     ]
    }
   ],
   "source": [
    "if s2orc_type == 'sample':\n",
    "    metadata_filename = 'sample'\n",
    "    pdf_parses_filename = 'sample'\n",
    "    \n",
    "    \n",
    "elif s2orc_type == 'full':\n",
    "    \n",
    "    if N is None:\n",
    "        print(f\"[WARNING] You set 'full' but no bucket index was specified. \\n \\\n",
    "        We'll use the index 0, so the first bucket will be used.\")\n",
    "        N = 0\n",
    "        \n",
    "    metadata_filename = f\"metadata_{N}\"\n",
    "    pdf_parses_filename = f\"pdf_parses_{N}\"\n",
    "    \n",
    "else:\n",
    "    raise NameError(f\"You must select an existed S2ORC dataset \\n \\\n",
    "                You selected {s2orc_type}, but options are ['sample' or 'full']\")\n",
    "\n",
    "meta_s2orc = data_base_dir +f'/s2orc-{s2orc_type}-20200705v1/{s2orc_type}/metadata/{metadata_filename}.jsonl'\n",
    "pdfs_s2orc = data_base_dir +f'/s2orc-{s2orc_type}-20200705v1/{s2orc_type}/pdf_/{pdf_parses_filename}.jsonl'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "progressive-capture",
   "metadata": {},
   "source": [
    "(TODO merge)\n",
    "```python\n",
    "class S2orcDataField(Enum):\n",
    "    TITLE: List[str] = [\"title\"]\n",
    "    ABSTRACT: List[str] = [\"abstract\"]\n",
    "    PAPER_ID: List[str] = [\"paper_id\"]\n",
    "    YEAH: List[str] = [\"year\"]\n",
    "    MAG_FIELD_OF_STUDY: List[str] = [\"mag_field_of_study\"]\n",
    "    S2_URL: List[str] = [\"s2_url\"]\n",
    "    TITLE_ABSTRACT: List[str] = [\"title\", \"abstract\"]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "composite-extreme",
   "metadata": {},
   "source": [
    "(TODO merge original function)\n",
    "```python\n",
    "def prepare_data(dataset_f: str,\n",
    "                tokenizer: PreTrainedTokenizer,\n",
    "                max_seq_length: int = None,\n",
    "                batch_size: int = 64,\n",
    "                num_workers: int = 0,\n",
    "                seed: int = SEED,\n",
    "                data_field: List[str] =  [\"title\", \"abstract\"]) -> Dict[str, DataLoader]:\n",
    "    \"\"\"Given an input file, prepare the train, test, validation dataloaders.\n",
    "    :param dataset_f: input file (format: .txt; line by line)\n",
    "    :param tokenizer: pretrained tokenizer that will prepare the data, i.e. convert tokens into IDs\n",
    "    :param max_seq_length: maximal sequence length. Longer sequences will be truncated\n",
    "    :param batch_size: batch size for the dataloaders\n",
    "    :param num_workers: number of CPU workers to use during dataloading. On Windows this must be zero\n",
    "    :return: a dictionary containing train, test, validation dataloaders\n",
    "    \"\"\"\n",
    "    max_seq_length = tokenizer.model_max_length if not max_seq_length else max_seq_length\n",
    "\n",
    "    def preprocess(sentences: List[str]): #-> Dict[str, Union[list, Tensor]]:\n",
    "        \"\"\"Preprocess the raw input sentences from the text file.\n",
    "        :param sentences: a list of sentences (strings)\n",
    "        :return: a dictionary of \"input_ids\"\n",
    "        \"\"\"\n",
    "        tokens = [s.strip().split() for s in sentences]\n",
    "        tokens = [t[:max_seq_length - 1] + [tokenizer.eos_token] for t in tokens]\n",
    "\n",
    "        # The sequences are not padded here. we leave that to the dataloader in a collate_fn\n",
    "        # ----------------------------------------------- #\n",
    "        # -------- TODO include the `collate_fn` -------- #\n",
    "        # ----------------------------------------------- #\n",
    "        # That means: a bit slower processing, but a smaller saved dataset size\n",
    "        encoded_d = tokenizer(tokens,\n",
    "                             add_special_tokens=False,\n",
    "                             is_pretokenized=True,\n",
    "                             return_token_type_ids=False,\n",
    "                             return_attention_mask=False)\n",
    "\n",
    "        return {\"input_ids\": encoded_d[\"input_ids\"]}\n",
    "\n",
    "    dataset_dict = load_dataset(\"json\", data_files=dataset_f)\n",
    "    # dataset = Dataset.from_dict({\"text\": Path(dataset_f).read_text(encoding=\"utf-8\").splitlines()})\n",
    "    dataset = dataset_dict['train']\n",
    "    # 90% (train), 20% (test + validation)\n",
    "    train_testvalid = dataset.train_test_split(test_size=0.2, seed=SEED)\n",
    "    # 10% of total (test), 10% of total (validation)\n",
    "    test_valid = train_testvalid[\"test\"].train_test_split(test_size=0.5, seed=SEED)\n",
    "\n",
    "    dataset = DatasetDict({\"train\": train_testvalid[\"train\"],\n",
    "                          \"test\": test_valid[\"test\"],\n",
    "                          \"valid\": test_valid[\"train\"]})\n",
    "    print(dataset)\n",
    "    \"\"\"\n",
    "    choose one of the dataset columns:\n",
    "    - IMPORTANT fields: \n",
    "        'title', 'authors', 'abstract', \n",
    "    - LESS important fields: \n",
    "        'paper_id', 'year', 'arxiv_id', 'acl_id', 'pmc_id', 'pubmed_id', 'doi', \n",
    "        'venue', 'journal', 'mag_id', 'mag_field_of_study', \n",
    "        'outbound_citations', 'inbound_citations', 'has_outbound_citations', 'has_inbound_citations', \n",
    "        'has_pdf_body_text', 'has_pdf_parse', 'has_pdf_parsed_abstract', 'has_pdf_parsed_body_text', 'has_pdf_parsed_bib_entries', 'has_pdf_parsed_ref_entries', \n",
    "        's2_url'\n",
    "    \"\"\"\n",
    "    dataset = dataset.map(preprocess, input_columns=data_field, batched=True)\n",
    "    dataset.set_format(\"torch\", columns=[\"input_ids\"])\n",
    "\n",
    "    return {partition: DataLoader(ds,\n",
    "                                 batch_size=batch_size,\n",
    "                                 shuffle=True,\n",
    "                                 num_workers=num_workers,\n",
    "                                 pin_memory=True) for partition, ds in dataset.items()}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pending-companion",
   "metadata": {},
   "source": [
    "(TODO merge function)\n",
    "```python\n",
    "# tokenizer from 'allenai/scibert_scivocab_uncased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "\n",
    "# tokenizer.add_special_tokens({\"eos_token\": \"[EOS]\"})\n",
    "DATA_FIELD =  [\"abstract\"]\n",
    "\n",
    "prepare_data(meta_s2orc, tokenizer, DATA_FIELD)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collectible-throw",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "## ❌ PARTIAL PREPARE\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "convertible-reducing",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'RUN_NUMBER' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-6e0555429cde>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mlast_checkpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0moutput_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mf'./tmp_trainer/#{RUN_NUMBER}_{RUN_ITER}_{RUN_NAME}'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mdo_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0moverwrite_output_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'RUN_NUMBER' is not defined"
     ]
    }
   ],
   "source": [
    "# Detecting last checkpoint.\n",
    "last_checkpoint = None\n",
    "\n",
    "output_dir=f'./tmp_trainer/#{RUN_NUMBER}_{RUN_ITER}_{RUN_NAME}'\n",
    "do_train = True\n",
    "overwrite_output_dir = False\n",
    "\n",
    "if os.path.isdir(output_dir) and do_train and not overwrite_output_dir:\n",
    "    last_checkpoint = get_last_checkpoint(output_dir)\n",
    "    if last_checkpoint is None and len(os.listdir(output_dir)) > 0:\n",
    "        raise ValueError(\n",
    "            f\"Output directory ({output_dir}) already exists and is not empty. \"\n",
    "            \"Use --overwrite_output_dir to overcome.\"\n",
    "        )\n",
    "    elif last_checkpoint is not None:\n",
    "        # logging.info\n",
    "        print(\n",
    "            f\"Checkpoint detected, resuming training at {last_checkpoint}. To avoid this behavior, change \"\n",
    "            \"the `--output_dir` or add `--overwrite_output_dir` to train from scratch.\"\n",
    "        )\n",
    "\n",
    "if last_checkpoint is not None:\n",
    "    checkpoint = last_checkpoint\n",
    "elif model_name_or_path is not None and os.path.isdir(model_name_or_path):\n",
    "    checkpoint = model_name_or_path\n",
    "else:\n",
    "    checkpoint = None\n",
    "\n",
    "print(checkpoint)\n",
    "print(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "banned-oriental",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'checkpoint' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-6679cea126d1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# tokenizer from 'allenai/scibert_scivocab_uncased' or from checkpoint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mif\u001b[0m \u001b[0mcheckpoint\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mPRETRAINED\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheckpoint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;31m# logging.info\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Checkpoint detected, model load from {PRETRAINED}.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'checkpoint' is not defined"
     ]
    }
   ],
   "source": [
    "# tokenizer from 'allenai/scibert_scivocab_uncased' or from checkpoint\n",
    "if checkpoint is not None:\n",
    "    PRETRAINED = checkpoint \n",
    "    # logging.info\n",
    "    print(f\"Checkpoint detected, model load from {PRETRAINED}.\")\n",
    "else:\n",
    "    PRETRAINED = MODEL_PATH\n",
    "    # logging.info\n",
    "    print(f\"Checkpoint DOESN'T exist, model load from scratch at {PRETRAINED}.\")\n",
    "    \n",
    "tokenizer = AutoTokenizer.from_pretrained(PRETRAINED) \n",
    "model = BertForMaskedLM.from_pretrained(PRETRAINED) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accredited-bachelor",
   "metadata": {},
   "source": [
    "Model BertConfig object for \n",
    "\n",
    "```python\n",
    "Model config BertConfig {\n",
    "  \"attention_probs_dropout_prob\": 0.1,\n",
    "  \"gradient_checkpointing\": false,\n",
    "  \"hidden_act\": \"gelu\",\n",
    "  \"hidden_dropout_prob\": 0.1,\n",
    "  \"hidden_size\": 768,\n",
    "  \"initializer_range\": 0.02,\n",
    "  \"intermediate_size\": 3072,\n",
    "  \"layer_norm_eps\": 1e-12,\n",
    "  \"max_position_embeddings\": 512,\n",
    "  \"model_type\": \"bert\",\n",
    "  \"num_attention_heads\": 12,\n",
    "  \"num_hidden_layers\": 12,\n",
    "  \"pad_token_id\": 0,\n",
    "  \"position_embedding_type\": \"absolute\",\n",
    "  \"transformers_version\": \"4.3.2\",\n",
    "  \"type_vocab_size\": 2,\n",
    "  \"use_cache\": true,\n",
    "  \"vocab_size\": 31090\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "behind-blond",
   "metadata": {},
   "source": [
    "Max sequence length from tokenizer, model and input might be differents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "indonesian-seeker",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512\n"
     ]
    }
   ],
   "source": [
    "max_seq_length = model.config.max_position_embeddings\n",
    "print(max_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vanilla-panel",
   "metadata": {},
   "outputs": [],
   "source": [
    "def partial_prepare_data(dataset_f: str,\n",
    "                tokenizer: PreTrainedTokenizer,\n",
    "                max_seq_length: int = None,\n",
    "                batch_size: int = 64,\n",
    "                num_workers: int = 4,\n",
    "                seed: int = SEED,\n",
    "                data_field: List[str] =  [\"title\", \"abstract\", \"\"]) -> Dict[str, DataLoader]:\n",
    "    \"\"\"Given an input file, prepare the train, test, validation dataloaders.\n",
    "    :param dataset_f: input file (format: .txt; line by line)\n",
    "    :param tokenizer: pretrained tokenizer that will prepare the data, i.e. convert tokens into IDs\n",
    "    :param max_seq_length: maximal sequence length. Longer sequences will be truncated\n",
    "    :param batch_size: batch size for the dataloaders\n",
    "    :param num_workers: number of CPU workers to use during dataloading. On Windows this must be zero\n",
    "    :return: a dictionary containing train, test, validation dataloaders\n",
    "    \"\"\"\n",
    "    print_all_debug = False\n",
    "    time_debug = True\n",
    "    print_some_debug = True\n",
    "\n",
    "    ## ------------------ ##\n",
    "    ## -- LOAD DATASET -- ##\n",
    "    ## ------------------ ##\n",
    "    if time_debug: start = time.time()\n",
    "    if time_debug: start_load = time.time()\n",
    "        \n",
    "    ## execution\n",
    "    max_seq_length = tokenizer.model_max_length if not max_seq_length else max_seq_length\n",
    "    if print_some_debug: print(max_seq_length)\n",
    "    dataset_dict = load_dataset(\"json\", data_files=dataset_f)\n",
    "\n",
    "    if time_debug: end_load = time.time()\n",
    "    if time_debug: print(f\"[TIME] load_dataset: {end_load - start_load}\")\n",
    "    \n",
    "    ## ------------------ ##\n",
    "    ## ---- MANAGING ---- ##\n",
    "    ## ------------------ ##\n",
    "    if time_debug: start_selection = time.time()\n",
    "    \n",
    "    ## execution\n",
    "    dataset = dataset_dict['train']\n",
    "    \n",
    "    if time_debug: end_selection = time.time()\n",
    "    if time_debug: print(f\"[TIME] dataset_train selection: {end_selection - start_selection}\")\n",
    "    if print_all_debug: print(dataset)\n",
    "   \n",
    "    ## ------------------ ##\n",
    "    ## --- REMOVE none -- ##\n",
    "    ## ------------------ ##\n",
    "    if time_debug: start_removing = time.time()\n",
    "    # clean input removing papers with **None** as abstract/title\n",
    "    if remove_None_papers:\n",
    "\n",
    "        ## --------------------- ##\n",
    "        ## --- REMOVE.indexes -- ##\n",
    "        ## --------------------- ##\n",
    "        if time_debug: start_removing_indexes = time.time()\n",
    "        if print_all_debug: print(data_field)\n",
    "        \n",
    "        ## execution\n",
    "        none_papers_indexes = {}\n",
    "        for field in data_field:\n",
    "            none_indexes = [ idx_s for idx_s, s in enumerate(dataset[f\"{field}\"]) if s is None]\n",
    "            none_papers_indexes = {**none_papers_indexes, **dict.fromkeys(none_indexes , False)}\n",
    "\n",
    "        if time_debug: end_removing_indexes = time.time()\n",
    "        if time_debug: print(f\"[TIME] remove.indexes: {end_removing_indexes - start_removing_indexes}\")\n",
    "        if print_all_debug: print(none_papers_indexes)\n",
    "        \n",
    "        ## --------------------- ##\n",
    "        ## --- REMOVE.concat --- ##\n",
    "        ## --------------------- ##\n",
    "        if time_debug: start_removing_concat = time.time()\n",
    "        \n",
    "        ## execution\n",
    "        to_remove_indexes = list(none_papers_indexes.keys())\n",
    "\n",
    "        if time_debug: end_removing_concat = time.time()\n",
    "        if time_debug: print(f\"[TIME] remove.concat: {end_removing_concat - start_removing_concat}\")\n",
    "        if print_all_debug: print(to_remove_indexes)\n",
    "        if print_all_debug: print([ dataset[\"abstract\"][i] for i in to_remove_indexes])\n",
    "\n",
    "        ## --------------------- ##\n",
    "        ## --- REMOVE.filter --- ##\n",
    "        ## --------------------- ##\n",
    "        if time_debug: start_removing_filter = time.time()\n",
    "        \n",
    "        ## execution\n",
    "        dataset = dataset.filter((lambda x, ids: none_papers_indexes.get(ids, True)), with_indices=True)\n",
    "        \n",
    "        if time_debug: end_removing_filter = time.time()\n",
    "        if time_debug: print(f\"[TIME] remove.filter: {end_removing_filter - start_removing_filter}\")\n",
    "        if print_all_debug: print(dataset)\n",
    "\n",
    "        \n",
    "    if time_debug: end_removing = time.time()\n",
    "    if time_debug: print(f\"[TIME] remove None fields: {end_removing - start_removing}\")\n",
    "\n",
    "    ## --------------------- ##\n",
    "    ## --- REMOVE.column --- ##\n",
    "    ## --------------------- ##\n",
    "    if time_debug: start_remove_unused_columns = time.time()\n",
    "    if remove_Unused_columns:\n",
    "        \n",
    "        for column in dataset.column_names:\n",
    "            if column not in data_field:\n",
    "                if debug: print(f\"{column}\")\n",
    "                dataset.remove_columns_(column)\n",
    "\n",
    "    if time_debug: end_remove_unused_columns = time.time()\n",
    "    if time_debug: print(f\"[TIME] remove.column: {end_remove_unused_columns - start_remove_unused_columns}\")\n",
    "        \n",
    "    ## ------------------ ##\n",
    "    ## --- SPLIT 1.    -- ##\n",
    "    ## ------------------ ##\n",
    "    if time_debug: start_first_split = time.time()\n",
    "    \n",
    "    # 80% (train), 20% (test + validation)\n",
    "    ## execution\n",
    "    train_testvalid = dataset.train_test_split(test_size=0.2, seed=SEED)\n",
    "    \n",
    "    if time_debug: end_first_split = time.time()\n",
    "    if time_debug: print(f\"[TIME] first [train-(test-val)] split: {end_first_split - start_first_split}\")\n",
    "\n",
    "    ## ------------------ ##\n",
    "    ## --- SPLIT 2.    -- ##\n",
    "    ## ------------------ ##\n",
    "    if time_debug: start_second_split = time.time()\n",
    "    \n",
    "    # 10% of total (test), 10% of total (validation)\n",
    "    ## execution\n",
    "    test_valid = train_testvalid[\"test\"].train_test_split(test_size=0.5, seed=SEED)\n",
    "\n",
    "    if time_debug: end_second_split = time.time()\n",
    "    if time_debug: print(f\"[TIME] second [test-val] split: {end_second_split - start_second_split}\")\n",
    "\n",
    "    ## execution\n",
    "    dataset = DatasetDict({\"train\": train_testvalid[\"train\"],\n",
    "                          \"test\": test_valid[\"test\"],\n",
    "                          \"valid\": test_valid[\"train\"]})\n",
    "    if time_debug: end = time.time()\n",
    "    if time_debug: print(f\"[TIME] TOTAL: {end - start}\") \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "certain-commercial",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time\n",
    "\n",
    "# tokenizer.add_special_tokens({\"eos_token\": \"[EOS]\"})\n",
    "DATA_FIELD =  [\"title\", \"abstract\"]\n",
    "\n",
    "# here we use meta_s2orc for speed, \n",
    "dataset = partial_prepare_data(meta_s2orc, tokenizer, data_field=DATA_FIELD, max_seq_length=max_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "western-shoulder",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "outside-dryer",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(*sentences_by_column, data, target): #-> Dict[str, Union[list, Tensor]]:\n",
    "    \"\"\"Preprocess the raw input sentences from the text file.\n",
    "    :param sentences: a list of sentences (strings)\n",
    "    :return: a dictionary of \"input_ids\"\n",
    "    \"\"\"\n",
    "    print_all_debug = False\n",
    "    time_debug = False\n",
    "    print_some_debug = False\n",
    "\n",
    "    if debug: print(f\"[INFO-START] Preprocess on data: {data}, target: {target}\") \n",
    "    \n",
    "    assert data == ['abstract'], \"data should be ['abstract']\"\n",
    "    if debug: print(data)\n",
    "    assert target == ['title'], \"target should be ['title']\"\n",
    "    if debug: print(target)\n",
    "        \n",
    "    data_columns_len = len(data)\n",
    "    target_columns_len = len(target)\n",
    "    columns_len = data_columns_len + target_columns_len\n",
    "    \n",
    "    assert data_columns_len == 1, \"data length should be 1\"\n",
    "    if debug: print(data_columns_len)\n",
    "    assert target_columns_len == 1, \"target length should be 1\"\n",
    "    if debug: print(target_columns_len)\n",
    "        \n",
    "    sentences_by_column = np.asarray(sentences_by_column)\n",
    "    input_columns_len = len(sentences_by_column)\n",
    "    \n",
    "    if debug: print(f'all sentences (len {input_columns_len}): {sentences_by_column}')\n",
    "    \n",
    "    if target_columns_len == 0:\n",
    "        raise NameError(\"No target variable selected, \\\n",
    "                    are you sure you don't want any target?\")\n",
    "        \n",
    "    data_sentences = sentences_by_column[0]\n",
    "    target_sentences = sentences_by_column[1] # if columns_len == input_columns_len else sentences_by_column[data_columns_len:-1]\n",
    "    \n",
    "    if debug: print(data_sentences)\n",
    "    if debug: print(target_sentences)\n",
    "\n",
    "    \"\"\"\n",
    "    # clean input removing **None**, converting them to **''**\n",
    "    if clean_None_data:\n",
    "        data_sentences = np.asarray([ s if s is not None else '' for s in data_sentences])\n",
    "        target_sentences = np.asarray([ s if s is not None else '' for s in target_sentences])\n",
    "\n",
    "    # clean input removing papers with **None** as abstract/title\n",
    "    elif remove_None_data:\n",
    "        none_data_indexes = np.asarray([ idx_s for idx_s, s in enumerate(data_sentences) if s is None])\n",
    "        none_target_indexes = np.asarray([ idx_s for idx_s, s in enumerate(target_sentences) if s is None])\n",
    "\n",
    "        if debug: print(none_data_indexes)\n",
    "        if debug: print(none_target_indexes)\n",
    "\n",
    "        to_removed_indexes = np.unique(none_data_indexes, none_target_indexes)\n",
    "\n",
    "        if debug: print(to_removed_indexes)\n",
    "\n",
    "        data_sentences = np.delete(data_sentences, to_removed_indexes)\n",
    "        target_sentences = np.delete(target_sentences, to_removed_indexes)\n",
    "    \n",
    "    if debug: print(data_sentences)\n",
    "    if debug: print(target_sentences)\n",
    "    \"\"\"\n",
    "    \n",
    "    # sentences = [s for s in sentences if s is not None]\n",
    "    # tokens = [s.strip().split() for s in sentences]\n",
    "    # tokens = [t[:max_seq_length - 1] + [tokenizer.eos_token] for t in tokens]\n",
    "\n",
    "    # The sequences are not padded here. we leave that to the dataloader in a collate_fn\n",
    "    # ----------------------------------------------- #\n",
    "    # -------- TODO include the `collate_fn` -------- #\n",
    "    # ----------------------------------------------- #\n",
    "    # That means: a bit slower processing, but a smaller saved dataset size\n",
    "    if print_some_debug: print(max_seq_length)\n",
    "        \n",
    "    data_encoded_d = tokenizer(\n",
    "                        text=data_sentences.tolist(),\n",
    "                        # add_special_tokens=False,\n",
    "                        # is_pretokenized=True,\n",
    "                        padding=True, truncation=True, max_length=max_seq_length,\n",
    "                        return_token_type_ids=False,\n",
    "                        return_attention_mask=False,\n",
    "                        # We use this option because DataCollatorForLanguageModeling (see below) is more efficient when it\n",
    "                        # receives the `special_tokens_mask`.\n",
    "                        return_special_tokens_mask=True,\n",
    "                        return_tensors='np'\n",
    "    )\n",
    "    \n",
    "    target_encoded_d = tokenizer(\n",
    "                        text=target_sentences.tolist(),\n",
    "                        # add_special_tokens=False,\n",
    "                        # is_pretokenized=True,\n",
    "                        padding=True, truncation=True, max_length=max_seq_length,\n",
    "                        return_token_type_ids=False,\n",
    "                        return_attention_mask=False,\n",
    "                        # We use this option because DataCollatorForLanguageModeling (see below) is more efficient when it\n",
    "                        # receives the `special_tokens_mask`.\n",
    "                        return_special_tokens_mask=True,\n",
    "                        return_tensors='np'\n",
    "    )\n",
    "\n",
    "                            \n",
    "\n",
    "    if debug: print(data_encoded_d[\"input_ids\"].shape)\n",
    "    if debug: print(target_encoded_d[\"input_ids\"].shape)\n",
    "    # return encoded_d\n",
    "    \n",
    "    return {\"data_input_ids\": data_encoded_d[\"input_ids\"], \"target_input_ids\": target_encoded_d[\"input_ids\"]}\n",
    "    # return {\"input_ids\": sum(encoded_d['input_ids'], [])} "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lucky-mumbai",
   "metadata": {},
   "source": [
    "print an example\n",
    "```python \n",
    "print(dataset['train'][:10]['title'], dataset['train'][:10]['abstract'])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coastal-quick",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = tokenizer.get_vocab()\n",
    "print(f\"[PAD]: {vocab['[PAD]']}\")\n",
    "print(f\"[UNK]: {vocab['[UNK]']}\")\n",
    "print(f\"[SEP]: {vocab['[SEP]']}\")\n",
    "print(f\"[CLS]: {vocab['[CLS]']}\")\n",
    "print(f\"0: {tokenizer.convert_ids_to_tokens(0)}\")\n",
    "print(f\"1: {tokenizer.convert_ids_to_tokens(1)}\")\n",
    "print(f\"2: {tokenizer.convert_ids_to_tokens(2)}\")\n",
    "print(f\"99: {tokenizer.convert_ids_to_tokens(99)}\")\n",
    "print(f\"100: {tokenizer.convert_ids_to_tokens(100)}\")\n",
    "print(f\"101: {tokenizer.convert_ids_to_tokens(101)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "described-adjustment",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "royal-ranking",
   "metadata": {},
   "source": [
    "Finally, I found [this](https://huggingface.co/docs/datasets/package_reference/main_classes.html?highlight=datasetdict#datasets.DatasetDict.map) documentation for the function `DatasetDict.map` from the `dataset` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "enormous-finger",
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary_input = { \"data\": [\"abstract\"], \"target\": [\"title\"], \"class\": [\"mag_field_of_study\"]}\n",
    "dictionary_columns = sum(dictionary_input.values(), [])\n",
    "dataset_map = dataset.map(preprocess, input_columns= dictionary_columns, fn_kwargs= dictionary_input, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bigger-corrections",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "progressive-entrance",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_map = dataset_map.rename_column(\"data_input_ids\", \"input_ids\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "respiratory-stock",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_map.set_format(\"torch\", columns=[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "elect-antarctica",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset_map['train'][1]['input_ids'].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "everyday-duration",
   "metadata": {},
   "outputs": [],
   "source": [
    "%store dataset_map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intermediate-dinner",
   "metadata": {},
   "source": [
    "```python\n",
    "def pad_collate(batch):\n",
    "    batch = np.asarray(batch)\n",
    "    \n",
    "    xx, yy = [], []\n",
    "    for elem in batch:\n",
    "        values = list(elem.values())\n",
    "        \n",
    "        x, y = values\n",
    "        xx.append(x)\n",
    "        yy.append(y)\n",
    "    \n",
    "    \n",
    "    # xx, yy = np.transpose([list(elem.values()) for elem in batch])\n",
    "    if debug: print(xx, yy)\n",
    "    \n",
    "    x_lens = [len(x) for x in xx]\n",
    "    y_lens = [len(y) for y in yy]\n",
    "    if debug: print(x_lens, y_lens)\n",
    "    \n",
    "    xx_pad = pad_sequence(xx, batch_first=True, padding_value=0)\n",
    "    yy_pad = pad_sequence(yy, batch_first=True, padding_value=0)\n",
    "\n",
    "    return xx_pad, yy_pad, x_lens, y_lens\n",
    "    # return 1,2,3,4\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coated-permit",
   "metadata": {},
   "source": [
    "```python\n",
    "dataset_map['test']['target_input_ids']\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hawaiian-clock",
   "metadata": {},
   "source": [
    "```python\n",
    "LIST_DICT = [\n",
    "    {'data_input_ids': [  \n",
    "          106,  3531,  3253,   165,  1151,   214,  9178,   140,   111,  8830, \n",
    "        16091, 30113,   190,   106,  3892, 13527, 1281,  1814,   256,   165,\n",
    "         3568,   191,   130,  3081,  2936,  5796,   190,   111,  2279,   131,\n",
    "          130,  4548,  5859,   205,   147,   535,  1313,   131,   111,  3551,\n",
    "         1171, 12062,   988,   131,   111, 23137,   137,  1346, 11250,   131,\n",
    "         8830, 16091, 30113,   205,   111,  8064,   137, 19876,  1382,   146,\n",
    "          145,   124,   422,  2194,   546,   165,  2030,   263,   111,  2027,\n",
    "          131,   111,  3081,  2936,  3551,   205,   185, 27618,   405,   111,\n",
    "          705,  2484,  1151,   263,  5197,  7423,   131,  5157,   205], \n",
    "     'target_input_ids': [  \n",
    "          130,  3081, 13204,   168,  5373,   655,  8064,   137,  1333,  4620,\n",
    "          131,  8830,  5035, 28067, 30118,  9365]\n",
    "    },\n",
    "    {\n",
    "        'data_input_ids': [], \n",
    "        'target_input_ids': [ \n",
    "         7831,   131, 11536,  1630,  4416,  2474,  4127,   235,  4353,  1352,\n",
    "         2329,   579,  3274,   579, 20356,   579,  3967,   579, 15969,  3396]\n",
    "    }\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "experimental-elder",
   "metadata": {},
   "source": [
    "```python\n",
    "pad_collate(np.asarray(LIST_DICT))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "threatened-saint",
   "metadata": {},
   "source": [
    "Create a DataLoader. (unuseful for the following procedures)\n",
    "```python\n",
    "dataset_result = {partition: DataLoader(ds,\n",
    "                                 batch_size=64,\n",
    "                                 shuffle=True,\n",
    "                                 num_workers=4,\n",
    "                                 collate_fn=pad_collate,\n",
    "                                 pin_memory=True) for partition, ds in dataset_map.items()}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "danish-processing",
   "metadata": {},
   "source": [
    "Testing the dataloader.\n",
    "```python\n",
    "for data_, target_, data_len, target_len in dataset_result['train']:  \n",
    "    print(f\"data_len      : {data_len}\\n\")\n",
    "    print(f\"data_         : {data_}\\n\")\n",
    "    print(f\"data_.shape   : {data_.shape}\\n\\n\")\n",
    "    \n",
    "    print(f\"target_len    : {target_len}\\n\")\n",
    "    print(f\"target_       : {target_}\\n\")\n",
    "    print(f\"target_.shape : {target_.shape}\\n\\n\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "educated-format",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "## ❌ FAKE PIPELINE for train BERT-based NETS\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "surprising-phenomenon",
   "metadata": {},
   "source": [
    "Testing the dataset_map elements.\n",
    "```python\n",
    "for data_ in dataset_map['train']:  \n",
    "    print(f\"data_        : {data_['input_ids'].size()}\\n\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "specified-chester",
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r dataset_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "comic-understanding",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['abstract', 'input_ids', 'target_input_ids', 'title'],\n",
       "        num_rows: 612900\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['abstract', 'input_ids', 'target_input_ids', 'title'],\n",
       "        num_rows: 76613\n",
       "    })\n",
       "    valid: Dataset({\n",
       "        features: ['abstract', 'input_ids', 'target_input_ids', 'title'],\n",
       "        num_rows: 76613\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "sixth-worcester",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_map.set_format(\"torch\", columns=[\"input_ids\", \"target_input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "unlike-return",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  102,  6935,   669,   131,  3101,  1262,   121,  3838,   131,  1505,\n",
       "          573, 11996, 30110,   579,  3641,  1146,  2385, 12006,  4566,   121,\n",
       "        14817,   103,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tokenizer = AutoTokenizer.from_pretrained() \n",
    "#model = BertForMaskedLM.from_pretrained() \n",
    "dataset_map['train'][0]['target_input_ids']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "continued-structure",
   "metadata": {},
   "source": [
    "From [here](https://github.com/huggingface/transformers/blob/master/examples/language-modeling/run_mlm.py) you can get an idea from were the code has been borrowed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ruled-average",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "No `TrainingArguments` passed, using `output_dir=tmp_trainer`.\n",
      "PyTorch: setting up devices\n",
      "The following columns in the training set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: abstract, target_input_ids, title.\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: abstract, target_input_ids, title.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<transformers.trainer.Trainer at 0x7f137d23e790>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data collator\n",
    "# This one will take care of randomly masking the tokens.\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=True, mlm_probability=0.15)\n",
    "\n",
    "train_dataset = dataset_map['train']\n",
    "eval_dataset = dataset_map['valid']\n",
    "\n",
    "# Inizialize TrainerArguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,           # [def.`tmp_trainer`] output directory\n",
    "    num_train_epochs=3,              # [def.   3 ] total # of training epochs\n",
    "    per_device_train_batch_size=64,  # [def.   8 ] batch size per device during training\n",
    "    per_device_eval_batch_size=64,   # [def.   8 ] batch size for evaluation\n",
    "    evaluation_strategy=\"steps\",     # [def. 'no'] evaluation is done (and logged) every eval_steps\n",
    "    warmup_steps=500,                # [def.   0 ] number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,               # [def.   0 ] strength of weight decay \n",
    "    learning_rate=5e-5,              # [def. 5e-5] \n",
    "    logging_dir='./logs',            # [def. runs/__id__] directory for storing logs. TensorBoard log directory.\n",
    ")\n",
    "\n",
    "# Initialize our Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "trainer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intimate-ranch",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "train_result = trainer.train(checkpoint)\n",
    "trainer.save_model()  # Saves the tokenizer too for easy upload\n",
    "metrics = train_result.metrics\n",
    "\n",
    "max_train_samples = len(train_dataset)\n",
    "metrics[\"train_samples\"] = min(max_train_samples, len(train_dataset))\n",
    "\n",
    "trainer.log_metrics(\"train\", metrics)\n",
    "trainer.save_metrics(\"train\", metrics)\n",
    "trainer.save_state()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decimal-special",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "\n",
    "#logger.info\n",
    "print(\"*** Evaluate ***\")\n",
    "\n",
    "metrics = trainer.evaluate()\n",
    "\n",
    "max_val_samples = len(eval_dataset)\n",
    "metrics[\"eval_samples\"] = min(max_val_samples, len(eval_dataset))\n",
    "perplexity = math.exp(metrics[\"eval_loss\"])\n",
    "metrics[\"perplexity\"] = perplexity\n",
    "\n",
    "trainer.log_metrics(\"eval\", metrics)\n",
    "trainer.save_metrics(\"eval\", metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sudden-illustration",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mv "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "supported-senior",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Introduction\n",
    "---\n",
    "\n",
    "The following datasets were downloaded from the internet (we try to provide links to those we have the right to do so). We divide the dataset based on the task they are mostly used for.\n",
    "\n",
    "### 1.1 Keyphrase task\n",
    "---\n",
    "\n",
    "SOTA: [keyphrase generation](https://arxiv.org/pdf/1704.06879.pdf).\n",
    "\n",
    "The Keyphrase datasets (***duc***, ***Inspect***, ***Krapivin***, ***NUS***, ***SemEval-2010***, ***KP20k dataset***, ***MagKP-CS***) are structured as follow:\n",
    "\n",
    "- title\n",
    "- abstract\n",
    "- fulltext\n",
    "- keywords\n",
    "\n",
    "The only dataset that variates is ***STACKEX*** that instead of having *abstract* and *keywords* has:\n",
    "\n",
    "- question (abstract)\n",
    "- tags (keywords)\n",
    "\n",
    "Here there is a list of the datasets previously cited, with some information:\n",
    "\n",
    "- **duc**, we haven't had much information on this dataset untill now.\n",
    "\n",
    "- **Inspec** [(Hulth, 2003)](https://www.aclweb.org/anthology/W03-1028.pdf), This dataset provides *2,000 paper abstracts*. We adopt the *500 testing* papers and their corresponding uncontrolled keyphrases for evaluation, and the remaining *1,500 papers* are used for *training* the supervised baseline models.\n",
    "\n",
    "- **Krapivin** [(Krapivin et al., 2008)](http://eprints.biblio.unitn.it/1671/1/disi09055-krapivin-autayeu-marchese.pdf): This dataset provides *2,304 papers with full-text* and *author-assigned keyphrases*. However, the author did not mention how to split testing data, so we selected the first *400 papers in alphabetical order as the testing data*, and the *remaining* papers are used to *train* the supervised baselines.\n",
    "\n",
    "- **NUS** [(Nguyen and Kan, 2007)](https://www.comp.nus.edu.sg/~kanmy/papers/icadl2007.pdf): We use both author-assigned and reader-assigned keyphrases and treat *all 211 papers as the testing data*. Since the NUS dataset did not specifically mention the ways of splitting training and testing data, the results of the supervised baseline models are obtained through a *five-fold cross-validation*.\n",
    "\n",
    "- **SemEval-2010** [(Kim et al., 2010)](https://www.aclweb.org/anthology/S10-1004.pdf): 288 articles were collected from the ACM Digital Library. 100 articles were used for testing and the rest were used for training supervised baselines.\n",
    "\n",
    "- **KP20k dataset** [(Meng et al., 2018)](https://arxiv.org/abs/1704.06879): They built a new testing dataset that contains the *titles, abstracts, and keyphrases* of *20,000 scientific articles* in computer science. They were *randomly selected from their obtained 567,830 articles*. Thus they took the 20,000 articles in the validation set to train the supervised baselines.\n",
    "\n",
    "- **MagKP-CS** (from OpenNMT-py and [OpenNMT-kpg-release](https://github.com/memray/OpenNMT-kpg-release)) that is available for download. \n",
    "\n",
    "- **STACKEX** (from [StackExchange](https://archive.org/details/stackexchange)) has been constructed from the computer science forums (CS/AI) at StackExchange using “title” + “body” as source text and “tags” as the target keyphrases. After removing questions without valid tags, they collected 330,965 questions. They randomly selected *16,000 for validation*, and another *16,000 as test set*. Note some questions in StackExchange forums contain large blocks of code, resulting in long texts (sometimes more than 10,000 tokens after tokenization), this is difficult for most neural models to handle. Consequently, the texts have been truncated to 300 tokens and 1,000 tokens for training and evaluation splits respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rural-enough",
   "metadata": {},
   "source": [
    "###### ⚠️ATTENTION\n",
    "> As we aren't going to use the Keyphrase dataset for now, we don't need any custom classes for managing this dataset. We will implement this functions and classes as we go, if there will be the needs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "warming-gates",
   "metadata": {},
   "source": [
    "### 1.2 Sentence embedding task\n",
    "---\n",
    "\n",
    "SOTA: [sBERT](https://arxiv.org/abs/1908.10084)\n",
    "\n",
    "- **SNLI** [(Bowman et al., 2015)](https://arxiv.org/abs/1508.05326) is a collection of *570,000 sentence pairs* annotated with the *labels contradiction, eintailment, and neutral*.\n",
    "\n",
    "- **MultiNLI** [(Williams et al., 2018)](https://arxiv.org/abs/1704.05426) contains *430,000 sentence pairs* and covers a *range of genres of spoken and written text*.\n",
    "\n",
    "- **SciTail** [(allenai)](http://ai2-website.s3.amazonaws.com/publications/scitail-aaai-2018_cameraready.pdf), the entailment dataset consists of 27k. In contrast to the SNLI and MultiNLI, it was not crowd-sourced but created from sentences that already exist “in the wild”. *Hypotheses* were created from *science questions* and the corresponding *answer candidates*, while relevant web sentences from a large corpus were used as premises. Models are evaluated based on accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incorrect-seeking",
   "metadata": {},
   "source": [
    "###### ❌ATTENTION\n",
    "> As we aren't going to use the NLI tasks dataset (for now), we don't need any custom classes for managing this dataset. We will implement this functions and classes as we go, if there will be the needs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "protected-constitutional",
   "metadata": {},
   "source": [
    "### 1.3 Generic NLP tasks\n",
    "---\n",
    "\n",
    "- **S2ORC** [(Lo et al., 2020)](https://github.com/allenai/s2orc) is a large corpus of *81.1M English-language academic papers* spanning many academic disciplines. The corpus consists of *rich metadata, paper abstracts, resolved bibliographic references*, as well as *structured full text for 8.1M open access papers*. Full text is annotated with automatically-detected inline mentions of citations, figures, and tables, each linked to their corresponding paper objects. In S2ORC, they aggregate papers from hundreds of academic publishers and digital archives into a unified source, and create the largest publicly-available collection of machine-readable academic text to date. Built for text mining over academic text.\n",
    "\n",
    "- **OAG** [(Tang et al., 2008)](http://keg.cs.tsinghua.edu.cn/jietang/publications/KDD08-Tang-et-al-ArnetMiner.pdf)  is a large knowledge graph unifying *two billion-scale academic graphs*: Microsoft Academic Graph (**MAG**) and **AMiner**. In mid 2017, they published OAG v1, which contains *166,192,182 papers from MAG and 154,771,162 papers from AMiner* and generated *64,639,608 linking (matching) relations between the two graphs*. This time, in OAG v2, author, venue and newer publication data and the corresponding matchings are available.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "impressive-scanner",
   "metadata": {},
   "source": [
    "###### ✅ATTENTION\n",
    "> We are going to use the S2ORC dataset as it contains full_text data as well as citation/reference informations. It contains also authorship - title - tables data that we will describe below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "activated-amendment",
   "metadata": {},
   "source": [
    "---\n",
    "# 2. S2ORC\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "color-indonesian",
   "metadata": {},
   "source": [
    "## 2.1 Description (s2orc)\n",
    "---\n",
    "The `S2ORC` dataset is in the `data` path under the folder `s2orc-full-20200705v1` (where `s2orc` is the name of the dataset, `full` is the type, as there is also a sample fingerprint; and `20200705v1` is the version). \n",
    "We can reach the data folder exiting by the project and entering in the data folder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "modular-provincial",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = '/home/vivoli/Thesis/data' \n",
    "!ls $DATA_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "revised-translation",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_path = f\"{DATA_PATH}/s2orc-full-20200705v1/full\"\n",
    "!ls $custom_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "activated-chemistry",
   "metadata": {},
   "source": [
    "As you can see (going into `s2orc-full-20200705v1/full/`) there are the `metadata` folder and the `pdf_parses` folder. The main difference (as we can already get it from the names) is that in the `metadata` you only have some information about the dataset (retrieved from the published metadata), while in the `pdf_parses` you get all the extensive data conteined in the paper (if the paper was present, was correctly parsed and no restriction in the paper data were applied due to limited licence permition). For some reason, the `title` of the paper is contained only in the `metadata` file, but it can get from the `paper_id` field of the paper itself.\n",
    "\n",
    "More information about the `S2ORC` dataset can be read in the [README.md](https://github.com/allenai/s2orc/blob/master/README.md) of the project and in the [project repository](https://github.com/allenai/s2orc/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dangerous-horror",
   "metadata": {},
   "source": [
    "### mag field\n",
    "- MAG fields of study:\n",
    "\n",
    "| class | Field of study | All papers | Full text |\n",
    "|-------|----------------|------------|-----------|\n",
    "|0      | Medicine       | 12.8M      | 1.8M      |\n",
    "|1      | Biology        | 9.6M       | 1.6M      |\n",
    "|2      | Chemistry      | 8.7M       | 484k      |\n",
    "|3      | n/a            | 7.7M       | 583k      |\n",
    "|4      | Engineering    | 6.3M       | 228k      |\n",
    "|5      | Comp Sci       | 6.0M       | 580k      |\n",
    "|6      | Physics        | 4.9M       | 838k      |\n",
    "|7      | Mat Sci        | 4.6M       | 213k      |\n",
    "|8      | Math           | 3.9M       | 669k      |\n",
    "|9      | Psychology     | 3.4M       | 316k      |\n",
    "|10     | Economics      | 2.3M       | 198k      |\n",
    "|11     | Poli Sci       | 1.8M       | 69k       |\n",
    "|12     | Business       | 1.8M       | 94k       |\n",
    "|13     | Geology        | 1.8M       | 115k      |\n",
    "|14     | Sociology      | 1.6M       | 93k       |\n",
    "|15     | Geography      | 1.4M       | 58k       |\n",
    "|16     | Env Sci        | 766k       | 52k       |\n",
    "|17     | Art            | 700k       | 16k       |\n",
    "|18     | History        | 690k       | 22k       |\n",
    "|19     | Philosophy     | 384k       | 15k       |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hungarian-recommendation",
   "metadata": {},
   "source": [
    "We need now a function that reads all the lines of the `jsonl` files inside both `metadata` and `pdf_parses` folders. Then we'll "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "commercial-continent",
   "metadata": {},
   "source": [
    "## `metadata` schema\n",
    "\n",
    "We recommend everyone work with `metadata/` as the starting point.  This is a JSONlines file (one line per paper) with the following keys:\n",
    "\n",
    "#### Identifier fields\n",
    "\n",
    "* `paper_id`: a `str`-valued field that is a unique identifier for each S2ORC paper.\n",
    "\n",
    "* `arxiv_id`: a `str`-valued field for papers on [arXiv.org](https://arxiv.org).\n",
    "\n",
    "* `acl_id`: a `str`-valued field for papers on [the ACL Anthology](https://www.aclweb.org/anthology/).\n",
    "\n",
    "* `pmc_id`: a `str`-valued field for papers on [PubMed Central](https://www.ncbi.nlm.nih.gov/pmc/articles).\n",
    "\n",
    "* `pubmed_id`: a `str`-valued field for papers on [PubMed](https://pubmed.ncbi.nlm.nih.gov/), which includes MEDLINE.  Also known as `pmid` on PubMed.\n",
    "\n",
    "* `mag_id`: a `str`-valued field for papers on [Microsoft Academic](https://academic.microsoft.com).\n",
    "\n",
    "* `doi`: a `str`-valued field for the [DOI](http://doi.org/).  \n",
    "\n",
    "Notably:\n",
    "\n",
    "* Resolved citation links are represented by the cited paper's `paper_id`.\n",
    "\n",
    "* The `paper_id` resolves to a Semantic Scholar paper page, which can be verified using the `s2_url` field.\n",
    "\n",
    "* We don't always have a value for every identifier field.  When missing, they take `null` value.\n",
    "\n",
    "\n",
    "#### Metadata fields\n",
    "\n",
    "* `title`: a `str`-valued field for the paper title.  Every S2ORC paper *must* have one, though the source can be from publishers or parsed from PDFs.  We prioritize publisher-provided values over parsed values.\n",
    "\n",
    "* `authors`: a `List[Dict]`-valued field for the paper authors.  Authors are listed in order.  Each dictionary has the keys `first`, `middle`, `last`, and `suffix` for the author name, which are all `str`-valued with exception of `middle`, which is a `List[str]`-valued field.  Every S2ORC paper *must* have at least one author.\n",
    "\n",
    "* `venue` and `journal`: `str`-valued fields for the published venue/journal.  *Please note that there is not often agreement as to what constitutes a \"venue\" versus a \"journal\". Consolidating these fields is being considered for future releases.*   \n",
    "\n",
    "* `year`: an `int`-valued field for the published year.  If a paper is preprinted in 2019 but published in 2020, we try to ensure the `venue/journal` and `year` fields agree & prefer non-preprint published info. *We know this decision prohibits certain types of analysis like comparing preprint & published versions of a paper.  We're looking into it for future releases.*  \n",
    "\n",
    "* `abstract`: a `str`-valued field for the abstract.  These are provided directly from gold sources (not parsed from PDFs).  We preserve newline breaks in structured abstracts, which are common in medical papers, by denoting breaks with `':::'`.     \n",
    "\n",
    "* `inbound_citations`: a `List[str]`-valued field containing `paper_id` of other S2ORC papers that cite the current paper.  *Currently derived from PDF-parsed bibliographies, but may have gold sources in the future.*\n",
    "\n",
    "* `outbound_citations`: a `List[str]`-valued field containing `paper_id` of other S2ORC papers that the current paper cites.  Same note as above.   \n",
    "\n",
    "* `has_inbound_citations`: a `bool`-valued field that is `true` if `inbound_citations` has at least one entry, and `false` otherwise.\n",
    "\n",
    "* `has_outbound_citations` a `bool`-valued field that is `true` if `outbound_citations` has at least one entry, and `false` otherwise.\n",
    "\n",
    "We don't always have a value for every metadata field.  When missing, `str` fields take `null` value, while `List` fields are empty lists.\n",
    "\n",
    "#### PDF parse-related metadata fields\n",
    "\n",
    "* `has_pdf_parse`:  a `bool`-valued field that is `true` if this paper has a corresponding entry in `pdf_parses/`, which means we had processed that paper's PDF(s) at some point.  The field is `false` otherwise.\n",
    "\n",
    "* `has_pdf_parsed_abstract`: a `bool`-valued field that is `true` if the paper's PDF parse contains a parsed abstract, and `false` otherwise.   \n",
    "\n",
    "* `has_pdf_parsed_body_text`: a `bool`-valued field that is `true` if the paper's PDF parse contains parsed body text, and `false` otherwise.\n",
    "\n",
    "* `has_pdf_parsed_bib_entries`: a `bool`-valued field that is `true` if the paper's PDF parse contains parsed bibliography entries, and `false` otherwise.\n",
    "\n",
    "* `has_pdf_parsed_ref_entries`: a `bool`-valued field that is `true` if the paper's PDF parse contains parsed reference entries (e.g. tables, figures), and `false` otherwise.\n",
    "\n",
    "Please note:\n",
    "\n",
    "* If `has_pdf_parse = false`, the other four fields will not be present in the JSON (trivially `false`).\n",
    "\n",
    "* If `has_pdf_parse = true` but `has_pdf_parsed_abstract`, `has_pdf_parsed_body_text`, or `has_pdf_parsed_ref_entries` are `false`, this can be because:\n",
    "\n",
    "    * Our PDF parser failed to extract that element\n",
    "    * Our PDF parser succeeded but that paper simply did not have that element (e.g. papers without abstracts)\n",
    "    * Our PDF parser succeeded but that element was removed because the paper is not identified as open-access.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "diverse-humidity",
   "metadata": {},
   "source": [
    "##### metadata_CLASS\n",
    "```python\n",
    "{\n",
    " \"paper_id\": (string), \n",
    " \"title\": (string), \n",
    " \"authors\": [\n",
    "     {\n",
    "         \"first\": (string), \n",
    "         \"middle\": [], \n",
    "         \"last\": (string), \n",
    "         \"suffix\": (string)\n",
    "     },\n",
    "     ...\n",
    "   ]: **Author_Class**, \n",
    " \"abstract\": (string), \n",
    " \"year\": (int), \n",
    " \"arxiv_id\": null, \n",
    " \"acl_id\": null, \n",
    " \"pmc_id\": null, \n",
    " \"pubmed_id\": null, \n",
    " \"doi\": null, \n",
    " \"venue\": null, \n",
    " \"journal\": (string), \n",
    " \"mag_id\": (string-number), \n",
    " \"mag_field_of_study\": [\n",
    "     \"Medicine\",\n",
    "     \"Computer Science\"\n",
    "   ]: **FieldOfStudy_Enum**, \n",
    " \"outbound_citations\": [], \n",
    " \"inbound_citations\": [], \n",
    " \"has_outbound_citations\": false, \n",
    " \"has_inbound_citations\": false, \n",
    " \"has_pdf_parse\": false, \n",
    " \"s2_url\": (string)\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "neural-centre",
   "metadata": {},
   "source": [
    "Here I represent Author_Class as an object of \n",
    "```python\n",
    "{\n",
    "    \"first\": (string), \n",
    "    \"middle\": [], \n",
    "    \"last\": (string), \n",
    "    \"suffix\": (string)\n",
    "}\n",
    "```\n",
    "and `FieldOfStudy_Enum` as an Enum of string such as `[ \"Medicine\", \"Computer Science\", \"Physics\", \"Mathematics\", ... ]`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aggregate-tsunami",
   "metadata": {},
   "source": [
    "\n",
    "## `pdf_parses` schema\n",
    "\n",
    "We view `pdf_parses/` as supplementary to the `metadata/` entries.  PDF parses are also represented as JSONlines file (one line per paper) with the following keys:\n",
    "\n",
    "* `paper_id`: a `str`-valued field which is the same S2ORC paper ID in `metadata/`\n",
    "\n",
    "* `_pdf_hash`: a `str`-valued field.  Internal usage only.  We use this for debugging.\n",
    "\n",
    "* `abstract` and `body_text` are `List[Dict]`-valued fields representing parsed text from the PDF.  Each `Dict` corresponds to a paragraph.  `List` preserves their original ordering.\n",
    "\n",
    "* `bib_entries` and `ref_entries` are `Dict`-valued fields representing extracted entities that can be referenced (inline) within the text.\n",
    "\n",
    "#### example 1\n",
    "\n",
    "One example paragraph in `abstract` or `body_text` might look like:\n",
    "\n",
    "```python\n",
    "{\n",
    "    \"section\": \"Introduction\",\n",
    "    \"text\": \"Dogs are happier cats [13, 15]. See Figure 3 for a diagram.\",\n",
    "    \"cite_spans\": [\n",
    "        {\"start\": 22, \"end\": 25, \"text\": \"[13\", \"ref_id\": \"BIBREF11\"},\n",
    "        {\"start\": 27, \"end\": 30, \"text\": \"15]\", \"ref_id\": \"BIBREF30\"},\n",
    "        ...\n",
    "    ],\n",
    "    \"ref_spans\": [\n",
    "        {\"start\": 36, \"end\": 44, \"text\": \"Figure 3\", \"ref_id\": \"FIGREF2\"},\n",
    "    ]\n",
    "}\n",
    "```\n",
    "\n",
    "and example `bib_entries` and `ref_entries` might look like:\n",
    "\n",
    "```python\n",
    "{\n",
    "    ...,\n",
    "    \"BIBREF11\": {\n",
    "        \"title\": \"Do dogs dream of electric humans?\",\n",
    "        \"authors\": [\n",
    "            {\"first\": \"Lucy\", \"middle\": [\"Lu\"], \"last\": \"Wang\", \"suffix\": \"\"}, \n",
    "            {\"first\": \"Mark\", \"middle\": [], \"last\": \"Neumann\", \"suffix\": \"V\"}\n",
    "        ],\n",
    "        \"year\": \"\", \n",
    "        \"venue\": \"barXiv\",\n",
    "        \"link\": null\n",
    "    },\n",
    "    ...\n",
    "}\n",
    "```\n",
    "\n",
    "```python\n",
    "{\n",
    "    \"TABREF4\": {\n",
    "        \"text\": \"Table 5. Clearly, we achieve SOTA here or something.\",\n",
    "        \"type\": \"table\"\n",
    "    }\n",
    "    ...,\n",
    "    \"FIGREF2\": {\n",
    "        \"text\": \"Figure 3. This is the caption of a pretty figure.\",\n",
    "        \"type\": \"figure\"\n",
    "    },\n",
    "    ...\n",
    "}\n",
    "```\n",
    "\n",
    "Notice: \n",
    "\n",
    "* Inline `spans` are represented by character start and end indices into the paragraph `text`\n",
    "* `spans` resolve to `BIBREF`, `TABREF` or `FIGREF` entries.\n",
    "* `BIBREF` are IDs of bibliographic elements of `bib_entries`.  Bib entries may be missing fields (e.g. `year`).  They can be linked to S2ORC papers, as specified by `link`, but we also preserve any unlinked entries by setting `link` to `null`.\n",
    "* `FIGREF` and `TABREF` are IDs of figure and table elements of `ref_entries`.  Ref entries contain the caption text of the corresponding object, and also indicate the type of object.\n",
    "\n",
    "\n",
    "#### example 2\n",
    "\n",
    "You may see empty `pdf_parses/` JSONs that look like: \n",
    "\n",
    "```python\n",
    "{\n",
    "    \"paper_id\": \"...\", \n",
    "    \"_pdf_hash\": \"...\", \n",
    "    \"abstract\": [], \n",
    "    \"body_text\": [], \n",
    "    \"bib_entries\": {}, \n",
    "    \"ref_entries\": {}\n",
    "}\n",
    "```\n",
    "\n",
    "We keep these around for our internal usage, but the way to interpret these is that there is no usable PDF parse here, despite the corresponding `metadata/` entry still displaying `has_pdf_parse = true`.\n",
    "\n",
    "These exist when (i) `bib_entries` does not successfully parse *and* (ii) the paper is not open-access, so we had to remove `abstract`, `body_text`, and `ref_entries`.   \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "golden-packet",
   "metadata": {},
   "source": [
    "##### pdf_parses_CLASS\n",
    "```python\n",
    "{\n",
    " \"paper_id\": (string), \n",
    " \"_pdf_hash\": (string-number), \n",
    " \"abstract\": [\n",
    "     {\n",
    "         \"section\": (string) \"Abstract\", \n",
    "         \"text\": (string), \n",
    "         \"cite_spans\": [\n",
    "             {\n",
    "                 \"start\": (int), \n",
    "                 \"end\": (int), \n",
    "                 \"text\": (string-number) \"[4, \n",
    "                 \"ref_id\": (string)\n",
    "             }\n",
    "           ]: **CiteSpan_Class**, \n",
    "         \"ref_spans\": []\n",
    "     },\n",
    "     ...\n",
    " ]: **TextSection_Class**, \n",
    " \"body_text\": [], \n",
    " \"bib_entries\": \n",
    "     {\n",
    "         \"BIBREF0\": \n",
    "             {\n",
    "              \"title\": (string), \n",
    "              \"authors\": [\n",
    "                  {\n",
    "                      \"first\": (string), \n",
    "                      \"middle\": [], \n",
    "                      \"last\": (string), \n",
    "                      \"suffix\": (string)\n",
    "                   }\n",
    "                 ], \n",
    "               \"year\": (int), \n",
    "               \"venue\": (string), \n",
    "               \"link\": (string-number)\n",
    "              }, \n",
    "          \"BIBREF1\": \n",
    "              {\n",
    "                  ...\n",
    "              }\n",
    "       }: **BIBREF_Class**, \n",
    " \"ref_entries\": {}\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "broke-complement",
   "metadata": {},
   "source": [
    "Here I represent `TextSection_Class` as an object of \n",
    "```python\n",
    "{\n",
    " \"section\": (string), \n",
    " \"text\": (string), \n",
    " \"cite_spans\": [\n",
    "     {\n",
    "         \"start\": (int), \n",
    "         \"end\": (int), \n",
    "         \"text\": (string-number) \"[4, \n",
    "         \"ref_id\": (string)\n",
    "     }\n",
    "   ], \n",
    " \"ref_spans\": []\n",
    "}\n",
    "```\n",
    "where `CiteSpan_Class` itself is another structured object:\n",
    "```python\n",
    "{\n",
    " \"start\": (int), \n",
    " \"end\": (int), \n",
    " \"text\": (string-number), \n",
    " \"ref_id\": (string)\n",
    "}\n",
    "```\n",
    "and `BIBREF_Class` as dictionary field with `BIBREF_#` as key and related to it an object as follow:\n",
    "```python\n",
    "\"BIBREF_#\": \n",
    " {\n",
    "  \"title\": (string), \n",
    "  \"authors\": [\n",
    "      {\n",
    "          \"first\": (string), \n",
    "          \"middle\": [] (list of string),\n",
    "          \"last\": (string), \n",
    "          \"suffix\": (string)\n",
    "       }\n",
    "     ], \n",
    "   \"year\": (int), \n",
    "   \"venue\": (string), \n",
    "   \"link\": null\n",
    "  }\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "general-webster",
   "metadata": {},
   "source": [
    "## 2.2 Creation (s2orc)\n",
    "---\n",
    "Now we have explored the `S2ORC` structure, we are ready to load the data (starting from the `sample` and following on the `full` folder). The first thing to do is create (as we did before) a method for read the json: `json_s2orc_read`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "running-albany",
   "metadata": {},
   "outputs": [],
   "source": [
    "TYPE = \"sample\" # \"full\"\n",
    "SAMPLE_FOLDER = f\"s2orc-{TYPE}-20200705v1/{TYPE}\"\n",
    "\n",
    "# sample data\n",
    "sample_file_names = [\n",
    "    \"sample.jsonl\"\n",
    "]\n",
    "\n",
    "# full data\n",
    "N = 1\n",
    "full_file_names = [ f\"metadata_{index}.jsonl\" for index in range(0,N) ]\n",
    "\n",
    "print('sample', sample_file_names)\n",
    "print('full', full_file_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "living-plenty",
   "metadata": {},
   "outputs": [],
   "source": [
    "# choosing the file_names to work with\n",
    "if TYPE == \"sample\": \n",
    "    file_names = sample_file_names\n",
    "else: \n",
    "    file_names = full_file_names\n",
    "    \n",
    "print(file_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "practical-scanning",
   "metadata": {},
   "source": [
    "Lets's see what's inside the folder (in this case `metadata` but should be the same for `pdf_parses`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "announced-impression",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PATH = f\"{DATA_PATH}/{SAMPLE_FOLDER}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brave-latter",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_metadata = f\"{DATA_PATH}/{SAMPLE_FOLDER}/metadata/\"\n",
    "path_pdf_parses = f\"{DATA_PATH}/{SAMPLE_FOLDER}/pdf_parses/\"\n",
    "\n",
    "metadata_output = !ls $path_metadata | grep \".*\\.jsonl$\"\n",
    "print('metadata:', metadata_output)\n",
    "\n",
    "pdf_parses_output = !ls $path_pdf_parses | grep \".*\\.jsonl$\"\n",
    "print('pdf_parses', pdf_parses_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "under-lawrence",
   "metadata": {},
   "source": [
    "So we can describe the function in charge to load the `jsonl` files. The function must have in input the `generic_path` (f\"{DATA_PATH}/{SAMPLE_FOLDER}\") and then searching in `metadata` and `pdf_parses` for the files present in `file_names`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accepting-niagara",
   "metadata": {},
   "source": [
    "(Unused)\n",
    "```python\n",
    "def read_json_list(jsonl_path):\n",
    "    json_list_of_dict = []\n",
    "    with open(jsonl_path, 'r') as input_json:\n",
    "        for json_line in input_json:\n",
    "            json_dict = json.loads(json_line)\n",
    "            json_list_of_dict.append(json_dict)\n",
    "    return json_list_of_dict\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "serious-release",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_json_list_dict(jsonl_path):\n",
    "    # list of dictionaries, one for each row in pdf_parses\n",
    "    json_list_of_dict = []\n",
    "    # dictionary of indexes, to obtain the object from list, starting from the `paper_id`\n",
    "    json_dict_of_index = {}\n",
    "    with open(jsonl_path, 'r') as input_json:\n",
    "        for index, json_line in enumerate(input_json):\n",
    "            json_dict = json.loads(json_line)\n",
    "            # append the dictionary to the dictionaries' list\n",
    "            json_list_of_dict.append(json_dict)\n",
    "            # insert (paper_id, index) pair as (key, value) to the dictionary\n",
    "            json_dict_of_index[json_dict['paper_id']] = index\n",
    "    return json_list_of_dict, json_dict_of_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "korean-breath",
   "metadata": {},
   "outputs": [],
   "source": [
    "def json_s2orc_chunk_read( dataset_path, file_name ):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        dataset_path (string): Path to the Dataset directory (es. '../data/s2orc-sample-20200705v1/sample').\n",
    "        file_name (string): Name of the file to read (es. 'sample.jsonl').\n",
    "            \n",
    "    Return:\n",
    "        json_dict (list of dict): Dictionary such as: \n",
    "                { 'metadata': [...], 'pdf_parses': [...] }\n",
    "            with objects of type metadata_CLASS and pdf_parses_CLASS respectively\n",
    "    \"\"\"\n",
    "    if verbose: print(\"[INFO-START] Chunk read: \", file_name)\n",
    "\n",
    "    json_dict_of_list = {'metadata': [], 'pdf_parses': [], 'meta_key_idx': {}, 'pdf_key_idx': {}}\n",
    "    \n",
    "    if verbose: print(\"[INFO] Metadata read: \", file_name)\n",
    "    jsonl_path_metadata = os.path.join(dataset_path, 'metadata', file_name)\n",
    "    json_list_metadata, json_dict_of_index_meta = read_json_list_dict(jsonl_path_metadata)\n",
    "    json_dict_of_list['metadata'] = json_list_metadata\n",
    "    json_dict_of_list['meta_key_idx'] = json_dict_of_index_meta\n",
    "    \n",
    "    if verbose: print(\"[INFO] Pdf_Parses read: \", file_name)\n",
    "    jsonl_path_pdf_parses = os.path.join(dataset_path, 'pdf_parses', file_name)\n",
    "    json_list_pdf_parses, json_dict_of_index_pdf = read_json_list_dict(jsonl_path_pdf_parses)\n",
    "    json_dict_of_list['pdf_parses'] = json_list_pdf_parses\n",
    "    json_dict_of_list['pdf_key_idx'] = json_dict_of_index_pdf\n",
    "    \n",
    "    if verbose: print(\"[INFO-END  ] Chunk read: \", file_name)                 \n",
    "    return json_dict_of_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nutritional-associate",
   "metadata": {},
   "outputs": [],
   "source": [
    "def json_s2orc_multichunk_read( dataset_path, file_names ):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        dataset_path (string): Path to the Dataset directory (es. '{data}/s2orc-sample-20200705v1/sample').\n",
    "        file_names (list of string): List of filenames (es. ['sample_0.jsonl', 'sample_1.jsonl'])\n",
    "            present in `{dataset_path}/metadata` and `{dataset_path}/pdf_parses`.\n",
    "    \n",
    "    \"\"\"\n",
    "    if verbose: print(\"[INFO-START] Multichunk read\")\n",
    "    \n",
    "    multichunks_lists = []\n",
    "    for file_name in file_names:\n",
    "\n",
    "        chunk_list = json_s2orc_chunk_read( dataset_path, file_name )\n",
    "        multichunks_lists.append(chunk_list)\n",
    "    \n",
    "    return multichunks_lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deadly-freeware",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(DATASET_PATH)\n",
    "print(file_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "concerned-limitation",
   "metadata": {},
   "outputs": [],
   "source": [
    "multichunks_lists = json_s2orc_multichunk_read( DATASET_PATH,file_names )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bulgarian-grain",
   "metadata": {},
   "source": [
    "We have used only the `sample.jsonl` or the pair (`metadata_0.jsonl`-`pdf_parses_0.jsonl`) so we just have one element in the `multichunks_lists`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "logical-judgment",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(multichunks_lists)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "subjective-animation",
   "metadata": {},
   "source": [
    "We have parses all the `metadata` and `pdf_parses` elements, so we have now a dictionary that is composed by:\n",
    "```python\n",
    "json_dict_of_list = {\n",
    "    'metadata': [], \n",
    "    'pdf_parses': {}, \n",
    "    'meta_key_idx': {}, \n",
    "    'pdf_key_idx': {}\n",
    "}\n",
    "```\n",
    "In this dictionary we see:\n",
    "* metadata - `List[dict]` of type `metadata`.\n",
    "* pdf_parses - `List[dict]` of type `pdf_parses`.\n",
    "* meta_key_idx - `dict` with keys: `paper_id` and values: `index` in the metadata list.\n",
    "* pdf_key_idx - `dict` with keys: `paper_id` and values: `index` in the pdf_parses list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "close-wagon",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = multichunks_lists[0]['meta_key_idx']['77499681']\n",
    "multichunks_lists[0]['metadata'][index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "actual-dressing",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = multichunks_lists[0]['pdf_key_idx']['77499681']\n",
    "print(multichunks_lists[0]['pdf_parses'][index]['paper_id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "polar-glass",
   "metadata": {},
   "source": [
    "## 2.3 Title Abstract - Full text  (s2orc)\n",
    "---\n",
    "We have loaded the `S2ORC` dataset, created our (one chunk) dataset parses and we want now starting creating our dataset objects (Classes and Loaders).\n",
    "\n",
    "Let's start with the datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hollywood-pitch",
   "metadata": {},
   "source": [
    "### Dataset creation\n",
    "We want to create the datasets for papers' title-abstract and fulltext-(title-abstract) generation. \n",
    "> we'd like also to create a KeyPhrase dataset, we are actualling waiting for the response from the `S2ORC` authors to understand where can we possibly obtain the keyphrases/keywords.\n",
    "\n",
    "In order to do this, we want to create the two datasets (saving them as `jsonl` files).\n",
    "We can organize the data folder as :\n",
    "```bash\n",
    "- data/\n",
    "    # keyphrase dataset \n",
    "    - keyphrase/\n",
    "        # (title - abstract - fulltext - keyphrase)\n",
    "        - s2orc/\n",
    "            - README.md\n",
    "            - chuncks_dataset_idx.json\n",
    "            - train/\n",
    "                - train_0.jsonl\n",
    "                - train_1.jsonl\n",
    "                - ...\n",
    "            - test/\n",
    "                - test_0.jsonl\n",
    "                - test_1.jsonl\n",
    "                - ...\n",
    "            - val/\n",
    "                - val_0.jsonl\n",
    "                - val_1.jsonl\n",
    "                - ...\n",
    "    \n",
    "    # sts datasets\n",
    "    - sts/ \n",
    "        # (title - abstract - cosine_similarity)\n",
    "        - s2orc_partial/\n",
    "            - README.md\n",
    "            - chuncks_dataset_idx.json\n",
    "            - train/\n",
    "                - train_0.jsonl\n",
    "                - train_1.jsonl\n",
    "                - ...\n",
    "            - test/\n",
    "                - test_0.jsonl\n",
    "                - test_1.jsonl\n",
    "                - ...\n",
    "            - val/\n",
    "                - val_0.jsonl\n",
    "                - val_1.jsonl\n",
    "                - ...\n",
    "                \n",
    "        # (title - abstract - fulltext - cosine_similarity)\n",
    "        - s2orc_full/\n",
    "            - README.md\n",
    "            - chuncks_dataset_idx.json\n",
    "            - train/\n",
    "                - train_0.jsonl\n",
    "                - train_1.jsonl\n",
    "                - ...\n",
    "            - test/\n",
    "                - test_0.jsonl\n",
    "                - test_1.jsonl\n",
    "                - ...\n",
    "            - val/\n",
    "                - val_0.jsonl\n",
    "                - val_1.jsonl\n",
    "                - ...\n",
    "```\n",
    "and in the `chuncks_dataset_idx.json` there is the dictionary that maps the `chuncks` (`metadata_{id}.jsonl, pdf_parses_{id}.jsonl for id in range(99)`) into the {train|test|validation}_{id}.\n",
    "\n",
    "A first step to not-using chuncks (neither metadata nor fulltext) anymore is to summarize the data we want into a new python structure (dict) as follow, and save them \n",
    "\n",
    "```python\n",
    "{\n",
    "    \"paper_id\": (string-int), \n",
    "    \"title\":  (string),\n",
    "    \"abstract\": (string), \n",
    "    \"fulltext\": (string), \n",
    "    \"keywords\": List[string],\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collaborative-conviction",
   "metadata": {},
   "source": [
    "1. get the training/validation dataset by extracting Title-Abstract from the `S2ORC` dataset, and getting the testing data from the `KeyPhrase` (*'inspec', 'krapivin', 'nus', 'semeval', 'kp20k', 'duc', 'stackexchange'*) datasets. We should have a pair of sentence (indicativelly a *title* and an *abstract*), possibly a *fulltext* and a *keywords* fields those can be\n",
    "\n",
    "    - completelly related (abstract and its corresponding title)\n",
    "    - someway related (abstract and a field-keyphrase related title {cs+(deep learning; metric learning; nlp; sts;)}\n",
    "    - unrelated but not far away (abstract and a field-**not**keyphrase related title {cs+(nlp; transformer;)-vs-(cv; attention)}\n",
    "    - completelly unrelated (abstract and title are field-keyphrase unrelated {cs+a -vs- phy+z})\n",
    "\n",
    "\n",
    "\n",
    "2. **🤗transformers**, we can see [here](https://huggingface.co/docs/datasets/loading_datasets.html#json-files) the dataset loader (from `jsonl` files) can be used to load train/validation datasets. As we have alrerady load the dataset as dictionary (it is called `multichunks_lists` now, depending on how many chuncks we need to load in one shot) we could also be using the example [here](https://huggingface.co/docs/datasets/loading_datasets.html#from-a-python-dictionary) in order to load the dataset from an existing dictionary. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "clear-values",
   "metadata": {},
   "source": [
    "1. **sentence-transformer**, [sBERT example for train](https://www.sbert.net/docs/training/overview.html#loss-functions) \n",
    "\n",
    "2. **🤗transformers**, we can see [here](https://huggingface.co/docs/datasets/loading_datasets.html#json-files) the dataset loader (from `jsonl` files) can be used to load train/validation datasets. As we have alrerady load the dataset as dictionary (it is called `multichunks_lists` now, depending on how many chuncks we need to load in one shot) we could also be using the example [here](https://huggingface.co/docs/datasets/loading_datasets.html#from-a-python-dictionary) in order to load the dataset from an existing dictionary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "express-tract",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
